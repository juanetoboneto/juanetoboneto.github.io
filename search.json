[
  {
    "objectID": "proyects.html",
    "href": "proyects.html",
    "title": "Lista de Proyectos",
    "section": "",
    "text": "Aquí encontraras todos los proyectos relacionados con ciencia de datos, estadística, visualización y creación de modelos para datos de mi portafolio hasta la fecha en formato de página web.\n\n\n\nEfectividad de una Estrategía con Regresión Lineal:\n\n\nSe analizó un caso de un banco que buscaba recuperar fondos de deudores morosos mediante una estrategia dividida por grados de morosidad. Se realizó un estudio estadístico previo y se empleó una regresión lineal para evaluar la efectividad de la estrategia. Haz click aquí para ver el proyecto\n\n\n\n\n\nPronóstico de Series de Tiempo mediante XGBoost:\n\n\nA partir de los datos históricos OHLCV del índice Standard & Poor’s 500, buscamos un modelo de series de tiempo capaz de pronosticar sus respectivos valores de cierre en el mercado. Para ello, probamos modelos clásicos y el modelo de aprendizaje automático de regresión XGBoost. Haz click aquí para ver el proyecto\n\n\n\n\n\nBase de Datos Relacionales con SQLite3:\n\n\nSe presenta un ejemplo de base de datos relacionales simple para una red social ficticia, que incluye usuarios, seguidores, publicaciones y likes. Se construyó utilizando Python y SQL, este último implementado a través del módulo sqlite3. Haz click aquí para ver el proyecto\n\n\n\n\n\nPredicción de Precios utilizando Bosque Aleatorio:\n\n\nEn este proyecto, buscamos construir un modelo de pronóstico para los precios de venta de autos en un concesionario de la India, a partir de otros datos del concesionario y de los propios autos. Dicho modelo se basa en una regresión de bosque aleatorio, que es un bagging o bootstrap del método de regresión de árboles de decisión. Haz click aquí para ver el proyecto"
  },
  {
    "objectID": "proyects/Rentabilidad_con_Regresion_Lineal.html#estadistica-descriptiva-de-variable-unidimensional",
    "href": "proyects/Rentabilidad_con_Regresion_Lineal.html#estadistica-descriptiva-de-variable-unidimensional",
    "title": "Proyecto: “Which Debts Are Worth the Bank’s Effort?“",
    "section": "5.1. Estadistica Descriptiva de Variable Unidimensional:",
    "text": "5.1. Estadistica Descriptiva de Variable Unidimensional:\n\nResumen de Estadistica Descriptiva en Una Variable: En nuestro proyecto particular deberemos hacer un estudio, primero, de cada variable presente en los datos; para esto nos sera util primero describir que tipos de variable se pueden presentar en la estadistica descriptiva (según Introductory Statistics, Prem S. Mann; Christoper Jay Lacke):\n\n\nLas Variables Cuantitativas: Estas son variables que pueden ser medidas numericamente, se dividen en continuas (medidas por numeros reales) y discretas (medidas por numeros enteros).\nLas Variables Cualitativas: Tambien llamadas Categoricas, son el tipo de variables que, apesar de no poder asumir valores numericos, pueden ser clasificadas en 2 o mas categorias no numericas.\n\nPara nuestro analisis necesitamos conocer las distintas clases de objetos que estaran presentes en nuestra Data, y como son entendidos tannto por el programa que usamos como por los diferentes modulos:\nTipos de Clasificacion en Python Pandas: Al ser nuestro compilador de data, para clasificar utilizaremos la funcionalidad de Pandas, para estos tipos se diferencian los elementos de 32 ó 64 Bits, si son diferentes no se podran operar entre si. * int: para los enteros. * float: para los reales. * datetime:, para los datos que clasifican en el tiempo. * category: especificamente para las variables cualitativas. * object: se usa para secuencias que pueden ser de numeros y caracteres.\nPara identificar cual es el tipo de cada variable de nustros datos a la vez podremos usar el siguiente comando:\n\n\nCode\nDatos.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1882 entries, 0 to 1881\nData columns (total 6 columns):\n #   Column                          Non-Null Count  Dtype  \n---  ------                          --------------  -----  \n 0   id                              1882 non-null   int64  \n 1   monto_de_recuperacion_esperado  1882 non-null   int64  \n 2   monto_de_recuperacion_real      1882 non-null   float64\n 3   estrategia_de_recuperacion      1882 non-null   object \n 4   edad                            1882 non-null   int64  \n 5   sexo                            1882 non-null   object \ndtypes: float64(1), int64(3), object(2)\nmemory usage: 88.3+ KB\n\n\nGracias a esta función podemos ver el tipo de cada una de nuestras variables. Con esto en mente podremos obtener a partir de las Variables Cuantitativas Estas van desde Medidas de Tendencia Central (Que buscan obtener un dato que represente el conjunto de los datos) como el promedio, pasando por Medidas de Dispercion como la desviacion estandar (Que determina que tan dispersos estan los datos con respecto al promedio), hasta Medidas de Posicion (Por la naturaleza de los datos que poseemos) de Datos ]Agruados, tales como los Cuartiles.\nDe estos datos el comando XX.describe() nos dara varias de esas medidas:\n\ncount: Es la cantidad de datos de dicha variable\nmean: Tambien llamada media aritmetica o promedio, medida de Tendencia Central única, es un valor que representa el comportamiento de los datos.\nstd: Es un valor que verifica qué tan agrupados o separados están los datos.\nmin: Es el mínimo valor que hay en los datos.\ncuartiles: son los valores de la distribución que la dividen en cuatro partes iguales, en intervalos que comprenden el mismo número de valores en este caso un 25%, 50%,75%.\nmax: Representa el valor maximo en la lista que tomo dicha variable, y coincide con el percentil del 100%.\n\n\n\nCode\nDatos.describe()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nmonto_de_recuperacion_esperado\nmonto_de_recuperacion_real\nedad\n\n\n\n\ncount\n1882.000000\n1882.000000\n1882.000000\n1882.000000\n\n\nmean\n1035.596174\n2759.967588\n4000.967837\n39.650372\n\n\nstd\n591.458429\n2019.826565\n4576.506350\n15.453072\n\n\nmin\n6.000000\n194.000000\n200.425000\n18.000000\n\n\n25%\n534.250000\n1261.250000\n1045.190300\n28.000000\n\n\n50%\n1029.500000\n2062.000000\n2115.446373\n35.000000\n\n\n75%\n1551.750000\n3569.500000\n5417.237207\n50.000000\n\n\nmax\n2056.000000\n9964.000000\n34398.479710\n84.000000\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nCon esto en mente pasemos a ver cada variable en particular:\n\n5.1.1. Estadistica Descriptiva de la Variable Monto de Recuperacion Esperado:\nPara poder hacer un estudio estadistico descriptivo de esta variable, notese que esta es de tipo int64 con lo cual de dicha variable es de tipo Cuantitativa Discreta y podremos realizar las medidas de Tendencia, Dispersion y Posicion de XX.describe(); Sin embargo, esta información puede parecer insuficiente, en ese caso podremos tomar otras medidas:\n\n\nCode\n# Mediana (Medida de Tendencia Central)\nmediana_MRE = median(Datos.monto_de_recuperacion_esperado)\nprint(mediana_MRE)\n\n\n2062.0\n\n\n\n\nCode\n# Moda (Medida de Tendencia Central):\nmoda_MRE = sci.stats.mode(Datos.monto_de_recuperacion_esperado)\nprint(moda_MRE)\n\n\nModeResult(mode=array([1386]), count=array([5]))\n\n\nFutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n  moda_MRE = sci.stats.mode(Datos.monto_de_recuperacion_esperado)\n\n\nLa moda de este conjunto de datos numéricos es el valor que más se repite, es decir, el que tiene el mayor número de frecuencias absolutas. En este caso el valor que más se repite es 1386 con 5 apariciones.\nOtras Medidas de Tendencia Central que nos pueden ser de utilidad, si por ejemplo, no nos sirve la Media Aritmetica, son los siguientes:\n\n\nCode\n# Centro de Amplitud:\ncentroA_MRE = (max(Datos.monto_de_recuperacion_esperado)+min(Datos.monto_de_recuperacion_esperado))/2\nprint(centroA_MRE)\n\n\n5079.0\n\n\nEste es el valor que queda en medio de los valores mínimo y máximo.\nNotese que no incluimos la Media Ponderada pues no poseemos informacion especifica sobre la importancia de cada cliente (Uno podria ser los Niveles de la Estrategia de Recuperacion pero en esta parte aun hablamos solo de Variables Unidimensionales).\nTambien podremos tomar otras Medidas de Dispersion que nos den una idea de que tan representativos son los de Tendencia Central:\n\n\nCode\n# Rango:\nRango_MRE = max(Datos.monto_de_recuperacion_esperado)-min(Datos.monto_de_recuperacion_esperado)\nprint(Rango_MRE)\n\n\n9770\n\n\nEl rango de una distribución es la diferencia entre el valor máximo y el valor mínimo de la variable estadística. En este caso nuestra variable estadística es “monto_de_recuperacion_esperado”, el cual tiene un rango de 9770.\nOtras Medidas de Posicion:\n\n\nCode\n# La Varianza:\nVar_MRE = np.std(Datos.monto_de_recuperacion_esperado)**2\nprint(Var_MRE)\n\n\n4077531.605219366\n\n\nLa varianza mide la mayor o menor dispersión de los valores de la variable respecto a la media aritmética. Cuanto mayor sea la varianza mayor dispersión existirá y por tanto, menor representatividad tendrá la media aritmética.\n\n\nCode\n# Coeficiente de Variacion:\nCVar_MRE = np.std(Datos.monto_de_recuperacion_esperado)/np.mean(Datos.monto_de_recuperacion_esperado)*100\nprint(CVar_MRE,\"%\")\n\n\n73.16353588681146 %\n\n\nEl coeficiente de variación de Pearson que se define como el cociente entre la desviación estándar y el valor absoluto de la media aritmética. Este coeficiente, representa el porcentaje que la desviación estándar contiene a la media aritmética y por lo tanto cuanto mayor es el coeficiente de variación mayor es la dispersión y menor la representatividad de la media.\nAhora Bien, despues de obtener toda esta información estadistica, demos una pequeña visualización a través de una gráfica:\nLa siguiente se trata de la Grafica de Densidad de la variable Monto de Recuperación Esperado:\n\n\nCode\nDatos.monto_de_recuperacion_esperado.plot.density(color='black', figsize = (20,15), fontsize=15)\n\n#Mediana(Azul clarito)\nx_1=[2062, 2062]\ny_1=[0,0.000243]\nl1= plt.plot(x_1,y_1, color=\"deepskyblue\")\n\n#Media(Azul detergente)\nx_2=[2759.967588, 2759.967588]\ny_2=[0,0.000166]\nl2= plt.plot(x_2,y_2)\n\n#Moda(Azul Oscuro)\nx_3=[1386, 1386]\ny_3=[0,0.000335]\nplt.plot(x_3,y_3, color=\"navy\")\n\nplt.xlim(-2500,12500)\nplt.ylim(0,0.00035)\nplt.legend([\"Densidad\",\"Mediana\",\"Media\",\"Moda\"],loc=\"upper right\")\nplt.rc('legend',fontsize=20)\n\nplt.title(\"Densidad del Monto de Recuperación Esperado\",fontsize=25)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nDe dicha gráfica, se puede inferir que el Banco, espera que la gran mayoria de montos que esperan recuperar estan entre 0 y 4000 Unidades Monetarias, apesar de ello tambien se puede ver que los montos superiores a estos no pueden ser ignorados.\nTambien podremos observar la Gráfica de Caja del Monto de Recuperación Esperado:\n\n\nCode\nfig = plt.figure(figsize =(15, 10))\nplt.boxplot(Datos['monto_de_recuperacion_esperado'])\n\n#Mediana(Azul clarito)\nx_1=[0.75,1.25]\ny_1=[2062,2062]\nl1= plt.plot(x_1,y_1, color=\"deepskyblue\", label=\"Mediana\")\n\n#Media(Azul detergente)\ny_2=[2759.967588, 2759.967588]\nx_2=[0.75,1.25]\nl2= plt.plot(x_2,y_2, label=\"Media\")\n\n#Moda(Azul oscuro)\ny_3=[1386, 1386]\nx_3=[0.75,1.25]\nplt.plot(x_3,y_3, color=\"navy\", label=\"Moda\")\n\n\nplt.rc('legend',fontsize=20)\nplt.legend( loc=\"upper right\")\n\nplt.title(\"Diagrama de Caja Monto de Recuperación Esperado\",fontsize=25)\nplt.show()\n\n\n\n\n\n\n\n\n\nEsta gráfica no solo confirma la acumulación de individuos en los que espera el Banco devuelva el monto alrededor de 0 a 4000, tambien hace evidente que no prodremos descartar los puntos atipicos.\n\n\n5.1.2. Estadistica Descriptiva de la Variable Monto de Recuperacion Real:\nEsta variable es de tipo float64 o según nuestra clasificación estadistica es Cuantitativa Continua con lo cual pudimos obtener las Medidas Centrales, de Dispersion y de Posición de XX.describe(), además de aquellas, podemos tomar otras medidas de esos 3 tipos, empezando por la mas común aparte de la media y mediana:\n\n\nCode\n# Mediana (Medida de Tendencia Central)\nmediana_MRR = median(Datos.monto_de_recuperacion_real)\nprint(mediana_MRR)\n\n\n2115.4463725\n\n\n\n\nCode\n# Moda (Medida de Tendencia Central):\nmoda_MRR = sci.stats.mode(Datos.monto_de_recuperacion_real)\nprint(moda_MRR)\n\n\nModeResult(mode=array([200.425]), count=array([1]))\n\n\nFutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n  moda_MRR = sci.stats.mode(Datos.monto_de_recuperacion_real)\n\n\nOtras Medidas de Tendencia Central:\n\n\nCode\n# Centro de Amplitud:\ncentroA_MRR = (max(Datos.monto_de_recuperacion_real)+min(Datos.monto_de_recuperacion_real))/2\nprint(centroA_MRR)\n\n\n17299.452355\n\n\nNotese que no incluimos la Media Ponderada pues no poseemos informacion especifica sobre la importancia de cada cliente (Uno podria ser los Niveles de la Estrategia de Recuperacion pero en esta parte aun hablamos solo de Variables Unidimensionales).\nTambien podremos tomar otras Medidas de Dispersion que nos den una idea de que tan representativos son los de Tendencia Central:\n\n\nCode\n# Rango:\nRango_MRR = max(Datos.monto_de_recuperacion_real)-min(Datos.monto_de_recuperacion_real)\nprint(Rango_MRR)\n\n\n34198.05471\n\n\nOtras Medidas de Posición:\n\n\nCode\n# La Varianza:\nVar_MRR = np.std(Datos.monto_de_recuperacion_real)**2\nprint(Var_MRR)\n\n\n20933281.567631625\n\n\n\n\nCode\n# Coeficiente de Variacion:\n# El coeficiente en este caso es extraño pues sobrepasa el 100%\nCVar_MRR = np.std(Datos.monto_de_recuperacion_real)/np.mean(Datos.monto_de_recuperacion_real)*100\nprint(CVar_MRR,\"%\")\n\n\n114.35458899641793 %\n\n\nDespues de mostrar esta información estadistica, veamos como podemos representar los datos de esta variable graficamente.\nLa siguiente se trata de la Grafica de Densidad de la variable Monto de Recuperación Real:\n\n\nCode\nDatos.monto_de_recuperacion_real.plot.density(color='black', figsize = (20,15))\n\n#Mediana(Azul clarito)\nx_1=[2115.4463725, 2115.4463725]\ny_1=[0,0.000165]\nl1= plt.plot(x_1,y_1, color=\"deepskyblue\")\n\n#Media(Azul detergente)\nx_2=[4000.967837    , 4000.967837   ]\ny_2=[0,0.0000705]\nl2= plt.plot(x_2,y_2)\n\n#Moda(Azul oscuro)\nx_3=[200.425, 200.42]\ny_3=[0,0.000130]\nplt.plot(x_3,y_3, color=\"navy\")\n\nplt.xlim(-10000,50000)\nplt.ylim(0,0.00021)\n\nplt.rc('legend',fontsize=20)\n\nplt.legend([\"Densidad\",\"Mediana\",\"Media\",\"Moda\"],loc=\"upper right\")\n\nplt.title(\"Densidad del Monto de Recuperación Real\")\nplt.show()\n\n\n\n\n\n\n\n\n\nProsiguiendo con el análisis podremos observar que la recuperación de dinero real se encuentra sobretodo en montos de entre 0 y 5000 Unidades Monetarias , o lo que es igual, al compararlo con el gráfico de Densidad del monto de recuperación esperado por el Banco, este posee un error en la prediccion de tendencia central de aproximadamente 1000 Unidades Monetarias, además claro la cola de la Densidad del monto real en este caso SI es despreciable.\nTambien podremos realizar el Gráfico de Caja para esta variable:\n\n\nCode\nfig = plt.figure(figsize =(15, 10))\nplt.boxplot(Datos['monto_de_recuperacion_real'])\n\n#Mediana(Azul clarito)\n\nx_1=[0.75,1.25]\ny_1=[2115.4463725,2115.4463725]\nl1 = plt.plot(x_1,y_1, color=\"deepskyblue\", label = \"Mediana\")\n\n#Media(Azul detergente)\n\ny_2=[4000.967837, 4000.967837]\nx_2=[0.75,1.25]\nl2 = plt.plot(x_2,y_2, label = \"Media\")\n\n#Moda(Azul oscuro)\n\ny_3=[200.425, 200.425]\nx_3=[0.75,1.25]\nplt.plot(x_3,y_3, color=\"navy\", label = \"Moda\")\n\n\nplt.rc('legend',fontsize=20)\nplt.legend( loc=\"upper right\")\n\nplt.title(\"Densidad del Monto de Recuperación Real\",fontsize=25)\nplt.show()\n\n\n\n\n\n\n\n\n\nEsta Gráfica puede ser engañosa, pues los extremos superior e inferior realmente no son realmente representativos de la Tendencia Central; cuyo intervalo es algo mayor, lo que a su vez explica la gran cantidad de Puntos Atipicos entre los 10000 y los 20000.\n\n\nAnálisis de la gráfica\n\n\n5.1.3. Estadistica Descriptiva de la Variable Estrategia de Recuperacion:\nEsta variable, según el comando Datos.info() es de tipo object esto significa que se trata de una cadena de simbolos alfa-numericos; es por ello que el comando Datos.describe() no nos provee informacion de la misma.\nPor lo que debemos usar esta misma función especificamente para esta variable:\n\n\nCode\nDatos.estrategia_de_recuperacion.describe()\n\n\ncount                        1882\nunique                          5\ntop       Nivel 1 de Recuperacion\nfreq                          670\nName: estrategia_de_recuperacion, dtype: object\n\n\nLa tabla de arriba nos provee de toda la información estadistica que podemos obtener de la variable Estrategia de Recuperación (Al menos por ahora pues solo tratamos con Variables Unidimensionales), asi que es importante revisar cuales son, de aquellos que no conocemos:\n\nunique: Son la lista de categorias Cualitativas que se le pueden asignar a los datos de esta variable, en este caso son 5, las cuales se tratan de las 5 Estrategias de Recuperación (de 0 a 4) cuyas diferenciacion categorica ya se explico en la parte de Datos del Proyecto.\ntop: La Moda (Medida de Tendencia Central), que por su definición tambien es aplicable a las Variables Cualitativas, en este caso se trata del Nivel de Recuperación 1.\nfreq: La cantidad de veces que se repite el dato que es la Moda, para esta variable este se repite 670 veces.\n\nOtro dato estadistico que podemos tomar de esta variable son las Proporciones de cada categoria, y para calcularlos, nos serviremos de algunos algoritmos, como sigue:\nProporcion del Nivel 0 de las Estrategias de Recuperación:\n\n\nCode\nDataE=np.array(Datos.estrategia_de_recuperacion)\nDataEst=DataE.transpose()\n# Numero de Elementos del Nivel 0 de Estrategia de Recuperación\nfor index,k in enumerate(DataEst):\n  if k=='Nivel 0 de Recuperacion':\n    DataEst[index]=1\n  else:\n    DataEst[index]=0\n\nNum_Nivel_0=0\nfor N0 in DataEst:\n  Num_Nivel_0 += N0\n\nprint(Num_Nivel_0)\n\n# Proporcion de elementos del Nivel 0 de Estrategia de Recuperación\nProp_Nivel_0 = Num_Nivel_0/len(Datos.estrategia_de_recuperacion)*100\nprint(Prop_Nivel_0,\"%\")\n\n\n247\n13.12433581296493 %\n\n\nProporción del Nivel 1 de las Estrategias de Recuperación:\n\n\nCode\nDataE=np.array(Datos.estrategia_de_recuperacion)\nDataEst=DataE.transpose()\n\n# Numero de Elementos del Nivel 1 de Estrategia de Recuperación\nfor index,k in enumerate(DataEst):\n  if k=='Nivel 1 de Recuperacion':\n    DataEst[index]=1\n  else:\n    DataEst[index]=0\n\nNum_Nivel_1=0\nfor N1 in DataEst:\n  Num_Nivel_1 += N1\n\nprint(Num_Nivel_1)\n\n# Proporcion de elementos Nivel 1 en Estrategia de Recuperación\nProp_Nivel_1 = Num_Nivel_1/len(Datos.estrategia_de_recuperacion)*100\nprint(Prop_Nivel_1,\"%\")\n\n\n670\n35.60042507970245 %\n\n\nProporción del Nivel 2 de las Estrategias de Recuperación:\n\n\nCode\nDataE=np.array(Datos.estrategia_de_recuperacion)\nDataEst=DataE.transpose()\n\n# Numero de Elementos del Nivel 2 de Estrategia de Recuperación\nfor index,k in enumerate(DataEst):\n  if k=='Nivel 2 de Recuperacion':\n    DataEst[index]=1\n  else:\n    DataEst[index]=0\n\nNum_Nivel_2=0\nfor N2 in DataEst:\n  Num_Nivel_2 += N2\n\nprint(Num_Nivel_2)\n\n# Proporcion de elementos Nivel 2 en Estrategia de Recuperación\nProp_Nivel_2 = Num_Nivel_2/len(Datos.estrategia_de_recuperacion)*100\nprint(Prop_Nivel_2,\"%\")\n\n\n333\n17.69394261424017 %\n\n\nProporción del Nivel 3 de las Estrategias de Recuperación:\n\n\nCode\nDataE=np.array(Datos.estrategia_de_recuperacion)\nDataEst=DataE.transpose()\n\n# Numero de Elementos del Nivel 3 de Estrategia de Recuperación\nfor index,k in enumerate(DataEst):\n  if k=='Nivel 3 de Recuperacion':\n    DataEst[index]=1\n  else:\n    DataEst[index]=0\n\nNum_Nivel_3=0\nfor N3 in DataEst:\n  Num_Nivel_3 += N3\n\nprint(Num_Nivel_3)\n\n# Proporcion de elementos Nivel 2 en Estrategia de Recuperación\nProp_Nivel_3 = Num_Nivel_3/len(Datos.estrategia_de_recuperacion)*100\nprint(Prop_Nivel_3,\"%\")\n\n\n368\n19.55366631243358 %\n\n\nProporción de Nivel 4 de las Estrategias de Recuperación:\n\n\nCode\nDataE=np.array(Datos.estrategia_de_recuperacion)\nDataEst=DataE.transpose()\n\n# Numero de Elementos del Nivel 4 de Estrategia de Recuperación\nfor index,k in enumerate(DataEst):\n  if k=='Nivel 4 de Recuperacion':\n    DataEst[index]=1\n  else:\n    DataEst[index]=0\n\nNum_Nivel_4=0\nfor N4 in DataEst:\n  Num_Nivel_4 += N4\n\nprint(Num_Nivel_4)\n\n# Proporcion de elementos Nivel 2 en Estrategia de Recuperación\nProp_Nivel_4 = Num_Nivel_4/len(Datos.estrategia_de_recuperacion)*100\nprint(Prop_Nivel_4,\"%\")\n\n\n264\n14.027630180658873 %\n\n\nYa con toda esta información estadistica, podemos pasar al análisis por medio de un Historigrama:\n\n\nCode\ncolorB=['darkslategray','teal','darkturquoise', 'dodgerblue','steelblue']\nfig, ax = plt.subplots(figsize =(12, 8))\nax.bar(['nivel 0','nivel 1', 'nivel 2', 'nivel 3', 'nivel 4'], [Num_Nivel_0,Num_Nivel_1,Num_Nivel_2,Num_Nivel_3,Num_Nivel_4],color=colorB)\nax.set_title('Niveles de estrategia de recuperación',loc = \"left\", fontdict = {'fontsize':14, 'fontweight':'bold'})\nplt.show()\n\n\n\n\n\n\n\n\n\nDe donde se ve que el Banco espera que se recuperen montos, principamente de entre 1000 y 2000 Unidades Monetarias.\n\n\n5.1.4. Estadistica Descriptiva de la Variable Edad:\nComo podemos observar mas arriba gracias al comando XX.info() esta variable es de tipo int64 con lo cual entra en la categoria de Variable Cuantitativa Discreta y podremos, no solo, obtener la información de XX.describe() sino tambien otro tipo de Medidas Estadisticas:\n\n\nCode\n# Moda (Medida de Tendencia Central):\nmoda_E = sci.stats.mode(Datos.edad)\nprint(moda_E)\n\n\nModeResult(mode=array([33]), count=array([82]))\n\n\nFutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n  moda_E = sci.stats.mode(Datos.edad)\n\n\nOtras Medidas de Tendencia Central:\n\n\nCode\n# Centro de Amplitud:\ncentroA_E = (max(Datos.edad)+min(Datos.edad))/2\nprint(centroA_E)\n\n\n51.0\n\n\nNotese que no incluimos la Media Ponderada pues no poseemos informacion especifica sobre la importancia de cada cliente (Uno podria ser los Niveles de la Estrategia de Recuperacion pero en esta parte aun hablamos solo de Variables Unidimensionales).\nTambien podremos tomar otras Medidas de Dispersion que nos den una idea de que tan representativos son los de Tendencia Central:\n\n\nCode\n# Rango:\nRango_E = max(Datos.edad)-min(Datos.edad)\nprint(Rango_E)\n\n\n66\n\n\nOtras Medidas de Posición:\n\n\nCode\n# La Varianza:\nVar_E = np.std(Datos.edad)**2\nprint(Var_E)\n\n\n238.6705338680333\n\n\n\n\nCode\n# Coeficiente de Variacion:\nCVar_E = np.std(Datos.edad)/np.mean(Datos.edad)*100\nprint(CVar_E,\"%\")\n\n\n38.96297740722824 %\n\n\nDespues de mostrar esta información estadistica, veamos como podemos representar los datos de esta variable atravez de una gráfica de violin, que sintetiza la información del gráfico de caja y del de densidad:\n\n\nClasificación de edad\nPara corroborar la información, podemos generar un diagrama de barras que muestre de manera clara lo afirmado anteriormente. primero clasificamos las edades por rangos:\n\n\nCode\nnp_edad=np.array(Datos.edad)\nEdadT=np_edad.transpose()\n\nfor index,k in enumerate(EdadT):\n  if 11&lt;=k&lt;=20:\n    np_edad[index]=1\n  else:\n    np_edad[index]=0\n\nEdad11_20=0\nfor M in EdadT:\n  Edad11_20 += M\n\nprint(Edad11_20)\n\n\n108\n\n\n\n\nCode\nnp_edad2=np.array(Datos.edad)\nEdadT2=np_edad2.transpose()\n\nfor index,k in enumerate(EdadT2):\n  if 21&lt;=k&lt;=30:\n    np_edad[index]=1\n  else:\n    np_edad[index]=0\n\nEdad21_30=0\nfor M in EdadT:\n  Edad21_30 += M\n\n\nprint(Edad21_30)\n\n\n520\n\n\n\n\nCode\nnp_edad3=np.array(Datos.edad)\nEdadT3=np_edad3.transpose()\n\nfor index,k in enumerate(EdadT3):\n  if 31&lt;=k&lt;=40:\n    np_edad[index]=1\n  else:\n    np_edad[index]=0\n\nEdad31_40=0\nfor M in EdadT:\n  Edad31_40 += M\n\n\nprint(Edad31_40)\n\n\n523\n\n\n\n\nCode\nnp_edad4=np.array(Datos.edad)\nEdadT4=np_edad4.transpose()\n\nfor index,k in enumerate(EdadT4):\n  if 41&lt;=k&lt;=50:\n    np_edad[index]=1\n  else:\n    np_edad[index]=0\n\nEdad41_50=0\nfor M in EdadT:\n  Edad41_50 += M\n\n\nprint(Edad41_50)\n\n\n276\n\n\n\n\nCode\nnp_edad5=np.array(Datos.edad)\nEdadT5=np_edad5.transpose()\n\nfor index,k in enumerate(EdadT5):\n  if 51&lt;=k&lt;=60:\n    np_edad[index]=1\n  else:\n    np_edad[index]=0\n\nEdad51_60=0\nfor M in EdadT:\n  Edad51_60 += M\n\n\nprint(Edad51_60)\n\n\n226\n\n\n\n\nCode\nnp_edad6=np.array(Datos.edad)\nEdadT6=np_edad6.transpose()\n\nfor index,k in enumerate(EdadT6):\n  if 61&lt;=k&lt;=70:\n    np_edad[index]=1\n  else:\n    np_edad[index]=0\n\nEdad61_70=0\nfor M in EdadT:\n  Edad61_70 += M\n\n\nprint(Edad61_70)\n\n\n133\n\n\n\n\nCode\nnp_edad7=np.array(Datos.edad)\nEdadT7=np_edad7.transpose()\n\nfor index,k in enumerate(EdadT7):\n  if 71&lt;=k&lt;=80:\n    np_edad[index]=1\n  else:\n    np_edad[index]=0\n\nEdad71_80=0\nfor M in EdadT:\n  Edad71_80 += M\n\n\nprint(Edad71_80)\n\n\n75\n\n\n\n\n- Diagrama de barras\nSe puede ver en la gráfica que efectivamente la mayoría de deudores se encuentran entre los 21 y 40 años.\n\n\nCode\nfig, ax = plt.subplots(figsize =(10, 5))\nax.bar(['11-20','21-30', '31-40', '41-50', '51-60','61-70','71-80','&gt;80'], [Edad11_20,Edad21_30,Edad31_40,Edad41_50,Edad51_60,Edad61_70,Edad71_80,1881-(Edad11_20+Edad21_30+Edad31_40+Edad41_50+Edad51_60+Edad61_70+Edad71_80)])\nax.set_title('Edades',loc = \"left\", fontdict = {'fontsize':14, 'fontweight':'bold'})\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDe esta gráfica se pueden observar varias cosas, entre ellas, que la tendencia es que los deudores tienden a decrecer con respecto a la Edad, apartir de los 30 años y a crecer de los 18 a los 30 años; tambien, la mayoria de deudores se encuentran entre los 20 y 40 años, aunque no se pueden descartar las Edades fuera de este rango.\n\n\n5.1.5. Estadistica Descriptiva de la Variable Sexo:\nEsta variable, según el comando Datos.info() es de tipo object esto significa que se trata de una cadena de simbolos alfa-numericos; es por ello que el comando Datos.describe() no nos provee informacion de la misma.\nPor lo que debemos usar esta misma función especificamente para esta variable:\n\n\nCode\nDatos.sexo.describe()\n\n\ncount          1882\nunique            2\ntop       Masculino\nfreq            973\nName: sexo, dtype: object\n\n\nLa tabla de arriba nos provee de toda la información estadistica que podemos obtener de la variable Sexo (Al menos por ahora pues solo tratamos con Variables Unidimensionales), asi que es importante revisar cuales son, aquellos que no conocemos:\n\nunique: Son la lista de categorias Cualitativas que se le pueden asignar a los datos de esta variable, en este caso 2, Masculino y Femenino.\ntop: La Moda (Medida de Tendencia Central), que por su definición tambien es aplicable a las Variables Cualitativas, en este caso es la categoria Masculino.\nfreq: La cantidad de veces que se repite la Moda, para esta variable es 973 veces.\n\nOtro dato estadistico que podemos tomar de esta variable son las Proporciones de cada categoria, y para calcularlos, nos serviremos de algunos algoritmos, como sigue:\n\n\nCode\nDataS=np.array(Datos.sexo)\nDataSex=DataS.transpose()\n\n# Numero de Elementos de Categoria Masculino en Sexo\nfor index,k in enumerate(DataSex):\n  if k=='Masculino':\n    DataSex[index]=1\n  else:\n    DataSex[index]=0\n\nNum_Masculino=0\nfor M in DataSex:\n  Num_Masculino += M\n\nprint(Num_Masculino)\n\n# Proporcion de elementos Masculino en Sexo\nProp_Sex_Masculino = Num_Masculino/len(Datos.sexo)*100\nprint(Prop_Sex_Masculino,\"%\")\n\n\n973\n51.70031880977683 %\n\n\nY las Proporciones de “Femenino” con respecto a la Variable serán:\n\n\nCode\nDataS=np.array(Datos.sexo)\nDataSex=DataS.transpose()\n\n# Numero de Elementos de Categoria Femenino en Sexo\nfor index,k in enumerate(DataSex):\n  if k=='Femenino':\n    DataSex[index]=1\n  else:\n    DataSex[index]=0\n\nNum_Femenino=0\nfor F in DataSex:\n  Num_Femenino += F\n\nprint(Num_Femenino)\n\n#Proporción de Femenino en Sexo\nProp_Sex_Femenino = Num_Femenino/len(Datos.sexo)*100\nprint(Prop_Sex_Femenino,\"%\")\n\n\n909\n48.29968119022317 %\n\n\nPodremos realizar una Grafica de Pastel de la Variable Sexo, en este caso se compara de entre la cantidad de mujeres y el de hombres. La variable Sexo no es interpretada por NumPy como una Numerica (mas especificamente del tipo object), por lo que debemos manipular los datos para cambiar su tipo:\n\n\nCode\nfig, ax = plt.subplots(figsize =(8, 10))\nGender=['Hombres','Mujeres']\nNum_Gender=[Num_Masculino,1881-Num_Masculino]\nplt.pie(Num_Gender, labels=Gender, autopct=\"%0.1f %%\")\nplt.show()\n\n\n\n\n\n\n\n\n\nY de donde se puede decir que la mayoria de deudores son hombres, aunque la diferencia no es significativa."
  },
  {
    "objectID": "proyects/Rentabilidad_con_Regresion_Lineal.html#análisis-de-dos-variables",
    "href": "proyects/Rentabilidad_con_Regresion_Lineal.html#análisis-de-dos-variables",
    "title": "Proyecto: “Which Debts Are Worth the Bank’s Effort?“",
    "section": "5.2 Análisis de dos variables",
    "text": "5.2 Análisis de dos variables\npodemos crear una gráfica de comparación pairplot de la edad (clasificando en dos grupos) con los montos de recuperación esperados y el monto de recuperación real, y ver que tipo e clientes realmente cumplen con el pago.\ngráficos de dispersión\nPara realizar el analisis gráfico comparando las variables que tenemos 2 a 2, necesitaremos expresar a las variables sexo y Estrategia de Recuperación como variables numericas:\n\n\nCode\nfind = ['Masculino', 'Femenino','Nivel 0 de Recuperacion','Nivel 1 de Recuperacion', 'Nivel 2 de Recuperacion','Nivel 3 de Recuperacion', 'Nivel 4 de Recuperacion']\nreplace = [0,1,0,1,2,3,4]\nDatos_Numerico = Datos.replace(find, replace)\nDatos_Numerico.head()\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\nmonto_de_recuperacion_esperado\nmonto_de_recuperacion_real\nestrategia_de_recuperacion\nedad\nsexo\n\n\n\n\n0\n2030\n194\n263.540\n0\n19\n0\n\n\n1\n1150\n486\n416.090\n0\n25\n1\n\n\n2\n380\n527\n429.350\n0\n27\n0\n\n\n3\n1838\n536\n296.990\n0\n25\n0\n\n\n4\n1995\n541\n346.385\n0\n34\n0\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n5.2.? Correlación de Todas las Variables:\n\n\n5.2.9 Gráficas Múltiples de Correlación 2 a 2:\nCon todo el estudio ya realizado en la relación de cada 2 variables podremos sintetizar toda esta información atraves de una Matriz de Correlacion:\n\n\nCode\ncorr_matrix = Datos_Numerico.iloc[:,1:7].corr()\ncorr_matrix\n\n\n\n\n  \n    \n      \n\n\n\n\n\n\nmonto_de_recuperacion_esperado\nmonto_de_recuperacion_real\nestrategia_de_recuperacion\nedad\nsexo\n\n\n\n\nmonto_de_recuperacion_esperado\n1.000000\n0.903727\n0.906582\n0.794451\n0.021485\n\n\nmonto_de_recuperacion_real\n0.903727\n1.000000\n0.801913\n0.716007\n0.033324\n\n\nestrategia_de_recuperacion\n0.906582\n0.801913\n1.000000\n0.801555\n0.034651\n\n\nedad\n0.794451\n0.716007\n0.801555\n1.000000\n-0.016876\n\n\nsexo\n0.021485\n0.033324\n0.034651\n-0.016876\n1.000000\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nAunque esta matriz nos provee de información útil, podremos mostrar la correlación entre variables de manera mas sintactica atravez de su respectiva Gráfica de Panal:\n\n\nCode\nplt.figure(figsize=(8,8))\nsns.heatmap(corr_matrix, annot = True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nMonto de Recuperación Esperado - Estrategia de Recuperación: La alta correlacion entre las dos es evidente pues una es la clasificación creada para el otro.\nMonto de Recuperación Real - Estrategia de Recuperación: Como se puede observar el coeficiente de correlacion entre estas 2 variables es del 80%, menor al 90% de correlacion entre el Nivel de Recuperación y el Monto supuesto por el Banco; sin embargo, esto indica una alta precision en cuanto a la asignación de estrategias por parte del Banco, sin embargo, desconocemos si este nivel es aceptable.\nSexo: Como es de esperar, el sexo no tiene correlación con ninguna de las tres variables ya que una persona puede estar endeudada en mayor o menor medida independientemente del sexo que tenga.\nEdad-Nivel de estrategia de recuperación: La alta correlación se debe a que mientras mayor sea una persona, esta tiene más capacidad de pagar una deuda, por tanto la estrategia de recuperación aplicada por el banco será mayor.\nAdemás podremos realizar un cuadro completo de las diferentes Gráficas de Dispersion entre cada 2 variables como sigue:\n\n\nCode\nplt.figure(figsize=(10,15))\nsns.pairplot(data=Datos_Numerico,vars=['monto_de_recuperacion_esperado', 'monto_de_recuperacion_real', 'edad'], hue=None)\nplt.show()\n\n\n&lt;Figure size 720x1080 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\nInserte observaciones apartir de la gráfica\n\nMonto de recuperación esperado - Monto de recuperación real: Al relacionar estas variables podemos evidenciar en la gráfica que entre más alto sea el monto de recuperación esperado más alto será el monto de recuperación real, ya que, es claro que entre más se le deba al banco más altas serán las cuotas.\nMonto de recuperación esperado - Edad: Al relacionar estas variables podemos deducir que el monto de recuperación esperado más alto se encuentra entre la edad de 50 a 80 años, lo cual es entendible ya que personas entre esa edad se relacionan a una mejor estabilidad económica y, por lo tanto, se espera recuperar una mayor cantidad de dinero de estas.\nMonto de recuperación real - Edad: Teniendo en cuenta lo anterior, al tener una mejor estabilidad económica las personas que están entre la edad de 50 a 80 años, entonces estas pueden pagar al banco una mayor cantidad y, por lo tanto, el dinero recuperado de estas es mayor."
  },
  {
    "objectID": "proyects/Rentabilidad_con_Regresion_Lineal.html#modelo-de-regresion-monto-de-recuperacion-esperado-monto-de-recuperacion-real",
    "href": "proyects/Rentabilidad_con_Regresion_Lineal.html#modelo-de-regresion-monto-de-recuperacion-esperado-monto-de-recuperacion-real",
    "title": "Proyecto: “Which Debts Are Worth the Bank’s Effort?“",
    "section": "6. Modelo de Regresion: Monto de Recuperacion Esperado-Monto de Recuperacion Real:",
    "text": "6. Modelo de Regresion: Monto de Recuperacion Esperado-Monto de Recuperacion Real:\nAhora, buscamos predecir el comportamiento del Monto de Recuperación Real a partir del Monto de Recuperación Esperado, para esto nos apoyamos en un modelo de regresión lineal:\n\n6.1. Sobre el Modelo:\nEn este primer modelo de regrecion lineal, tomaremos el Monto de Recuperacion Esperado como nuestra Variable Independiente y al Monto de Recuperacion Real lo designaremos como la Variable Dependiente (Esto es posible debido a la alta correlación que existe entre estos dos).\nPara ello utilizaremos el primer valor del umbral dado de 1000$, ahora, tomaremos un intervalo alrededor de este umbral, el intervalo será (900,1100), no es necesario tomar intervalos más grandes ya que lo que nos interesa es la distribución de los datos cerca del umbral.\n\n\nCode\n#Asignemos un Nombre al intervalo\nintervalo_900_1100 = Datos.loc[(Datos['monto_de_recuperacion_esperado']&lt;1100) & (Datos['monto_de_recuperacion_esperado']&gt;=900)]\n\nprint(intervalo_900_1100)\n\n\n       id  monto_de_recuperacion_esperado  monto_de_recuperacion_real  \\\n158   520                             900                  504.790000   \n159  1036                             900                  539.535000   \n160  1383                             900                  554.745000   \n161   998                             901                  887.005000   \n162  1351                             903                  667.035000   \n..    ...                             ...                         ...   \n336  1184                            1096                 1077.218384   \n337  1664                            1096                 2053.290126   \n338   302                            1098                  876.997775   \n339   554                            1098                 1836.918718   \n340  1501                            1099                 1277.630578   \n\n    estrategia_de_recuperacion  edad       sexo  \n158    Nivel 0 de Recuperacion    34  Masculino  \n159    Nivel 0 de Recuperacion    34   Femenino  \n160    Nivel 0 de Recuperacion    24  Masculino  \n161    Nivel 0 de Recuperacion    32  Masculino  \n162    Nivel 0 de Recuperacion    28  Masculino  \n..                         ...   ...        ...  \n336    Nivel 1 de Recuperacion    38  Masculino  \n337    Nivel 1 de Recuperacion    18   Femenino  \n338    Nivel 1 de Recuperacion    34  Masculino  \n339    Nivel 1 de Recuperacion    26   Femenino  \n340    Nivel 1 de Recuperacion    42   Femenino  \n\n[183 rows x 6 columns]\n\n\nAhora calcularemos el monto de recuperación real promedio para aquellos clientes justo por debajo y por encima del umbral utilizando un rango de 900 a 1100. Luego, realizaremos una prueba de Kruskal-Wallis para ver si las cantidades de recuperación reales son diferentes justo por encima y por debajo del umbral.\n\n\nCode\nNivel_0_actual = intervalo_900_1100.loc[Datos['estrategia_de_recuperacion']=='Nivel 0 de Recuperacion']['monto_de_recuperacion_real']\nNivel_1_actual = intervalo_900_1100.loc[Datos['estrategia_de_recuperacion']=='Nivel 1 de Recuperacion']['monto_de_recuperacion_real']\nstats.kruskal(Nivel_0_actual,Nivel_1_actual)\n\n\nKruskalResult(statistic=65.37966302528878, pvalue=6.177308752803109e-16)\n\n\nHay que tener en cuenta que el método de Kruskal-Wallis se utiliza para corroborar si existen diferencias relevantes a nivel estadístico entre dos o más grupos de una variable independiente en una variable dependiente.\nLa prueba determina si las madianas de dos o más grupos son diferentes. De esta forma calcula un estadístico de prueba y lo compara con un punto de corte de la distribución.\nEl P-valor es la probabilidad que mide la evidencia en contra de la hipótesis nula. Las probabilidades más bajas proporcionan una evidencia más fuerte en contra de la hipótesis nula. Con lo que en este caso se rechaza la hipótesis nula y se concluye que no todas las medianas son iguales, con lo que es conveniente realizar una regresión lineal.\n\n\n6.1.1. Regresión lineal:\nAhora queremos adoptar un enfoque basado en la regresión lineal para estimar el impacto del programa en el umbral de $1000 utilizando datos que están justo por encima y por debajo del umbral.\nEn este modelo, no estamos representando el umbral sino simplemente viendo cómo la variable utilizada para asignar los clientes (monto de recuperación esperado) se relaciona con la variable de resultado (monto de recuperación real).\nBuscamos observar si el dinero extra invertido en asignar una Estrategia de Recuperación alta es rentable para el banco.\n\n\nCode\n#Asignemos las Variables:\nx = intervalo_900_1100['monto_de_recuperacion_esperado']\ny = intervalo_900_1100['monto_de_recuperacion_real']\n\n\n\n\nCode\nn = len(x)\nz = np.array(x)\nw = np.array(y)\nsumx = sum(z)\nsumy = sum(w)\nsumx2 = sum(z**2)\nsumy2 = sum(w**2)\nsumxy = sum(z*w)\npromx = sumx/n\npromy = sumy/n\nm = (sumx*sumy-n*sumxy)/(sumx*sumx-n*sumx2)\nb = promy-m*promx\nplt.figure(figsize =(20, 10))\nplt.scatter(z,w, label=\"Datos\", s=12)\nplt.plot(z,m*z+b, label=\"Ajuste\", color=\"red\")\nplt.xlabel(\"Monto de recuperación esperado\")\nplt.ylabel(\"Monto de recuperación real\")\nplt.title(\"Regresión lineal\")\nplt.grid()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nAhora, necesitamos el coeficiente de determinación para saber que tan buen ajuste fue el que acabamos de hacer.\n\n\nCode\nvarix = np.sqrt(sumx2/n - promx*promx)\nvariy = np.sqrt(sumy2/n - promy*promy)\ncovarixy = sumxy/n - promx*promy\nR2 = (covarixy/(varix*variy))**2\nprint(R2)\n\n\n0.2605541661465666\n\n\nDicho coeficiente nos muestra que el ajuste no es tan bueno, sin embargo es el mejor que se puede realizar.\nAhora procederemos a verificar que el modelo que realizamos mediante métodos matemáticos coincide con el generado por el comando\nsm.OLS(y,x).fit()\n\n\nCode\n\nx = sm.add_constant(x)\nmodelo = sm.OLS(y, x).fit()\npredicciones = modelo.predict(x)\n\n\nmodelo.summary()\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmonto_de_recuperacion_real\nR-squared:\n0.261\n\n\nModel:\nOLS\nAdj. R-squared:\n0.256\n\n\nMethod:\nLeast Squares\nF-statistic:\n63.78\n\n\nDate:\nMon, 06 Mar 2023\nProb (F-statistic):\n1.56e-13\n\n\nTime:\n19:07:24\nLog-Likelihood:\n-1278.9\n\n\nNo. Observations:\n183\nAIC:\n2562.\n\n\nDf Residuals:\n181\nBIC:\n2568.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nconst\n-1978.7597\n347.741\n-5.690\n0.000\n-2664.907\n-1292.612\n\n\nmonto_de_recuperacion_esperado\n2.7577\n0.345\n7.986\n0.000\n2.076\n3.439\n\n\n\n\n\n\nOmnibus:\n64.493\nDurbin-Watson:\n1.777\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n185.818\n\n\nSkew:\n1.463\nProb(JB):\n4.47e-41\n\n\nKurtosis:\n6.977\nCond. No.\n1.80e+04\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The condition number is large, 1.8e+04. This might indicate that there arestrong multicollinearity or other numerical problems.\n\n\n\nEn esta tabla podemos ver algunos de los valores que usamos para construir el ajuste tales como el Coeficiente de la Constante (que coincide con el valor del intercepto de la recta con el eje y) y el Coeficiente de Monto de Recuperación Esperado (que coincide con la pendiente de la recta).\nAdemás podemos observar que el coeficiente de determinación hallado matemáticamente coincide con el valor de R-cuadrado mostrado en la tabla.\n\n\n6.2 Conclusiones.\nFinalmente, a partir de la regresión lineal podremos decir, interpretando el valor que obtuvimos de la pendiente siendo este aproximadamente 2.76, estadísticamente significativo, lo cual quiere decir que por cada UM que se aumente en el Monto de Recuperación Esperado el Monto de Recuperacion Real aumentará en 2.76 UM.\nTeniendo en cuenta el intervalo tomado (de 900 a 1100) junto con el umbral (de 1000) en donde se aplica una estrategia de recuperación, entonces las 100 UM que se aumentan en el Monto de Recuperación Esperado (de 900 a 1000) hasta llegar al umbral, se aumentará en 276 UM en el Monto de Recuperación Real.\nAdemás se sabe que el aplicar una Estrategia de Recuperación alta requiere una inversión de 50 UM por lo que aplicar una Estrategia de Recuperación alta a un a un cliente con mayor deuda es rentable para el banco, ya que recuperará alrededor 276 UM."
  },
  {
    "objectID": "proyects/Base_de_Datos_Relacionales_SQL.html",
    "href": "proyects/Base_de_Datos_Relacionales_SQL.html",
    "title": "\nBase de Datos Relacionales para una Red Social\n",
    "section": "",
    "text": "Base de Datos Relacionales para una Red Social\n\n\n1. Descripción del Proyecto\n\n\nEn este proyecto crearemos una serie de clases de python para crear una simulación de base de datos para una red social basada en texto que posea las siguientes características:\n\n\nQue solo los administradores puedan realizar queries no preestablecidas o crear/eliminar las tablas de la base de datos utilizando una contraseña.\n\nAlmacene la información personal de los usuarios al momento de estos registrarse en la red social.\n\nGenere un registro que permita relacionar seguidores con seguidos dentro de la red social.\n\nAlmacene las públicaciones en formato de texto de los usuarios y les asigne un registro único que relacione publicación y publicador.\n\nLleve una lista de que usuarios hicieron like a que publicaciones y se puedan ver quienes son los usuarios con mayor número de seguidores.\n\nQue las bases de datos actualicen su información en caso algun usuario deje de seguir a otro o deje de dar like a alguna publicacion.\n\nQue se puedan consultar de manera pública las publicaciones de un usuario junto con sus respectivos likes.\n\nLa base de datos debe estar almacenada en el ordenador local.\n\nPara todo lo anterior utilizaremos el módulo de sqlite3 que nos permitira crear bases de datos relacionales utilizando el lenguaje SQL mediante la librería SQLite, tambien trabajaremos con Pandas; haciendo asi que podamos trabajar dentro de un notebook de Jupyter Lab.\n\n\n\nCode\n# %load_ext sql &lt;-- Este se usaria si no tenemos dicha extensión instalada.\nimport sqlite3\nimport pandas as pd\n\n\n\n2. Creando las Bases de Datos\n\n\nPara cumplir con las condiciones que se nos piden, primero debemos crear la estructura de la Base de Datos, para ello sera necesario crear 4 tablas con las siguientes caracteristicas: \n\nTabla de Usuarios: Que almacenara todas la información del usuario para que se registre, a saber: Su Seudónimo (que debe ser único), nombre (primero y segundo), teléfono y correo electrónico, además de llevar registro de cuando se creo la cuenta. Por razones obvias esta tabla no dependera de ninguna otra.\n\nTabla de Seguidores: Que almacene el quien sigue a quien y donde evidentemente no se puede seguir a uno mismo o lo que es igual debe poseer: la identificación del usuario seguidor y la identificación del usuario seguido. Por lo anterior esta tabla solo dependera de la Tabla de Usuarios.\n\nTabla de Publicaciones: Esta tabla debe almacenar: la identificación de la publicación, el texto de la publicación, los likes de la publicación y por supuesto la fecha en la que se hizo la publicación. De esta manera, se relacionara la tabla con la Tabla de Usuarios y la Tabla de Likes.\n\nTabla de likes: Esta tabla debe contener un registro de quien realizo el like y a cual publicación la hizo. Se relacionara entonces con la Tabla de Usuarios y la Tabla de Publicaciones.  Con todo lo anterior tendremos que la estructura de nuestra base de datos es de la siguiente manera:\n\n\n\n\nimagen\n\n\n\nTeniendo en cuenta lo que se nos pide, debemos limitar las queries que realizamos a la base de datos para que únicamente puedan ser ejecutadas por los administradores, lo cual puede ser algo tedioso ya que se deben cubrir bastantes posibilidades. A pesar de ello, servirá para optimizar las demás clases que necesitamos crear. Todo esto, junto con la creación y eliminación de las tablas, lo llevaremos a cabo dentro de una clase que llamaremos Administrador, como sigue:”\n\n\n\nCode\n#Clase con las funcionalidades de Administrador :\nclass administrador:\n \n # Empecemos con las funciones de consultas para la Data Base:\n\n #Función para verificar la contraseña de administrador:    \n contraseña = \"contraseñaxd\" \n def verificar_contraseña(contraseña_ingresada):\n    return contraseña_ingresada == administrador.contraseña\n\n #Funcion para realizar las querys de sqlite3 (sin objetos instanciados):\n def query(contraseña_ingresada, peticion):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        path = 'C:\\\\Users\\\\User\\\\Desktop\\\\Portafolio\\\\DataSets\\\\bases_de_datos_relacionales_i\\\\red_social_database.db'\n        conexion = sqlite3.connect(path)\n        cursor = conexion.cursor()\n        cursor.execute(peticion)\n        conexion.close()\n    else:\n        pass\n    \n #Funcion de Query con commit (sin objetos instanciados):\n def query_commit(contraseña_ingresada, peticion):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        path = 'C:\\\\Users\\\\User\\\\Desktop\\\\Portafolio\\\\DataSets\\\\bases_de_datos_relacionales_i\\\\red_social_database.db'\n        conexion = sqlite3.connect(path)\n        cursor = conexion.cursor()\n        cursor.execute(peticion)\n        conexion.commit()\n        conexion.close()\n    else:\n        pass\n    \n # Query con objetos instanciados:\n def query_instanciado(contraseña_ingresada, peticion, objetos):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        path = 'C:\\\\Users\\\\User\\\\Desktop\\\\Portafolio\\\\DataSets\\\\bases_de_datos_relacionales_i\\\\red_social_database.db'\n        conexion = sqlite3.connect(path)\n        cursor = conexion.cursor()\n        cursor.execute(peticion, objetos)\n        conexion.close()\n    else:\n        pass\n\n # Query de commit con objetos instanciados:\n def query_instanciado_commit(contraseña_ingresada, peticion, objetos):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        path = 'C:\\\\Users\\\\User\\\\Desktop\\\\Portafolio\\\\DataSets\\\\bases_de_datos_relacionales_i\\\\red_social_database.db'\n        conexion = sqlite3.connect(path)\n        cursor = conexion.cursor()\n        cursor.execute(peticion, objetos)\n        conexion.commit()\n        conexion.close()\n    else:\n        pass\n\n # Un Query instanciado con petición:\n def query_busqueda(contraseña_ingresada, peticion, objetos):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        path = 'C:\\\\Users\\\\User\\\\Desktop\\\\Portafolio\\\\DataSets\\\\bases_de_datos_relacionales_i\\\\red_social_database.db'\n        conexion = sqlite3.connect(path)\n        cursor = conexion.cursor()\n        cursor.execute(peticion, objetos)\n        resultado = str(cursor.fetchone()[0])\n        conexion.close()\n        return resultado\n    else:\n        pass\n    \n  # Query de busqueda múltiple:\n def query_busqueda_multiple(contraseña_ingresada, peticion, objetos):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        path='C:\\\\Users\\\\User\\\\Desktop\\\\Portafolio\\\\DataSets\\\\bases_de_datos_relacionales_i\\\\red_social_database.db'\n        conexion = sqlite3.connect(path)\n        cursor = conexion.cursor()\n        cursor.execute(peticion, objetos)\n        resultado = cursor.fetchall()\n        conexion.close()\n        return resultado\n    else:\n        pass\n    \n # Query de busqueda múltiple sin objetos:\n def query_BMSO(contraseña_ingresada, peticion):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        path='C:\\\\Users\\\\User\\\\Desktop\\\\Portafolio\\\\DataSets\\\\bases_de_datos_relacionales_i\\\\red_social_database.db'\n        conexion = sqlite3.connect(path)\n        cursor = conexion.cursor()\n        cursor.execute(peticion)\n        resultado = cursor.fetchall()\n        conexion.close()\n        return resultado\n    else:\n        pass\n    \n# Con ello podremos empezar las funciones de administrador para las tablas de la Data Base:\n    \n #Funcionalidades de administrador para la tabla de usuarios:\n\n def crear_tabla_usuarios(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):    \n     try:\n      administrador.query(contraseña_ingresada, \"\"\"CREATE TABLE IF NOT EXISTS tabla_usuarios (\n                        identificacion_usuario INTEGER PRIMARY KEY,\n                        seudonimo_usuario VARCHAR(20) NOT NULL UNIQUE,\n                        correo_electronico VARCHAR(40) NOT NULL UNIQUE,\n                        primer_nombre VARCHAR(80) NOT NULL,\n                        segundo_nombre VARCHAR(80) NOT NULL,\n                        telefono VARCHAR(12) UNIQUE,\n                        creado_el TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP )\"\"\")\n      print(\"Se pudo crear la Tabla de Usuarios correctamente.\")   \n     except Exception as e:\n            print(f\"No se pudo crear la tabla de usuarios. Error: {e}\")   \n    else:\n        print(\"Contraseña incorrecta\")\n    \n def eliminar_tabla_usuarios(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n     try:\n      administrador.query(contraseña_ingresada,\"\"\"DROP TABLE IF EXISTS tabla_usuarios\"\"\")\n      print(\"Se pudo eliminar la Tabla de Usuarios correctamente.\") \n     except Exception as e:\n            print(f\"No se pudo eliminar la tabla de usuarios. Error: {e}\")\n    else:\n        print(\"Contraseña incorrecta\")\n    \n #Funcionalidades de administrador para la tabla de seguidores:\n #Nota: esta tabla tiene la condición de que nadie puede seguirse a sí mismo\n def crear_tabla_seguidores(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        try:\n         administrador.query(contraseña_ingresada, \"\"\"CREATE TABLE IF NOT EXISTS tabla_seguidores (\n                           identificacion_seguidor INT NOT NULL,\n                           identificacion_seguido INT NOT NULL,\n                           FOREIGN KEY (identificacion_seguidor) REFERENCES tabla_usuarios(identificacion_usuario),\n                           FOREIGN KEY (identificacion_seguido) REFERENCES tabla_usuarios(identificacion_usuario),\n                           PRIMARY KEY (identificacion_seguidor, identificacion_seguido) )\"\"\")\n         print(\"Se pudo crear la Tabla de Seguidores correctamente.\")\n        except Exception as e:\n            print(f\"No se pudo crear la Tabla de de Seguidores. Error: {e}\")\n    else:\n        print(\"Contraseña incorrecta\") \n\n def eliminar_tabla_seguidores(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        try:\n         administrador.query(contraseña_ingresada, \"\"\"DROP TABLE IF EXISTS tabla_seguidores\"\"\")\n         print(\"Se pudo eliminar la Tabla de Seguidores correctamente.\")\n        except Exception as e:\n            print(f\"No se pudo eliminar la Tabla de Seguidores. Error: {e}\")\n    else:\n        print(\"Contraseña incorrecta\")\n        \n #Funcionalidades de administrador para la tabla de publicaciones:\n \n def crear_tabla_publicaciones(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        try:\n         administrador.query(contraseña_ingresada, \"\"\"CREATE TABLE IF NOT EXISTS tabla_publicaciones (\n                           identificacion_publicacion INTEGER PRIMARY KEY,\n                           identificacion_usuario INT NOT NULL,\n                           texto_publicacion VARCHAR(12000) NOT NULL,\n                           numero_likes INT DEFAULT 0,\n                           creado_en TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n                           FOREIGN KEY (identificacion_usuario) REFERENCES tabla_usuarios(identificacion_usuario) )\"\"\")\n         print(\"Se pudo crear la Tabla de Publicaciones correctamente.\")\n        except Exception as e:\n            print(f\"No se pudo crear la Tabla de Publicaciones. Error:{e}\")\n    else:\n        print(\"Contraseña incorrecta\")\n            \n def eliminar_tabla_publicaciones(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        try:\n         administrador.query(contraseña_ingresada, \"\"\"DROP TABLE IF EXISTS tabla_publicaciones\"\"\")\n         print(\"Se pudo eliminar la Tabla de Publicaciones correctamente.\")\n        except Exception as e:\n            print(f\"No se pudo eliminar la Tabla de Publicaciones. Error:{e}\")\n    else:\n        print(\"Contraseña incorrecta\")\n            \n #Funcionalidades de administrador para la tabla de \"likes\":\n\n def crear_tabla_likes(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        try:\n         administrador.query(contraseña_ingresada, \"\"\"CREATE TABLE IF NOT EXISTS tabla_likes (\n                        identificacion_usuario INT NOT NULL,\n                        identificacion_publicacion INT NOT NULL,\n                        FOREIGN KEY (identificacion_usuario) REFERENCES tabla_usuarios(identificacion_usuario),\n                        FOREIGN KEY (identificacion_publicacion) REFERENCES tabla_publicaciones(identificacion_publicacion),\n                        PRIMARY KEY (identificacion_usuario, identificacion_publicacion))\"\"\")\n         print(\"Se pudo crear la Tabla de Likes correctamente.\")\n        except Exception as e:\n            print(f\"No se pudo crear la Tabla de Likes. Error:{e}\")\n    else:\n        print(\"Contraseña incorrecta\")\n\n def eliminar_tabla_likes(contraseña_ingresada):\n    if administrador.verificar_contraseña(contraseña_ingresada):\n        try:\n         administrador.query(contraseña_ingresada, \"\"\"DROP TABLE IF EXISTS tabla_likes\"\"\")\n         print(\"Se pudo eliminar la Tabla de Likes correctamente.\")\n        except Exception as e:\n            print(f\"No se pudo eliminar la Tabla de Likes. Error:{e}\")\n    else:\n        print(\"Contraseña incorrecta\")\n        \n # Operaciones con multiples tablas:\n\n def crear_todas_las_tablas(contraseña_ingresada):\n   administrador.crear_tabla_usuarios(contraseña_ingresada)\n   administrador.crear_tabla_seguidores(contraseña_ingresada)\n   administrador.crear_tabla_publicaciones(contraseña_ingresada)\n   administrador.crear_tabla_likes(contraseña_ingresada)\n\n def eliminar_todas_las_tablas(contraseña_ingresada):\n  administrador.eliminar_tabla_usuarios(contraseña_ingresada)\n  administrador.eliminar_tabla_seguidores(contraseña_ingresada)\n  administrador.eliminar_tabla_publicaciones(contraseña_ingresada)\n  administrador.eliminar_tabla_likes(contraseña_ingresada)\n\n\n\n3. Creando las Funcionalidades de Usuario\n\n\nLa siguiente clase debe poseer todas las funciones que solo estan disponibles para las personas registradas en la base de datos, esto quiere decir que necesariamente estan registrados en la red social, o lo que es igual existen valores de ellos en la Tabla de Usuarios. Por las condiciones que se nos fueron dadas deben existir al menos las siguientes funciones: \n\nAlmacenar los datos de un usuario al registrarse en la red social y que, si este lo desea, eliminar su cuenta y la información que dio.\n\nDespues de registrado, ser capaz de seguir o dejar de seguir otras cuentas.\n\nPoder publicar comentarios de texto y ser capaz de eliminarlos dado el caso.\n\nPoder dar like o anular el like a comentarios propios o de otros usuarios.   Para todas estas funcionalidades crearemos la clase Usuarios:\n\n\n\nCode\n# Creemos una clase que sintetize todas las funcionalidades para el usuario:\nclass usuario:\n \n # Funcionalidades de Usuario para la tabla de usuarios:\n    \n # Utilizaremos el instanciamiento de un usuario en la clase como su registro en la red social: \n def __init__(self,seudonimo_usuario, correo_electronico, primer_nombre, segundo_nombre, telefono):\n    \n    self.seudonimo_usuario = seudonimo_usuario\n    self.correo_electronico = correo_electronico\n    self.primer_nombre = primer_nombre\n    self.segundo_nombre = segundo_nombre\n    self.telefono = telefono\n\n    administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"INSERT INTO tabla_usuarios \n                   (seudonimo_usuario, correo_electronico, primer_nombre, segundo_nombre, telefono) VALUES(?,?,?,?,?)\"\"\", \n                   (self.seudonimo_usuario, self.correo_electronico, self.primer_nombre, self.segundo_nombre, self.telefono))\n    \n    print(f\"Su cuenta {seudonimo_usuario} ha sido creada correctamente.\")\n    \n    \n def eliminar_usuario(usuario):\n  try:\n    administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"DELETE  FROM tabla_usuarios \n                                           WHERE seudonimo_usuario = ?\"\"\", (usuario.seudonimo_usuario,))\n    print(f\"La cuenta {usuario.seudonimo_usuario} fue eliminada correctamente.\")\n  except Exception as e:\n    print(f\"No se pudo eliminar la cuenta. Error:{e}\")\n\n \n # Funcionalidades de la tabla de seguidores:\n\n # Estas funcionalidades son algo más complicadas debido a que necesitaremos realizar queris por orden a la tabla de usuarios\n # para poder luego guardarlas en la tabla de seguidores:\n\n def seguir_usuario(usuario, seguir):\n  try:\n   # realizando la query del ID del seguidor:     \n   x_1 = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario \n                    FROM tabla_usuarios WHERE seudonimo_usuario=?\"\"\", (usuario.seudonimo_usuario, ))\n   # realizando la query del ID del seguido:\n   x_2 = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario \n                    FROM tabla_usuarios WHERE seudonimo_usuario=?\"\"\", (seguir, ))\n   # guardando dicha relacion en la tabla de seguidores:\n   administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"INSERT INTO tabla_seguidores\n                                          (identificacion_seguidor, identificacion_seguido) VALUES(?,?)\"\"\", (x_1, x_2))\n   print(f\"{usuario.seudonimo_usuario} ahora sigue a {seguir}\")\n  except Exception as e:\n    print(\"Tu cuenta o la de la persona que intentas seguir no existen.\")\n    \n def dejar_de_seguir_usuario(usuario, dejar_de_seguir):\n    try:\n     # realizando la query del ID del seguidor:\n     x_1 = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario \n                                  FROM tabla_usuarios WHERE seudonimo_usuario=?\"\"\", (usuario.seudonimo_usuario, ))\n     # realizando la query del ID del seguido:\n     x_2 = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario \n                      FROM tabla_usuarios WHERE seudonimo_usuario=?\"\"\", (dejar_de_seguir, ))\n     # eliminando dicha relación de la tabla de seguidores:\n     administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"DELETE FROM tabla_seguidores\n                                            WHERE (identificacion_seguidor, identificacion_seguido) = (?, ?)\"\"\", (x_1, x_2))\n     print(f\"{usuario.seudonimo_usuario} dejo de seguir a {dejar_de_seguir}\")\n    except Exception as e:\n        print(f\"No se pudo anular el seguido: Error {e}\")\n        \n # Funcionalidades de usuario para la Tabla de Publicaciones:\n\n # Podriamos utilizar una condición \"if\" para asegurarnos que la entrada de las publicaciones sea necesariamente\n # un string, sin embargo esto extenderia innecesariamente el código, solo para que a efectos prácticos se tenga \n # el mismo resultado que introducir una variable \"normal\".\n    \n def publicar_comentario(usuario, publicacion):\n    try:\n     id_usuario = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario \n                      FROM tabla_usuarios WHERE seudonimo_usuario=?\"\"\", (usuario.seudonimo_usuario, ))\n     administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"INSERT INTO tabla_publicaciones \n                            (identificacion_usuario, texto_publicacion) VALUES(?,?)\"\"\", (id_usuario, publicacion))\n     print(\"Su comentario a sido publicado con éxito.\")\n    except Exception as e:\n     print(f\"Tu comentario no pudo ser publicado. Error:{e}\")\n  \n # Para la funcion de eliminar publicacion necesitaremos realizar 2 querys, una que nos de el ID del usuario y\n # otra que nos permita eliminar su comentario:\n def eliminar_publicacion(usuario, publicacion):\n  try:\n   id_usuario = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario FROM tabla_usuarios \n                                WHERE seudonimo_usuario=?\"\"\", (usuario.seudonimo_usuario, ))\n   administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"DELETE FROM tabla_publicaciones \n                                     WHERE (identificacion_usuario, texto_publicacion) = (?,?)\"\"\",(id_usuario, publicacion))\n   print(\"Su publicación a sido eliminado con éxito.\")\n  except Exception as e:\n   print(f\"Tu publicación no pudo ser eliminada. Error:{e}\")\n    \n    \n # Funcionalidades de usuario para la Tabla de Likes:\n\n def dar_like(usuario, publicador, publicacion):\n  try:\n   id_publicador = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario FROM tabla_usuarios \n                    WHERE seudonimo_usuario=?\"\"\", (publicador, ))\n   id_publicacion = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_publicacion FROM tabla_publicaciones \n                    WHERE (identificacion_usuario, texto_publicacion)=(?,?)\"\"\", (id_publicador, publicacion))\n   administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"INSERT INTO tabla_likes \n                    (identificacion_usuario, identificacion_publicacion) VALUES(?,?)\"\"\", (id_publicador, id_publicacion))\n   print(f\"A {usuario.seudonimo_usuario} le a gustado {publicacion}\")\n  except Exception as e:\n   print(f\"No se pudo dar like. Error:{e}\") \n\n def anular_like(usuario, publicador, publicacion):\n  try:\n   id_publicador = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario FROM tabla_usuarios \n                    WHERE seudonimo_usuario=?\"\"\", (publicador, ))\n   id_publicacion = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_publicacion FROM tabla_publicaciones \n                    WHERE (identificacion_usuario, texto_publicacion)=(?,?)\"\"\", (id_publicador, publicacion))\n   administrador.query_instanciado_commit(\"contraseñaxd\", \"\"\"DELETE FROM tabla_likes \n                    WHERE (identificacion_usuario, identificacion_publicacion)=(?,?)\"\"\", (id_publicador, id_publicacion))\n  except Exception as e:\n    print(f\"No se pudo anular tu like. Error: {e}\")\n\n\n\n4. Funcionalidades Públicas\n\n\nEn nuestra base de datos, además de realizar cambios como administradores y consultas como usuarios, también es necesario que podamos extraer cierta información de manera totalmente pública. Para ello, crearemos una clase diferenciada, la cual no deberá instanciar objetos ni utilizar contraseñas para obtener datos y que debe tener las siguientes funcionalidades:\n\n\nQue cualquiera en la red pueda consultar el nombre de un usuario en la red social.\n\nQue cualquiera pueda encontrar la identificación o registro de una publicación dentro de la red social.\n\nSe pueda ver una lista de las publicaciones de cualquier persona junto con el número de likes de cada una.\n\nCualquiera pueda observar quiénes son los 3 usuarios más seguidos en orden descendente y su respectivo número de seguidores.\n\nPara todas estas funcionalidades que faltan, creamos la clase Público:\n\n\n\nCode\nclass publico:\n\n # Funcion para buscar usuarios:\n def buscar_usuario(usuario_buscado):\n  try:\n    usuario_encontrado = administrador.query_busqueda_multiple(\"contraseñaxd\",\n                          \"\"\"SELECT primer_nombre, segundo_nombre FROM tabla_usuarios \n                             WHERE seudonimo_usuario=?\"\"\",(usuario_buscado,))\n    print(f\"El nombre del usuario al que buscas es: {usuario_encontrado[0][0]} {usuario_encontrado[0][1]}.\")\n  except Exception as e:\n    print(f\"No se pudo encontrar al Usuario. Error:{e}\")\n    \n # Buscar la identificacion de una publicacion   \n def buscar_registro_publicacion(usuario_publicador, texto_publicacion):\n  try:\n    id_usuario = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario \n                      FROM tabla_usuarios WHERE seudonimo_usuario=?\"\"\", (usuario_publicador, ))\n    id_publicaciones = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_publicacion \n       FROM tabla_publicaciones WHERE (identificacion_usuario, texto_publicacion) = (?,?)\"\"\", (id_usuario, texto_publicacion))\n    print(f\"El número de registro de la publicación del usuario {usuario_publicador} es {id_publicaciones}.\")\n  except Exception as e:\n    print(f\"Ocurrio un problema durante la busqueda. Error:{e}\")\n\n # Generar una lista de comentarios junto con likes:\n def lista_publicaciones(usuario_publicador):\n  try:\n   id_usuario = administrador.query_busqueda(\"contraseñaxd\", \"\"\"SELECT identificacion_usuario \n                      FROM tabla_usuarios WHERE seudonimo_usuario=?\"\"\", (usuario_publicador, ))\n   informacion = administrador.query_busqueda_multiple(\"contraseñaxd\",\n                    \"\"\"SELECT texto_publicacion, COUNT(*) AS numero_likes FROM tabla_publicaciones\n                       WHERE identificacion_usuario =?\"\"\", (id_usuario,))\n   informacion = pd.DataFrame(informacion, columns=[\"Comentario\", \"Número de Likes\"])\n   print(informacion)\n  except Exception as e:\n    print(f\"Ocurrio algun fallo en la busqueda. Error:{e}\")\n    \n # Top 3 los usuarios más seguidos:\n def top_seguidos():\n  try:\n    lista = administrador.query_BMSO(\"contraseñaxd\", \n             \"\"\"SELECT seudonimo_usuario, COUNT(identificacion_seguidor) FROM tabla_seguidores \n                JOIN tabla_usuarios ON (tabla_usuarios.identificacion_usuario = tabla_seguidores.identificacion_seguido)\n                GROUP BY identificacion_seguido ORDER BY COUNT(identificacion_seguidor) DESC LIMIT 3\"\"\")\n    lista = pd.DataFrame(lista, columns=[\"Usuario\", \"Número de seguidores\"])\n    print(lista)\n  except Exception as e:\n    print(f\"No se pudo cargar la lista. Error{e}\")\n\n\n\n5. Probando nuestras Bases de Datos\n\n\nPor último veamos como se comportan las funcionalidades de las diferentes clases el momento de interactuar con la base de dato relacional de nuestra red social, esto para comprobar de primera mano que satisface las condiciones que se nos pidieron al principio.  Empecemos intentando crear las tablas sin la contraseña de administrador:\n\n\nCode\nadministrador.crear_tabla_usuarios(\"No la contraseña correcta\")\n\n\nContraseña incorrecta\n\n\n\nAhora veamos como se pueden eliminar y crear toda la estructura de bases de datos a la vez, siendo un administrador con la contraseña correcta:\n\n\n\nCode\nadministrador.crear_todas_las_tablas(\"contraseñaxd\")\n\n\nSe pudo crear la Tabla de Usuarios correctamente.\nSe pudo crear la Tabla de Seguidores correctamente.\nSe pudo crear la Tabla de Publicaciones correctamente.\nSe pudo crear la Tabla de Likes correctamente.\n\n\n\nTambien, creemos un ejemplo de 3 Usuarios que se instancian como objetos de dicha clase. Registrandose en la base de datos de la red social:\n\n\n\nCode\njuan = usuario(\"juan\", \"juanes@gmail.com\", \"juan\", \"esteban\", \"3014415299\")\njose = usuario(\"jose\", \"josefo@gmail.com\", \"jose\", \"manuel\", \"3113739070\")\npepe = usuario(\"pepe\", \"pepe@gmail.com\", \"pepe\", \"gomez\", \"3122520503\")\n\n\nSu cuenta juan ha sido creada correctamente.\nSu cuenta jose ha sido creada correctamente.\nSu cuenta pepe ha sido creada correctamente.\n\n\n\nImaginemos a alguien que no posee una cuenta en la red social y busca a un usuario en específico:\n\n\n\nCode\npublico.buscar_usuario(\"juan\")\n\n\nEl nombre del usuario al que buscas es: juan esteban.\n\n\n\nAhora veamos que esa misma persona intenta eliminar la cuenta de ese usuario registrado, este ejemplo tambien serviría para alguien que SI esta registrado y que intenta eliminar la cuenta de otro usuario:\n\n\n\nCode\nalguien.eliminar_usuario(\"juan\")\n\n\nNameError: name 'alguien' is not defined\n\n\n\nVeamos en cambio, como los usuarios registrados empiezan a seguirse o dejar de seguirse entre ellos:\n\n\n\nCode\njuan.seguir_usuario(\"jose\")\njuan.dejar_de_seguir_usuario(\"jose\")\n\njose.seguir_usuario(\"juan\")\njose.seguir_usuario(\"pepe\")\n\npepe.seguir_usuario(\"juan\")\npepe.seguir_usuario(\"jose\")\n\n\njuan ahora sigue a jose\njuan dejo de seguir a jose\njose ahora sigue a juan\njose ahora sigue a pepe\npepe ahora sigue a juan\npepe ahora sigue a jose\n\n\n\nUna vez ya establecidas las relaciones entre seguidores y seguidos, veamos (desde la perspectiva de alguien de afuera) como se realiza una consulta de la lista de los más seguidos:\n\n\n\nCode\npublico.top_seguidos()\n\n\n  Usuario  Número de seguidores\n0    juan                     2\n1    pepe                     1\n2    jose                     1\n\n\n\nSiguiendo con el ejemplo anterior veamos como Juan realiza la primera publicacion de texto dentro de la red y como Jose le da like al mismo:\n\n\n\nCode\njuan.publicar_comentario(\"Hola gente, este es mi primer comentario siganme en esta red social plz\")\njose.dar_like(\"juan\", \"Hola gente, este es mi primer comentario siganme en esta red social plz\")\n\n\nSu comentario a sido publicado con éxito.\nA jose le a gustado Hola gente, este es mi primer comentario siganme en esta red social plz\n\n\n\nVeamos como una persona de fuera de la red busca el código de registro que tiene el comentario de Juan:\n\n\n\nCode\npublico.buscar_registro_publicacion(\"juan\", \"Hola gente, este es mi primer comentario siganme en esta red social plz\")\n\n\nEl número de registro de la publicación del usuario juan es 1.\n\n\n\nAhora veamos como alguien al que le interesa las públicaciones de Juan, pero no esta registrado en la red, puede consultar dichas publicaciones y los “likes” respectivos de cada una:\n\n\n\nCode\npublico.lista_publicaciones(\"juan\")\n\n\n                                          Comentario  Número de Likes\n0  Hola gente, este es mi primer comentario sigan...                1\n\n\n\nVeamos el caso en el que Juan se arrepiente de la publicación que realizo y decide eliminarla:\n\n\n\nCode\njuan.eliminar_publicacion(\"Hola gente, este es mi primer comentario siganme en esta red social plz\")\n\n\nSu publicación a sido eliminado con éxito.\n\n\n\nPodriamos eliminar toda la estructura de datos que hicimos. Sin embargo, por razones didácticas prefiero no hacerlo."
  },
  {
    "objectID": "posts/tipos-de-interes.html",
    "href": "posts/tipos-de-interes.html",
    "title": "Teoría del Interés I",
    "section": "",
    "text": "En un artículo anterior definimos el valor del dinero en el tiempo y vimos como se definia formalmente la tasa de interés \\(i\\), ahora nos encargaremos de estudiar las ganancias generadas por dicha tasa de interés en el caso de que esta sea fija. Para ello, primero tomaremos el tiempo como una variable numérica \\(t\\). Entonces, el cómo una inversión aumenta el capital invertido a través del tiempo será una función \\(a(t)\\) con respecto al tiempo a la cual llamaremos Función de Acumulación, y debe de poseer dos cualidades básicas: \\(a(0) = 1\\) y \\(a\\) es creciente. Es decir, si un periodo de tiempo \\(t_1\\) es menor que otro periodo \\(t_2\\), entonces \\(a(t_1) \\leq a(t_2)\\). Así, para una inversión de capital \\(P &gt; 0\\), la función que determina el Capital Total Resultante \\(F(t)\\) de la inversión tendrá la forma:\n\\[F(t) = P \\cdot a(t)\\]\nEn donde es y en la cual simplemente tomamos lo que invertimos y lo multiplicamos por la forma o proporción en la que va a aumentar. Un ejemplo de \\(a(t)\\) es un porcentaje del \\(9\\%\\) mensual. Al invertir \\(100,000\\) COP, acumularemos en un mes:\n\\[F(1) = P \\cdot a(1) = 100,000 \\cdot 1.09 = 109,000\\]\nNótese que si el tiempo como variable no es continuo, entonces tomaremos como unidades los plazos establecidos para obtener el porcentaje de interés. Por ejemplo, si el interés del 9% se obtiene en un plazo trimestral, la unidad de tiempo \\(t = 1\\) equivaldrá a tres meses, \\(t = 2\\) equivaldría a seis meses, y así sucesivamente. Por lo tanto, el interés ganado en un periodo \\(n\\)-ésimo, es decir, \\(t = n\\), es:\n\\[I_n = F(n) - F(n-1)\\]\n\n\nSi no conocemos el porcentaje o tasa de interés de una inversión, pero sí sabemos cuál es la función de acumulación \\(a(t)\\), entonces podremos determinar primero el monto de dinero correspondiente a la unidad de tiempo por medio de la siguiente fórmula:\n\\[i = a(1) - a(0)\\]\nSi además quisiéramos saber la tasa de interés como un porcentaje, usamos en cambio esta otra (deducida de nuevo por la medición del cambio relativo):\n\\[i = \\frac{a(1) - a(0)}{a(0)} = \\frac{A(1) - A(0)}{A(0)} = \\frac{I_1}{A(0)}.\\]"
  },
  {
    "objectID": "posts/tipos-de-interes.html#la-función-de-acumulación",
    "href": "posts/tipos-de-interes.html#la-función-de-acumulación",
    "title": "Teoría del Interés I",
    "section": "",
    "text": "En un artículo anterior definimos el valor del dinero en el tiempo y vimos como se definia formalmente la tasa de interés \\(i\\), ahora nos encargaremos de estudiar las ganancias generadas por dicha tasa de interés en el caso de que esta sea fija. Para ello, primero tomaremos el tiempo como una variable numérica \\(t\\). Entonces, el cómo una inversión aumenta el capital invertido a través del tiempo será una función \\(a(t)\\) con respecto al tiempo a la cual llamaremos Función de Acumulación, y debe de poseer dos cualidades básicas: \\(a(0) = 1\\) y \\(a\\) es creciente. Es decir, si un periodo de tiempo \\(t_1\\) es menor que otro periodo \\(t_2\\), entonces \\(a(t_1) \\leq a(t_2)\\). Así, para una inversión de capital \\(P &gt; 0\\), la función que determina el Capital Total Resultante \\(F(t)\\) de la inversión tendrá la forma:\n\\[F(t) = P \\cdot a(t)\\]\nEn donde es y en la cual simplemente tomamos lo que invertimos y lo multiplicamos por la forma o proporción en la que va a aumentar. Un ejemplo de \\(a(t)\\) es un porcentaje del \\(9\\%\\) mensual. Al invertir \\(100,000\\) COP, acumularemos en un mes:\n\\[F(1) = P \\cdot a(1) = 100,000 \\cdot 1.09 = 109,000\\]\nNótese que si el tiempo como variable no es continuo, entonces tomaremos como unidades los plazos establecidos para obtener el porcentaje de interés. Por ejemplo, si el interés del 9% se obtiene en un plazo trimestral, la unidad de tiempo \\(t = 1\\) equivaldrá a tres meses, \\(t = 2\\) equivaldría a seis meses, y así sucesivamente. Por lo tanto, el interés ganado en un periodo \\(n\\)-ésimo, es decir, \\(t = n\\), es:\n\\[I_n = F(n) - F(n-1)\\]\n\n\nSi no conocemos el porcentaje o tasa de interés de una inversión, pero sí sabemos cuál es la función de acumulación \\(a(t)\\), entonces podremos determinar primero el monto de dinero correspondiente a la unidad de tiempo por medio de la siguiente fórmula:\n\\[i = a(1) - a(0)\\]\nSi además quisiéramos saber la tasa de interés como un porcentaje, usamos en cambio esta otra (deducida de nuevo por la medición del cambio relativo):\n\\[i = \\frac{a(1) - a(0)}{a(0)} = \\frac{A(1) - A(0)}{A(0)} = \\frac{I_1}{A(0)}.\\]"
  },
  {
    "objectID": "posts/tipos-de-interes.html#interés-simple",
    "href": "posts/tipos-de-interes.html#interés-simple",
    "title": "Teoría del Interés I",
    "section": "Interés Simple",
    "text": "Interés Simple\nEl interés simple es aquel en el cual los intereses obtenidos en los periodos anteriores no afectan al actual ni a los futuros. Dicho de otra forma, la inversión siempre gana el mismo interés en cada periodo. Teniendo en cuenta que el tiempo\\(t\\) se mide en dichos periodos, vamos a tener que en \\(t\\) periodos tendremos la inversión inicial o principal llamado \\(P\\) junto con el valor generado por la tasa de interés \\(i\\) en los \\(t\\) periodos con respecto a dicha inversión \\(P\\). Por principio de suma y de multiplicación, esto sería lo mismo que decir:\n\\[F(t) = P+P\\cdot i\\cdot t = P \\cdot (1 + it)\\]\nDonde \\(I\\) es el capital total de la inversión más lo obtenido. Como ya se puede intuir (y como acabamos de ver), la función de acumulación de este tipo de interés es dicho factor:\n\\[a(t) = 1 + it.\\]"
  },
  {
    "objectID": "posts/tipos-de-interes.html#interés-compuesto",
    "href": "posts/tipos-de-interes.html#interés-compuesto",
    "title": "Teoría del Interés I",
    "section": "Interés Compuesto",
    "text": "Interés Compuesto\nEl interés compuesto ocurre cuando, en vez de mantener un valor de inversión principal constante, vamos añadiendo el ingreso generado por el interés del periodo de tiempo inmediatamente anterior al periodo de inversión actual. Imaginemos dicha situación: realizamos la inversión de un principal \\(P\\) que posee una tasa de interés \\(i\\). En ese caso:\n\\[\n\\begin{array}{|c|c|}\n\\hline\n\\textbf{Periodo de Tiempo }(t) & \\textbf{Capital Total Final }(F) \\\\\n\\hline\nt=1 & P+Pi=P(1+i) \\\\\n\\hline\nt=2 & P+P(1+i)i=P(1+i)^2 \\\\\n\\hline\nt=3 & P+P(1+i)^2i=P(1+i)^3 \\\\\n\\hline\n... & ... \\\\\n\\hline\n\\end{array}\n\\]\nY así sucesivamente, de manera tal que en un periodo de tiempo \\(t\\) obtendremos el total de capital total resultante \\(F(t)\\) como sigue:\n\\[F(t) = P(1 + i)^t\\]\nDe donde se infiere que la función de acumulación \\(a(t)\\) del interés compuesto es:\n\\[a(t) = (1 + i)^t.\\]\n\n¿Por qué elegir el Interés Compuesto?, Ejemplo 1:\nSi en el caso de una inversión de interés constante tenemos la oportunidad de realizar un interés compuesto, esto es deseable en cualquier oportunidad. Aquí simplemente tomamos los rendimientos y los volvemos a invertir junto con el principal. Supongamos que decido invertir cien unidades monetarias \\(P = 100\\ U\\) a una tasa de interés trimestral fija de \\(i = 9\\%\\) durante un periodo de \\(30\\) años, y debo elegir si realizar un interés simple \\(I_{S}\\) o compuesto \\(I_{C}\\). En ese caso, el capital final total de cada tipo de inversión será:\n\\[F_{S}(t) = 100 \\left(1 + 0.09 \\cdot \\frac{30\\cdot12}{3}\\right) = 1,180\\ U\\]\n\\[F_{C}(t) = 100 \\left(1 + 0.09\\right)^{\\frac{30\\cdot12}{3}} \\approx 3'098,701.57\\ U\\]\nComo en cualquier otro caso, la inversión con interés compuesto (sin tomar en cuenta el riesgo, de ahí la tasa de interés fija) es mejor que el interés simple. Para dejarlo más claro, véase la gráfica que compara el crecimiento de la función de acumulación del interés simple vs. el compuesto:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Definimos el intervalo [0, 100] para t\nt = np.linspace(0, 100, 500)\n\n# Definimos las funciones\na_S = 100 * (1 + 0.09 * t)\na_C = 100 * (1 + 0.09) ** t\n\n# Graficamos las funciones\nplt.figure(figsize=(10, 6))\nplt.plot(t, a_S, label='Interés Simple', color='blue')\nplt.plot(t, a_C, label='Interés Compuesto', color='red')\n\n# Añadimos etiquetas y título\nplt.xlabel(r'Periodo $(t)$ de $100$ trimestres (25 años)')\nplt.ylabel(r'Capital Total Resultante $(F)$')\nplt.title('Crecimiento del Interés Simple VS Compuesto')\nplt.legend()\nplt.grid(True)\n\n# Mostramos el gráfico\nplt.show()"
  },
  {
    "objectID": "posts/tipos-de-interes.html#valor-presente",
    "href": "posts/tipos-de-interes.html#valor-presente",
    "title": "Teoría del Interés I",
    "section": "Valor Presente",
    "text": "Valor Presente\nEl valor presente es la cantidad de dinero que debemos invertir hoy para obtener, en un tiempo dado, cierta cantidad de ganancias en el futuro. Es decir, es una función que, en base al capital final \\(F\\) y al tiempo \\(t\\), nos permite calcular el principal \\(P\\). Podemos obtener esta función a partir de las fórmulas del interés que vimos anteriormente.\nPara el interés simple, el valor presente se obtiene despejando la siguiente ecuación:\n\\[F=P(t)\\cdot(1+it)\\Rightarrow P(t)=\\frac{F}{1+it}\\]\nEl factor por el que se multiplica el valor final \\(F\\) también puede entenderse (en general) como una función del tiempo denominada Factor de Descuento y se denota como \\(v(t)\\). Es decir, en el caso del interés simple, el factor de descuento es:\n\\[v(t)=\\frac{1}{1+it}.\\]\nPara el interés compuesto, también podemos determinar la fórmula del valor presente en función del capital final, obteniendo así el siguiente resultado:\n\\[F=P(t)\\cdot(1+i)^t\\Rightarrow P(t)=\\frac{F}{(1+i)^t}\\]\nEn este caso, el factor de descuento tiene la forma:\n\\[v(t)=\\frac{1}{(1+i)^t}.\\]\n\nEjemplo de Valor Presente\nImaginemos una situación parecida a la anterior, es decir, una inversión con una tasa de interés trimestral fija de \\(i = 9\\%\\), y queremos saber ¿cuál es la cantidad inicial de dinero que debemos invertir para obtener una ganancia de \\(F = 10,000,000\\) COP en un plazo de 5 años? Podemos realizar este ejercicio tanto con interés simple como compuesto. Cinco años equivalen a \\(20\\) trimestres, de tal manera que \\(t = 20\\), y los resultados serán:\n\nPara el interés simple:\n\n\\[P_{S}(20) = \\frac{10,000,000}{1 + (0.09)(20)} \\approx 3'571,428\\ \\text{COP}\\]\n\nPara el interés compuesto:\n\n\\[P_{C}(20) = \\frac{10,000,000}{(1 + 0.09)^{20}} \\approx 1'784,309\\ \\text{COP}\\]\nDe nuevo, se puede ver que con un interés compuesto se puede obtener un rendimiento igual con una inversión inicial menor, en este caso específico con una inversión inferior a la mitad de la del interés compuesto."
  },
  {
    "objectID": "posts/tipos-de-interes.html#referencias",
    "href": "posts/tipos-de-interes.html#referencias",
    "title": "Teoría del Interés I",
    "section": "Referencias",
    "text": "Referencias\n\nHuertas Campos, J. A. (2001). Cálculo actuarial: Contingencias de vida individual. Universidad Nacional de Colombia.\nMeza Orozco, Juan de Jesús (2011). Matemáticas financieras aplicadas: Uso de las calculadoras financieras y Excel (4ta Ed.). ECOE Ediciones."
  },
  {
    "objectID": "posts/otros-conceptos-de-pruebas-de-hipotesis.html",
    "href": "posts/otros-conceptos-de-pruebas-de-hipotesis.html",
    "title": "Otros Conceptos de las Pruebas de Hipótesis I",
    "section": "",
    "text": "No solo se pueden formular las hipótesis nula y alternativa como vimos en los ejemplos de las pruebas de hipótesis, donde \\(H_1\\) es solo una estricta negación de \\(H_0\\). En cambio, si el contexto de la prueba lo requiere, podemos asumir que la opción alternativa es que el valor hipotético de la media es mayor o menor que el valor experimental. Así pues, asumamos que el valor obtenido experimentalmente de la media poblacional es \\(\\mu_0\\). Dicha formulación se puede hacer así:\n\nLa hipótesis nula será que, dadas \\(\\overline{x}\\) y \\(\\sigma\\), la media poblacional es igual o menor al valor obtenido experimentalmente:\n\n\\[H_0:\\mu\\leq\\mu_0\\]\n\nLa hipótesis alternativa será que, dadas \\(\\overline{x}\\) y \\(\\sigma\\), la media poblacional es estrictamente mayor que el valor obtenido experimentalmente:\n\n\\[H_1:\\mu&gt;\\mu_0\\]\nPor supuesto, dependiendo de lo que queramos hacer, la formulación de estas hipótesis puede cambiar, invirtiéndose, es decir, haciendo que la hipótesis nula sea que el valor obtenido experimentalmente es mayor o igual a \\(\\mu\\) y que la alternativa sea que el valor de \\(\\mu\\) es estrictamente menor. Por las características de estas formas alternativas para las hipótesis, en la distribución de probabilidad solo nos interesarán resultados referentes al lado derecho o izquierdo, por lo que este tipo de pruebas entran en la clasificación de las pruebas de hipótesis Unilaterales.\n\n\nTomemos el mismo ejemplo base de los tipos de prueba de hipótesis, solo que en esta ocasión queremos ver si la campaña de marketing mencionada tuvo un efecto estrictamente positivo en el valor de las ventas. Esto significa que solo nos interesa la cola derecha de la distribución de probabilidad. Recordemos que en este ejemplo el valor de la media poblacional es \\(\\mu_0=10345\\), la desviación estándar poblacional es \\(\\sigma=552\\) y la media muestral es \\(\\overline{x}=11641\\). Lo que va a variar es la formulación de nuestra hipótesis, las cuales serán:\n\\[H_0:\\mu\\leq10345\\]\n\\[H_1:\\mu&gt;10345\\]\n\n\nCode\n# Establecemos los parámetros:\nmedia_poblacional = 10345\nmedia_muestral = 11641\ndesv_std = 552\n\n# Junto con la significancia:\nsignificancia = 0.05\n\n\nAhora bien, todos los métodos que ya vimos para realizar pruebas de hipótesis centradas en la media poblacional con distribución normal pueden ser aplicados a esta nueva formulación de hipótesis. Para este caso, utilizaremos el mismo nivel de significancia que en el artículo anterior \\((5\\%)\\) junto con el método del \\(p\\)-valor.\nPara ello, basta con calcular la probabilidad de que la media muestral sea menor o igual al valor experimental con una distribución normal de parámetros \\(\\mu=10345\\) y \\(\\sigma=552\\). Esto nos dará directamente el \\(p\\)-valor que, si es menor que la significancia estadística, se rechazará la hipótesis nula y, si en cambio es mayor que la significancia, entonces se aceptará.\n\nSi nuestra hipótesis nula \\(H_0\\) es que la media poblacional es menor al valor experimental, la prueba del \\(p\\)-valor debe ser unilateral sobre la cola izquierda. Esto significa que el \\(p\\)-valor estará dado por la función (CDF) como sigue:\n\n\\[p=P(x\\leq\\overline{x})=F(\\overline{x};\\mu,\\sigma)\\]\n\nSi en cambio, la hipótesis nula \\(H_0\\) es que la media poblacional es mayor al valor experimental, la prueba es sobre la cola derecha y el \\(p\\)- valor se establece como:\n\n\\[p=P(x\\geq\\overline{x})=1-F(\\overline{x};\\mu,\\sigma)\\]\nNótese que para este ejemplo, la prueba es sobre la cola izquierda. Veamos cómo realizar dicha prueba en Python:\n\n\nCode\n# Importemos los módulos necesarios:\nfrom scipy.stats import norm\n\n# Calculemos el p-valor usando la función CDF:   \np = 1-norm.cdf(media_muestral, media_poblacional, desv_std)\n\n# Comparemos el p-valor con la significancia estadística:\nif p &lt; significancia:\n print(\"La hipótesis nula H_0 se puede rechazar.\")\nelse:\n print(\"No se puede rechazar la hipótesis nula H_0.\")\n\n\nLa hipótesis nula H_0 se puede rechazar.\n\n\nPor lo tanto, la mejor decisión es rechazar la hipótesis nula, que decía que la media muestral surgió de forma aleatoria de una media poblacional menor a \\(10345\\). Con esto, podemos afirmar que es probable que la campaña de marketing tuvo algún efecto positivo en el valor total de las ventas y que este no surgió de manera aleatoria, algo que ya se podía intuir a partir del resultado mostrado en las gráficas del artículo anterior."
  },
  {
    "objectID": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#otras-formas-de-formular-la-hipótesis",
    "href": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#otras-formas-de-formular-la-hipótesis",
    "title": "Otros Conceptos de las Pruebas de Hipótesis I",
    "section": "",
    "text": "No solo se pueden formular las hipótesis nula y alternativa como vimos en los ejemplos de las pruebas de hipótesis, donde \\(H_1\\) es solo una estricta negación de \\(H_0\\). En cambio, si el contexto de la prueba lo requiere, podemos asumir que la opción alternativa es que el valor hipotético de la media es mayor o menor que el valor experimental. Así pues, asumamos que el valor obtenido experimentalmente de la media poblacional es \\(\\mu_0\\). Dicha formulación se puede hacer así:\n\nLa hipótesis nula será que, dadas \\(\\overline{x}\\) y \\(\\sigma\\), la media poblacional es igual o menor al valor obtenido experimentalmente:\n\n\\[H_0:\\mu\\leq\\mu_0\\]\n\nLa hipótesis alternativa será que, dadas \\(\\overline{x}\\) y \\(\\sigma\\), la media poblacional es estrictamente mayor que el valor obtenido experimentalmente:\n\n\\[H_1:\\mu&gt;\\mu_0\\]\nPor supuesto, dependiendo de lo que queramos hacer, la formulación de estas hipótesis puede cambiar, invirtiéndose, es decir, haciendo que la hipótesis nula sea que el valor obtenido experimentalmente es mayor o igual a \\(\\mu\\) y que la alternativa sea que el valor de \\(\\mu\\) es estrictamente menor. Por las características de estas formas alternativas para las hipótesis, en la distribución de probabilidad solo nos interesarán resultados referentes al lado derecho o izquierdo, por lo que este tipo de pruebas entran en la clasificación de las pruebas de hipótesis Unilaterales.\n\n\nTomemos el mismo ejemplo base de los tipos de prueba de hipótesis, solo que en esta ocasión queremos ver si la campaña de marketing mencionada tuvo un efecto estrictamente positivo en el valor de las ventas. Esto significa que solo nos interesa la cola derecha de la distribución de probabilidad. Recordemos que en este ejemplo el valor de la media poblacional es \\(\\mu_0=10345\\), la desviación estándar poblacional es \\(\\sigma=552\\) y la media muestral es \\(\\overline{x}=11641\\). Lo que va a variar es la formulación de nuestra hipótesis, las cuales serán:\n\\[H_0:\\mu\\leq10345\\]\n\\[H_1:\\mu&gt;10345\\]\n\n\nCode\n# Establecemos los parámetros:\nmedia_poblacional = 10345\nmedia_muestral = 11641\ndesv_std = 552\n\n# Junto con la significancia:\nsignificancia = 0.05\n\n\nAhora bien, todos los métodos que ya vimos para realizar pruebas de hipótesis centradas en la media poblacional con distribución normal pueden ser aplicados a esta nueva formulación de hipótesis. Para este caso, utilizaremos el mismo nivel de significancia que en el artículo anterior \\((5\\%)\\) junto con el método del \\(p\\)-valor.\nPara ello, basta con calcular la probabilidad de que la media muestral sea menor o igual al valor experimental con una distribución normal de parámetros \\(\\mu=10345\\) y \\(\\sigma=552\\). Esto nos dará directamente el \\(p\\)-valor que, si es menor que la significancia estadística, se rechazará la hipótesis nula y, si en cambio es mayor que la significancia, entonces se aceptará.\n\nSi nuestra hipótesis nula \\(H_0\\) es que la media poblacional es menor al valor experimental, la prueba del \\(p\\)-valor debe ser unilateral sobre la cola izquierda. Esto significa que el \\(p\\)-valor estará dado por la función (CDF) como sigue:\n\n\\[p=P(x\\leq\\overline{x})=F(\\overline{x};\\mu,\\sigma)\\]\n\nSi en cambio, la hipótesis nula \\(H_0\\) es que la media poblacional es mayor al valor experimental, la prueba es sobre la cola derecha y el \\(p\\)- valor se establece como:\n\n\\[p=P(x\\geq\\overline{x})=1-F(\\overline{x};\\mu,\\sigma)\\]\nNótese que para este ejemplo, la prueba es sobre la cola izquierda. Veamos cómo realizar dicha prueba en Python:\n\n\nCode\n# Importemos los módulos necesarios:\nfrom scipy.stats import norm\n\n# Calculemos el p-valor usando la función CDF:   \np = 1-norm.cdf(media_muestral, media_poblacional, desv_std)\n\n# Comparemos el p-valor con la significancia estadística:\nif p &lt; significancia:\n print(\"La hipótesis nula H_0 se puede rechazar.\")\nelse:\n print(\"No se puede rechazar la hipótesis nula H_0.\")\n\n\nLa hipótesis nula H_0 se puede rechazar.\n\n\nPor lo tanto, la mejor decisión es rechazar la hipótesis nula, que decía que la media muestral surgió de forma aleatoria de una media poblacional menor a \\(10345\\). Con esto, podemos afirmar que es probable que la campaña de marketing tuvo algún efecto positivo en el valor total de las ventas y que este no surgió de manera aleatoria, algo que ya se podía intuir a partir del resultado mostrado en las gráficas del artículo anterior."
  },
  {
    "objectID": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#unidad-tipificada-y-comparación-de-datos",
    "href": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#unidad-tipificada-y-comparación-de-datos",
    "title": "Otros Conceptos de las Pruebas de Hipótesis I",
    "section": "Unidad Tipificada y Comparación de Datos",
    "text": "Unidad Tipificada y Comparación de Datos\nEl último concepto que debemos revisar relacionado con las pruebas de hipótesis es el de las unidades tipificadas tambien conocidas como z-valores, que en el caso de tener datos con una distribución normal nos permitiran escalar nuestros datos a una Distribución Normal Estándar con media \\(\\mu=0\\) y desviación estándar \\(\\sigma=0\\). En otras palabras, los \\(z\\)-valores son un método de Normalización.\nLa normalización de los datos por unidad tipificada nos permite comparar diferentes muestras o poblaciones, esto al cambiar el marco de referencia, en el cual los datos estaran centrados en \\(0\\) alrededor del eje \\(y\\) y la distancia entre los datos estara medida en Unidades de Desviación Estándar (tambien llamado “sigmas”) de un valor con respecto a la media. Por ejemplo, si tenemos datos normalizamos y un valor \\(v\\) se encuentra a \\(-2\\) unidades de distancia de la media \\(\\mu=0\\) diremos que \\(z\\) se encuentra a \\(-2\\sigma\\) con respecto a \\(0\\).\nFormalmente sea \\(x\\in{\\overline{X}}\\) , donde la media y varianza de \\(\\overline{X}\\) son \\(\\mu\\) y \\(\\sigma\\) respectivamente. El valor estandarizado de \\(x\\) denotado como \\(z\\) esta dado como:\n\\[z=\\frac{x-\\mu}{\\sigma}\\]\nDe donde decimos que \\(x\\) esta en \\(z\\) sigma unidades. Además de ello tambien podremos realizar el proceso inverso, es decir, re-escalar la distribución normal estándar a una media \\(\\mu_0\\) y una desviación estándar \\(\\sigma_0\\) dadas, esto por medio de la sigiente fórmula\n\\[\\overline{x}_{\\text{CR}}=\\mu_0\\pm z\\sigma_0\\]\nComo mencionamos antes al normalizar en un único marco de referencia datos que pueden estar en diferentes proporciones o unidades de medida, podremos utilizar los \\(z\\)-valores de manera similar al siguiente ejemplo:\n\nEjemplo (Unidad Tipificada):\nUna empresa esta evaluando el rendimiento de \\(2\\) empleados distintos en \\(2\\) diferentes departamentos, el departamento de ventas y el departamento de marketing. La evaluación se basa en las puntuaciones de desempeño anual que han recibido los empleados de cada departamento del \\(1\\) al \\(100\\) y la empresa quiere determinar ¿cuál empleado ha obtenido la puntuación más destacada en comparación a sus respectivos departamentos?.\nEl empleado de ventas \\(v\\) tiene una puntuación de 85, mientras que la media de su departamento es de \\(\\mu_v=75\\), con una desviación estándar de \\(\\sigma_v=10\\). El empleado de marketing \\(m\\) tiene una puntuación de \\(90\\), mientras que la media de su departamento es de \\(\\mu_m=80\\) con una desviación estándar de \\(12\\).\nPara poder comparar ambos valores, estos deben estar en el mismo marco de referencia, lo implica utilizar la unidad tipificada, asi, calculando los \\(z\\)-valores respectivos obtendremos:\n\\[z_v=\\frac{x_v-\\mu_v}{\\sigma_v}=\\frac{85-75}{10}=1,\\]\n\\[z_m=\\frac{x_m-\\mu_m}{\\sigma_m}=\\frac{90-80}{12}=0.8334.\\]\nPor lo cual, según este criterio, el empleado del departamento de ventas es el que ha obtenido la mayor puntuación con respecto a su departamento."
  },
  {
    "objectID": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#errores-tipo-i-y-ii",
    "href": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#errores-tipo-i-y-ii",
    "title": "Otros Conceptos de las Pruebas de Hipótesis I",
    "section": "Errores Tipo I y II",
    "text": "Errores Tipo I y II\nComo ya mencionamos antes, en la introducción a las pruebas de hipótesis existen dos tipos de errores posibles que podemos cometer. El primero es rechazar la hipótesis nula siendo esta realmente verdadera. Este fenómeno se conoce como Error de Tipo I y la probabilidad de que suceda es equivalente a la Significancia Estadística y se denota por \\(\\alpha\\). Esto es:\n\\[\\alpha=P(\\text{error tipo II})=\\text{Significancia estadística}\\]\nEl otro posible error es aceptar la hipótesis nula cuando esta es, de hecho, falsa. En ese caso, ocurre un llamado Error de Tipo II. La probabilidad de este tipo de error, a la que denotaremos por la letra griega \\(\\beta\\), solo puede calcularse respecto a un valor específico cuando la hipótesis alternativa \\(H_1\\) posee un rango establecido.\n\nEjemplos (Errores I y II):\nYa sabemos que si la hipótesis alternativa solo nos indica que \\(H_1:\\mu\\neq\\mu_0\\) no podremos calcular una probabilidad \\(\\beta\\) para el error de Tipo II. También cabe mencionar que para el cálculo necesitaremos conocer el tamaño \\(n\\) de nuestra media muestral. Por lo anterior, tomaremos el mismo ejemplo de las formulaciones de hipótesis, de tal manera que nuestras hipótesis son:\n\\[H_0:\\mu\\leq10345\\]\n\\[H_1:\\mu&gt;10345\\]\nAdemás, asumiremos que el tamaño de nuestra muestra es \\(n=200\\). En ese caso, la probabilidad de cometer un error de Tipo I es simplemente la significancia estadística:\n\\[\\alpha=5\\%\\]\nEn cambio, para calcular la probabilidad \\(\\beta\\), al ya haber formulado la hipótesis, lo segundo que debemos hacer es hallar la desviación estándar de la muestra, lo cual podremos hacer conociendo la desviación estándar de la población \\(\\sigma=552\\) y el tamaño de la muestra \\(n=200\\) mediante la siguiente fórmula:\n\\[\\sigma_{\\overline{x}}=\\frac{\\sigma}{\\sqrt{n}}\\]\nPodemos computar esto así:\n\n\nCode\n# Exportemos directamente la funcionalidad de raíz cuadrada:\nfrom math import sqrt\n\n# Establecemos el tamaño de la muestra como parámetro:\nn = 200\n\n# Realizamos el cálculo:\ndesv_std_mue = 552/sqrt(200)\nprint(f'la desviación estándar de la muestra es {round(desv_std_mue,4)}')\n\n\nla desviación estándar de la muestra es 39.0323\n\n\nEl tercer paso es calcular el punto crítico correspondiente a tres datos: la desviación muestral que acabamos de calcular \\(\\sigma_{\\overline{x}}\\), la media poblacional \\(\\mu_0\\), y el valor \\(z\\) respectivo de la significancia estadística al \\(5\\%\\) en pruebas unilaterales por la cola derecha, es decir, \\(z=+1.645\\) (véase la tabla de la significancia en pruebas de hipótesis). Esto se hace por medio del reescalado que vimos en la parte de los valores \\(z\\):\n\\[\\overline{x}_{\\text{CR}}=\\mu_0+z\\sigma_{\\overline{x}}.\\]\nDeterminemos su valor:\n\n\nCode\n# Calculamos el valor crítico:\ncritico = media_poblacional+(1.645*desv_std_mue)\nprint(f\"el valor crítico es de {round(critico,4)}\")\n\n\nel valor crítico es de 10409.2081\n\n\nCon ello, el cuarto paso es normalizar dicho punto crítico a una distribución normal estándar por medio del valor \\(z\\) espectivo al punto crítico con respecto a los datos (media y desviación) muestrales:\n\\[z_{\\beta}=\\frac{\\overline{x}_{\\text{CR}}-\\overline{x}}{\\sigma_{\\overline{x}}}.\\]\nobtengamos dicho zeta valor:\n\n\nCode\n# Calculamos el z-valor:\nz_b = (critico-media_muestral)/desv_std_mue\nprint(f\"El z-valor para determinar la probabilidad de el error tipo II es {round(z_b,4)}\")\n\n\nEl z-valor para determinar la probabilidad de el error tipo II es -31.5583\n\n\nEl paso final será usar la función \\(F\\) (CDF) para calcular la probabilidad \\(\\beta\\) sobre la distribución normal estándar por medio de la probabilidad complementaria, o lo que es igual:\n\\[\\beta=P(\\text{error tipo II})=P(z_{\\beta}&lt;z)=1-F(z_{\\beta};0,1).\\]\n\n\nCode\n# Finalmente calculamos la probabilidad del error tipo II:\nbeta = 1-norm.cdf(z_b,0,1)\nprint(f\"La probabilidad de cometer un error de tipo II si aceptamos la hipótesis nula es aproximadamente de {round(beta*100,4)}%.\")\n\n\nLa probabilidad de cometer un error de tipo II si aceptamos la hipótesis nula es aproximadamente de 100.0%.\n\n\nComo puede ver, ese valor es prácticamente \\(1\\), lo cual tiene sentido, pues el error de Tipo II solo ocurre cuando aceptamos la hipótesis nula, y por el ejemplo del principio sabemos que lo más probable es rechazar dicha hipótesis."
  },
  {
    "objectID": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#referencias",
    "href": "posts/otros-conceptos-de-pruebas-de-hipotesis.html#referencias",
    "title": "Otros Conceptos de las Pruebas de Hipótesis I",
    "section": "Referencias",
    "text": "Referencias\n\nDevore, Jay L. Probabilidad y estadística para ingeniería y ciencias. Novena edición, Pearson, 2011.\nNield, Thomas.(2020).Essential Math for Data Science. O’Reilly Media."
  },
  {
    "objectID": "posts/distribuciones-de-probabilidad.html",
    "href": "posts/distribuciones-de-probabilidad.html",
    "title": "Fundamentos de las Distribuciones de Probabilidad",
    "section": "",
    "text": "imagen"
  },
  {
    "objectID": "posts/distribuciones-de-probabilidad.html#distribuciones-discretas-y-continuas.",
    "href": "posts/distribuciones-de-probabilidad.html#distribuciones-discretas-y-continuas.",
    "title": "Fundamentos de las Distribuciones de Probabilidad",
    "section": "Distribuciones Discretas y Continuas.",
    "text": "Distribuciones Discretas y Continuas.\nEn probabilidad, el espacio muestral será discreto si es un conjunto contable. Esto quiere decir que puedo “listar” los elementos de \\(X\\) y asignarles a cada uno un número natural. La distribución de probabilidad de dichos espacios muestrales debe satisfacer las siguientes condiciones:\n\nLa función de densidad de probabilidad nunca es negativa: \\[ f(x)\\geq 0, \\ \\forall{x\\in{X}}. \\]\nLa suma de todas las probabilidades es \\(1\\) (o sea un \\(100\\%\\)): \\[\\sum_{X} f(x)=1.\\]\nLa probabilidad de que un evento \\(x\\) suceda es igual a la función aplicada en este: \\[P(x=X)=f(x).\\]\n\nSi la función \\(f\\) cumple con estas condiciones, el conjunto de sus imágenes son una Distribución de Probabilidad Discreta. Estas distribuciones se suelen representar gráficamente a través de historigramas o similares.\nPor otro lado, el espacio muestral será continuo si existe una función biyectiva entre este y algún intervalo real (por ejemplo el intervalo \\([0,1]\\subset{\\Re}\\), lo que quiere decir que sus elementos son equivalentes a los puntos de un segmento de una línea.\nSus distribuciones de probabilidad respectivas deben satisfacer las siguientes condiciones:\n\nLa función de densidad de probabilidad nunca es negativa sobre la recta real: \\[f(x) \\geq 0, \\ \\forall{x \\in \\mathbb{R}}.\\]\nLa integral de la función sobre la recta real es \\(1\\): \\[\\int_{-\\infty}^{\\infty} f(x) dx = 1.\\]\nLa probabilidad de un intervalo de eventos es igual a la integral de la función: \\[P(a &lt; x &lt; b) = \\int_{a}^{b} f(x) dx.\\]\n\nNótese que esta última condición implica necesariamente que en un espacio muestral continuo no se puede calcular la probabilidad de un evento \\(x\\) en particular, sino solamente en subintervalos de eventos.\nSi la función \\(f\\) cumple con dichas condiciones, el conjunto de las imágenes \\(f(x)\\) se conocera como Distribución de Probabilidad Continua. Evidentemente, y dada su naturaleza, dichas funciones son del espacio muestral a los reales \\(f:X\\longrightarrow\\Re\\) y se representan gráficamente con curvas sobre el plano cartesiano.\nPuede pensarse que en estadística lo más común es utilizar distribuciones discretas. Sin embargo, muchas veces ocurre que la cantidad de datos es tan grande que resulta más útil trabajar con distribuciones continuas. Además, esto también depende del tipo de datos: si estos se refieren a medidas espaciales o físicas en general, se suelen usar también distribuciones continuas, sin mencionar casos especiales como la Distribución Normal. En resumen, en estadística el tipo de distribución que utilicemos depende más de los propios datos del caso particular y de lo que busquemos hacer.\nNaturalmente, cabe preguntarse: ¿qué pasaría si nuestras probabilidades no dependieran únicamente de una variable, sino más bien de múltiples? La respuesta a esto es bastante simple y la expondré a continuación."
  },
  {
    "objectID": "posts/distribuciones-de-probabilidad.html#distribuciones-de-probabilidad-multivariada.",
    "href": "posts/distribuciones-de-probabilidad.html#distribuciones-de-probabilidad-multivariada.",
    "title": "Fundamentos de las Distribuciones de Probabilidad",
    "section": "Distribuciones de Probabilidad Multivariada.",
    "text": "Distribuciones de Probabilidad Multivariada.\nAhora, en vez de tener únicamente un espacio muestral \\(X\\), supondremos que existen \\(n\\) espacios muestrales, a los cuales denotaremos como \\(X_1, X_2, ..., X_1\\). En ese caso, la distinción entre distribuciones discretas y continuas se mantendrá, y realmente lo único que cambiará serán las condiciones necesarias que deben satisfacer ambos tipos de distribución.\nPara las distribuciones de probabilidad multivariada la función PDF será una función multivariada del producto cartesiano de los múltiples espacios muestrales al intervalo de \\(0\\) a \\(1\\), es decir, \\(f: X_1\\times X_2\\times ...\\times X_n \\longrightarrow (0,1)\\).\nEn el caso de ser todos los \\(X_k\\) discretos se deberan cumplir las siguientes condiciones:\n\nLa función de densidad debe seguir siendo no negativa:\n\n\\[f(x_1,x_2,...,x_n)\\geq 0;\\]\n\\[\\forall{(x_1,x_2,...,x_n)}\\in{X_1\\times X_2\\times ... \\times X_n}.\\]\n\nLógicamente, la suma de todas las probabilidades debe ser igual a \\(1\\):\n\n\\[\\sum_{X_1}\\sum_{X_2}...\\sum_{X_n} f(x_1,x_2,x_3,...,x_n)=1.\\]\n\nComo mencionamos antes, la probabilidad específica dependera de cada una de los valores de las variables: \\[P(X_1=x_1,X_2=x_2,...,X_n=x_n)=\\] \\[f(x_1,x_2,...,x_n).\\]\n\nY en el caso de las Distribuciones de ser continuas, las condiciones serán las siguentes:\n\nLa función de densidad debe yacer o estar por encima del hiperplano nulo:\n\n\\[f(x_1,x_2,...,x_n)\\geq 0;\\]\n\\[\\forall{(x_1,x_2,...,x_n)}\\in{\\Re^n}.\\]\n\nLa integral múltiple de la función evaluada en todo el espacio muestral es \\(1\\):\n\n\\[\\int_{-\\infty}^{\\infty}...\\int_{-\\infty}^{\\infty}f(x_1,...,x_n)dx_1 ...dx_n.\\]\n\nLa probabilidad solo se obtendra de un subconjunto del producto cartesiano de los espacios muestrales:\n\n\\[P[(x_1,x_2,...,x_n)\\in{A}]=\\] \\[\\idotsint_{A} f(x_1,x_2,...,x_n) dx_1dx_2...dx_n;\\] \\[\\forall{A}\\subset{X_1\\times X_2\\times ... \\times X_n}.\\]\nComo se puede observar, a diferencia de las funciones de densidad de probabilidad univariadas, en vez de trabajar sobre rectas o planos cartesianos, en las distribuciones multivariadas tratamos con subespacios de \\(\\Re^n\\).\nTambién se puede notar que el cálculo de estas funciones de probabilidad utiliza sumatorias o integrales múltiples en cada caso, con lo cual deberemos implementar funciones algo distintas (vectoriales) en vez de las más comunes funciones reales. De nuevo, esto dependerá de las variables presentes en nuestros datos, cómo se relacionan entre ellas y qué información queramos obtener de los mismos.\nPor ahora, trabajaremos solamente con funciones de densidad de probabilidad univariadas, más específicamente con la distribución de probabilidad normal. Esto lo haremos por una serie de razones que trataré en un futuro artículo. Además, en los últimos artículos no he agregado ejemplos porque hasta ahora solo he definido conceptos. Cuando hablemos de la distribución normal, podré agregar más ejemplos para que estos conceptos se entiendan mejor."
  },
  {
    "objectID": "posts/distribuciones-de-probabilidad.html#referencias.",
    "href": "posts/distribuciones-de-probabilidad.html#referencias.",
    "title": "Fundamentos de las Distribuciones de Probabilidad",
    "section": "Referencias.",
    "text": "Referencias.\n\nDevore, Jay L. Probabilidad y estadística para ingeniería y ciencias. Novena edición, Pearson, 2011."
  },
  {
    "objectID": "posts/conceptos-basicos-para-las-pruebas-de-hipotesis.html",
    "href": "posts/conceptos-basicos-para-las-pruebas-de-hipotesis.html",
    "title": "Conceptos Básicos de las Pruebas de Hipótesis.",
    "section": "",
    "text": "imagen"
  },
  {
    "objectID": "posts/conceptos-basicos-para-las-pruebas-de-hipotesis.html#referencias.",
    "href": "posts/conceptos-basicos-para-las-pruebas-de-hipotesis.html#referencias.",
    "title": "Conceptos Básicos de las Pruebas de Hipótesis.",
    "section": "Referencias.",
    "text": "Referencias.\n\nJ.Kazmier, Leonard.(1998).Estadística Aplicada a la Administración y a la Economía.McGRAW-HILL.\nNield, Thomas.(2020).Essential Math for Data Science.O’Reilly Media."
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "Lista de Artículos",
    "section": "",
    "text": "Aquí encontraras todos los artículos relacionados con estadística y ciencia de datos que se han publicado en este blog hasta la fecha.\n\n\n\nConceptos Básicos de las Pruebas de Hipótesis\n\n\nExplora los fundamentos de las pruebas de hipótesis en estadística, la formulación de hipótesis nula y alternativa hasta la comprensión de la significancia estadística y los errores de tipo I y II… Leer más\n\n\n\n\n\nFundamentos de las Distribuciones de Probabilidad\n\n\nCubre variables aleatorias, espacios muestrales, distribuciones discretas y continuas, y distribuciones de probabilidad multivariadas. El artículo destaca las condiciones clave para cada tipo de distribución y su aplicación en el análisis estadístico… Leer más\n\n\n\n\n\nLa Distribución Normal o Gaussiana\n\n\nEn este artículo, exploramos la distribución normal, desde su función de distribución acumulada (CDF) hasta la función de punto de probabilidad (PPF). Además, examinamos el teorema central del límite (TCL), revelando su amplio impacto en diversos campos. Este análisis resalta la omnipresencia y la importancia de la distribución normal en estadística y probabilidad… Leer más\n\n\n\n\n\nTipos de Pruebas de Hipótesis:\n\n\nEn este artículo, hablo sobre los métodos para realizar pruebas de hipótesis relativas a la media poblacional para datos que siguen una distribución normal. Los tres métodos más comunes son: valores críticos, el valor \\(p\\), los intervalos de confianza y su versión normalizada…. Leer más\n\n\n\n\n\nOtros Conceptos de las Pruebas de Hipótesis I:\n\n\nEn este artículo complementario, explico otras formas de formular las hipótesis, la unidad tipificada o valores \\(z\\), y cómo y en qué condiciones se puede calcular la probabilidad de los errores de tipo I y II en dichas pruebas…. Leer más\n\n\n\n\n\nIntroducción a las Matemáticas Financieras:\n\n\nEl artículo explica la importancia de las matemáticas financieras en la asignación de recursos, se centra en el valor del dinero en el tiempo. Introduce el concepto de interés y sus causas, incluyendo la inflación, el costo de oportunidad y la preferencia temporal, y presenta la tasa de interés y su cálculo….. Leer más\n\n\n\n\n\nAnálisis de Decisión I (Tablas de Pago y Árboles de Decisión):\n\n\nEn este artículo trato la primera parte del análisis de decisión, abarcando tanto las tablas de pago con sus criterios de decisión basados en precios y probabilidades, como los árboles de decisión. También se incluye el uso de estos en clasificación, regresión y bosques aleatorios…… Leer más\n\n\n&lt;div clas=\"article-item\"&gt;\n\n\nTeoría del Interés I:\n\n\nEn este artículo, trato las aplicaciones del valor del dinero en el tiempo para una tasa de interés fija. Abordo la función de acumulación, el interés simple, el interés compuesto y el valor presente para ambos tipos…… Leer más"
  },
  {
    "objectID": "annexes/anexo-varianza-y-media-normal.html",
    "href": "annexes/anexo-varianza-y-media-normal.html",
    "title": "Anexo: Demostración de la Media y Varianza Normal",
    "section": "",
    "text": "Para ambas demostraciones necesitaremos el hecho de que la distribucion normal es una distribución de probabilidad continua, por lo que la función de densidad de probabilidad (PDF) de la distribución normal (denotada por \\(f(x;\\mu,\\sigma)\\)) debe satisfacer la condición dos de dichas distribuciones y por tanto se cumple la siguiente fórmula:\n\\[1=\\int_{-\\infty}^{+\\infty}f(x;0,1)dx=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}e^{-\\frac{x^2}{2}}dx\\]\n\\[\n\\Rightarrow \\int_{-\\infty}^{+\\infty} e^{-\\frac{x^2}{2}}dx=\\sqrt{2\\pi}.  \\ (1)\n\\]\nTeniendo encuenta lo anterior, podemos empezar."
  },
  {
    "objectID": "annexes/anexo-varianza-y-media-normal.html#demostración-de-la-media-normal",
    "href": "annexes/anexo-varianza-y-media-normal.html#demostración-de-la-media-normal",
    "title": "Anexo: Demostración de la Media y Varianza Normal",
    "section": "Demostración de la Media Normal:",
    "text": "Demostración de la Media Normal:\nProbemos que \\(\\mu_0\\) es la media de la distribución normal. Sabemos que para toda distribución de probabilidad continua la media se define como\n\\[\\mu=E(x)=\\int_{-\\infty}^{+\\infty}xf(x)dx\\]\nPara evitar confuciones diremos que\n\\[f(x;\\mu_0,\\sigma)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu_0}{\\sigma}) ^2}dx\\]\nPor lo que bastara con probar que \\(\\mu_0=E(x)\\), como sigue:\n\\[\n\\begin{align}\nE(x) &= \\int_{-\\infty}^{+\\infty}xf(x;\\mu_0,\\sigma)dx\n     &= \\int_{-\\infty}^{+\\infty}\\frac{x}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu_0}{\\sigma})^2}dx\n\\end{align}\n\\]\nAhora realizamos la sustitución \\(z=\\frac{x-\\mu_0}{\\sigma}\\) (normalización por medio del \\(z\\)-valor) lo cual implica que \\(\\sigma dz=dx\\), resultando en que\n\\[\nE(x) = \\int_{-\\infty}^{+\\infty}\\frac{(\\sigma z+\\mu_0)}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}(\\sigma)dz\n\\]\n\\[\n     = \\frac{1}{\\sqrt{2\\pi}}[\\sigma\\int_{-\\infty}^{+\\infty}ze^{-\\frac{z^2}{2}}dz+\\mu_0\\int_{-\\infty}^{+\\infty}e^{-\\frac{z^2}{2}}dz]\n\\]\nPor la fórmula \\((1)\\) obtendremos lo siguiente\n\\[E(x)=\\frac{1}{\\sqrt{2\\pi}}[\\sigma\\int_{-\\infty}^{+\\infty}ze^{-\\frac{z^2}{2}}dz+\\mu_0(\\sqrt{2\\pi})]\\]\nPodemos solucionar esa última integral por medio de una nueva sustitución, donde \\(u=-\\frac{z^2}{2}\\) y por tanto \\(-du=zdz\\), de esta forma\n\\[\\int_{-\\infty}^{+\\infty}ze^{-\\frac{z^2}{2}}dz = \\]\n\\[-\\int_{+\\infty}^{-\\infty}e^udu = \\]\n\\[\\int_{-\\infty}^{+\\infty}e^udu = \\]\n\\[\\lim_{z\\to-\\infty}e^{-\\frac{z^2}{2}}-\\lim_{z\\to+\\infty}e^{-\\frac{z^2}{2}} = \\]\n\\[\\lim_{z\\to-\\infty}\\frac{1}{e^{\\frac{z^2}{2}}}-\\lim_{z\\to+\\infty}\\frac{1}{e^{\\frac{z^2}{2}}} = 0.\\]\n\\[\\Rightarrow\\int_{-\\infty}^{+\\infty}ze^{-\\frac{z^2}{2}}dz = 0. \\ (2)\\]\nPor lo cual obtendremos finalmente el resultado deseado\n\\[\nE(x)=\\frac{1}{\\sqrt{2\\pi}}[\\sigma(0)+\\mu_0(\\sqrt{2\\pi})]=\\mu_0.\n\\]\nQ.E.D."
  },
  {
    "objectID": "annexes/anexo-varianza-y-media-normal.html#demostración-de-la-varianza-normal",
    "href": "annexes/anexo-varianza-y-media-normal.html#demostración-de-la-varianza-normal",
    "title": "Anexo: Demostración de la Media y Varianza Normal",
    "section": "Demostración de la Varianza Normal",
    "text": "Demostración de la Varianza Normal\nAhora probemos que \\(\\sigma_{0}^{2}\\) es la varianza de la distribución normal. Sabemos que para toda distribución de probabilidad continua la varianza se define como\n\\[\\sigma^2=E[(x-\\mu)^2]=\\int_{-\\infty}^{+\\infty}(x-\\mu)^2f(x)dx\\]\nPara evitar confuciones diremos que\n\\[f(x;\\mu,\\sigma_0)=\\frac{1}{\\sigma_0\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma_0})^2}\\]\nPor lo que bastara con probar que \\(E[(x-\\mu)^2]=\\sigma_{0}^{2}\\), como se hace acontinuación\n\\[\nE[(x-\\mu)^2] = \\int_{-\\infty}^{+\\infty}(x-\\mu)^2f(x;\\mu,\\sigma_0)dx\n\\]\n\\[\n             = \\int_{-\\infty}^{+\\infty}\\frac{(x-\\mu)^2}{\\sigma_0\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma_0})^2}dx\n\\]\nDe nuevo, sustituyendo por la normalización \\(z=\\frac{x-\\mu}{\\sigma_0}\\) en donde \\(\\sigma_0dz=dx\\), obtendremos\n\\[\n\\begin{align}\nE[(x-\\mu)^2] &= \\int_{-\\infty}^{+\\infty}\\frac{(\\sigma_0z)^2}{\\sigma_0\\sqrt{2\\pi}}e^{-\\frac{z^2}{2}}\\sigma_0dz\n               &= \\frac{\\sigma_{0}^{2}}{\\sqrt{2\\pi}}\\int_{-\\infty}^{+\\infty}z^2e^{-\\frac{z^2}{2}}dz\n\\end{align}\n\\]\nUtilizando el método de sustitución por partes, donde tomamos \\(u=z\\) y \\(dv=ze^{-\\frac{z^2}{2}}\\) tendremos la siguiente igualdad\n\\[E[(x-\\mu)^2]=\\frac{\\sigma_{0}^{2}}{\\sqrt{2\\pi}}[-\\int_{-\\infty}^{+\\infty}ze^{-\\frac{z^2}{2}}dz+\\int_{-\\infty}^{+\\infty}e^{-\\frac{z^2}{2}}dz]\\]\nY finalmente, debido a las igualdades \\((1)\\) y \\((2)\\) tendremos que\n\\[\nE[(x-\\mu)^2]=\\frac{\\sigma_{0}^{2}}{\\sqrt{2\\pi}}(0+\\sqrt{2\\pi})=\\sigma_{0}^{2}.\n\\]\nPor lo que queda probado."
  },
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Sobre el Autor",
    "section": "",
    "text": "Soy un matemático apasionado y altamente motivado con una sólida formación académica. Me considero una persona responsable y estoy dispuesto a utilizar mis conocimientos en matemáticas, así como distintas herramientas de software, para adaptarme y cumplir diferentes objetivos de trabajo. Además, poseo un conjunto de habilidades técnicas y teóricas robustas que me respaldan para abordar desafíos analíticos complejos mediante el uso de programas informáticos.\n\n\n\n\n\nCel. (+57) 3204940335\nEmail. juaneschavez31@gmail.com"
  },
  {
    "objectID": "about_me.html#contactame",
    "href": "about_me.html#contactame",
    "title": "Sobre el Autor",
    "section": "",
    "text": "Cel. (+57) 3204940335\nEmail. juaneschavez31@gmail.com"
  },
  {
    "objectID": "annexes.html",
    "href": "annexes.html",
    "title": "Lista de Anexos",
    "section": "",
    "text": "En esta sección encontrarán contenido adicional a los proyectos y artículos. Estos no están pensados para introducir o explicar un tema en particular, ni para desarrollar un trabajo. Están menos pulidos que las demás secciones y están destinados a aquellos que ya tienen conocimientos en matemáticas y están interesados en demostraciones y otros temas algo más técnicos.\n\n\nAnexo: Demostración de la Muestra y Varianza Normal\n\n\nDemostración de la media y la varianza de la distribución normal son los respectivos parámetros, a partir de la sus definiciones dadas por la esperanza matemática. Ver anexo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "¡Bienvenido a Estadística en Acción!",
    "section": "",
    "text": "Este es un blog personal dedicado a quienes deseen explorar las numerosas aplicaciones de la matemática, la estadística y la ciencia de datos. A través de esta página, publicaré artículos relacionados con estos temas y también compartiré algunos de mis proyectos personales.\nBasado fundamentalmente en Jupyter Lab, en este espacio no solo expondré temas, métodos y modelos teóricos con ejemplos, sino que también mostraré implementaciones computacionales principalmente en R, MariaDB y Python.\nEspero les guste."
  },
  {
    "objectID": "index.html#artículo-más-reciente",
    "href": "index.html#artículo-más-reciente",
    "title": "¡Bienvenido a Estadística en Acción!",
    "section": "Artículo más Reciente",
    "text": "Artículo más Reciente\n\n\n\nTeoría del Interés I:\n\n\nEn este artículo, trato las aplicaciones del valor del dinero en el tiempo para una tasa de interés fija. Abordo la función de acumulación, el interés simple, el interés compuesto y el valor presente para ambos tipos…… Leer más"
  },
  {
    "objectID": "index.html#proyecto-más-reciente",
    "href": "index.html#proyecto-más-reciente",
    "title": "¡Bienvenido a Estadística en Acción!",
    "section": "Proyecto más Reciente:",
    "text": "Proyecto más Reciente:\n\n\n\nPredicción de Precios utilizando Bosque Aleatorio:\n\n\nEn este proyecto, buscamos construir un modelo de pronóstico para los precios de venta de autos en un concesionario de la India, a partir de otros datos del concesionario y de los propios autos. Dicho modelo se basa en una regresión de bosque aleatorio, que es un bagging o bootstrap del método de regresión de árboles de decisión. Haz click aquí para ver el proyecto"
  },
  {
    "objectID": "index.html#lista-de-anexos",
    "href": "index.html#lista-de-anexos",
    "title": "¡Bienvenido a Estadística en Acción!",
    "section": "Lista de Anexos:",
    "text": "Lista de Anexos:\n\n\nAnexo: Demostración de la Muestra y Varianza Normal\n\n\nDemostración de la media y la varianza de la distribución normal son los respectivos parámetros, a partir de la sus definiciones dadas por la esperanza matemática. Ver anexo"
  },
  {
    "objectID": "posts/distribucion-normal.html",
    "href": "posts/distribucion-normal.html",
    "title": "La Distribución Normal",
    "section": "",
    "text": "imagen"
  },
  {
    "objectID": "posts/distribucion-normal.html#cdf-y-ppf-de-la-distribución-normal.",
    "href": "posts/distribucion-normal.html#cdf-y-ppf-de-la-distribución-normal.",
    "title": "La Distribución Normal",
    "section": "CDF y PPF de la Distribución Normal.",
    "text": "CDF y PPF de la Distribución Normal.\nYa conociendo que es la distribución Gaussiana lo siguiente que podriamos preguntarnos ¿cómo utilizarla?, para ello, introduciremos \\(2\\) funciones que se definen para todas las distribuciones de probabilidad:\nLa primera es la Función de Distribución Acumulativa (CDF) la cual se utiliza para calcular la probabilidad correspondiente a un intervalo dado (recordemos que la normal es una distribución continua), dicha función se denota por \\(F\\) y es expresa como sigue:\n\\[\n\\begin{align}\nP(a&lt;X&lt;b) &=  F(x;a,b) \\\\\n         &=  \\int_{a}^{b}f(x)dx \\\\\n         &=  \\int_{a}^{b}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-1}{2}(\\frac{x-\\mu}{\\sigma})^2}dx.\n\\end{align}\n\\]\nTristemente, la función \\(f\\) no posee una función antiderivada elemental como tal, asi que deberemos aproximarla, bien por métodos numéricos (a travez de la Función Error de Gauss \\(\\DeclareMathOperator\\erf{erf} \\erf(x)\\) y su expanción en Serie de Taylor) o, de manera más práctica, por medios computacionales. Al ser una integral, la función CDF podra ser interpretada como el área bajo la curva de \\(f\\) y sobre el intervalo \\((a,b)\\).\n\n\nCode\n# Crear la gráfica (esto se facilitara por el código anterior):\nplt.figure(figsize=(10, 6))\n\n# Utilizar Seaborn para hacer la gráfica más estética:\nsns.lineplot(x=x, y=f_x, color='blue')\n\n# Denotar la curva como f(x):\nplt.plot(x, f_x, label='$f(x)$')\n\n# Definir los límites de la región a sombrear:\na, b = -0.5, 1\n\n# Sombrear la región bajo la curva entre a y b\nx_fill = np.linspace(a, b, 1000)\ny_fill = norm.pdf(x_fill, mu, sigma)\nplt.fill_between(x_fill, y_fill, color='cyan', alpha=0.5, label=r'$F(x;a,b)=P(a&lt;X&lt;b)$')\n\n# Añadir las etiquetas y la leyenda\nplt.legend()\n\n# Añadir títulos y etiquetas de los ejes\nplt.title('Distribución Normal con $\\mu = 1$ y $\\sigma = 0.5$')\nplt.xlabel('x')\nplt.ylabel('$f(x)$')\n\n# Mostrar la gráfica\nplt.show()\n\n\n\n\n\n\n\n\n\nLa segunda, se trata de La Función Punto de Probabilidad (PPF), determina el valor \\(x_0\\) correspondiente a una probabilidad dada \\(P\\). En otras palabras, para una probabilidad \\(P\\) específica, la PPF encuentra el valor \\(x_0\\) tal que el área bajo la curva de la función de densidad de probabilidad en el intervalo \\((-\\infty,x_0)\\) es igual a \\(P\\). Esto es más comprensible gráficamente:\n\n\nCode\n# Crear la gráfica\nplt.figure(figsize=(10, 6))\n\n# Utilizar Seaborn para hacer la gráfica más estética\nsns.lineplot(x=x, y=f_x, color='blue')\n\n# Denotar la curva como f(x)\nplt.plot(x, f_x, label='$f(x)$')\n\n# Definir los límites de la región a sombrear\na, b = -0.5, 0.75\n\n# Sombrear la región bajo la curva entre a y b\nx_fill = np.linspace(a, b, 1000)\ny_fill = norm.pdf(x_fill, mu, sigma)\nplt.fill_between(x_fill, y_fill, color='cyan', alpha=0.5, label=r'$P$')\n\n# Añadir anotación en el punto x=b (denotado como x0)\nplt.axvline(b, color='red', linestyle='--', ymax=norm.pdf(b, mu, sigma)/max(f_x), label='$x_0$')\nplt.text(b, -0.02, '$x_0$', fontsize=12, color='red', ha='center')\n\n# Añadir las etiquetas y la leyenda\nplt.legend()\n\n# Añadir títulos y etiquetas de los ejes\nplt.title('Distribución Normal con $\\mu = 1$ y $\\sigma = 0.5$')\nplt.xlabel('x')\nplt.ylabel('$f(x)$')\n\n# Mostrar la gráfica\nplt.show()\n\n\n\n\n\n\n\n\n\nNaturalmente al utilizar de entrada la probabilidad y de salida intervalos, se puede afirmar que la función punto de probabilidad PPF se corresponde con la inversa de la función de distribución acumulativa CDF:\n\\[F^{-1}[P(X&lt;x_0)]=x_0\\]\nEl problema claro esta, es al momento de calcular intervalos en \\(2\\) partes separadas o en un intervalo que no parta desde \\(-\\infty\\), pero esto se puede hacer fácilmente restando, sumando o dividiendo áreas como mostramos en el artículo de Pruebas de Hipótesis por medio del Valor Crítico."
  },
  {
    "objectID": "posts/distribucion-normal.html#por-qué-elegir-la-distribución-normal.",
    "href": "posts/distribucion-normal.html#por-qué-elegir-la-distribución-normal.",
    "title": "La Distribución Normal",
    "section": "¿Por qué elegir la Distribución Normal?.",
    "text": "¿Por qué elegir la Distribución Normal?.\nComo mencionamos antes, esta distribución se llama “normal” por lo común que es, al emerger en forma aproximada muchos fenómenos que ocurren en la naturaleza la industria y la investigación. Hay una serie de razones que podemos listar por las cuales esto ocurre:\n\nSe adapta bien a datos de los cuales la mayoría se acercan a la tendencia central (comunmente el promedio) y muy pocas son extremadamente altas o bajas (lo que se suele llamar valores atípicos).\nA pesar de ser una distribución para variables aletorias continuas esta es capaz de aproximar las distribuciones discretas más usadas tales como la distribución binomial y distribución de poisson.\nLas distribuciones estadísticas como la media muestral y la proporción muestral se aproximan bien a la distribución normal cuando el tamaño de la muestra es grande, independientemente de como se distribuye la poblacion de origen.\n\neste último punto es el mas importante y se debe a un teorema estadístico demasiado importante como para no mencionar.\n\nEl Teorema Central de Límite.\nEl enunciado formal de este teorema es el siguiente:\nSi \\(\\overline{X}\\) es la media de una muestra elatoria de tamaño \\(n\\), tomada de una población con media \\(\\mu\\) y varianza \\(\\sigma^2\\), entonces la forma del límite de la distribución de \\[Z=\\frac{\\overline{X}-\\mu}{\\sigma/\\sqrt{n}},\\] a medida que \\(n\\rightarrow\\infty\\), es la Distribución Normal Estándar \\(f(z;0,1)\\).\nEste teorema implica que la distribución de muestreo de la media tendrá (o al menos se aproximara a) una distribución normal, independientemente de la forma de la distribución de la población de la que fue tomada la muestra. Para efectos prácticos, puede suponerse esto puede suponerse siempre que el tamaño de la muestra sea \\(n\\geq30\\) (lo cual concordemos se cumple en la mayoría de muestras que tomamos en la vida real)."
  },
  {
    "objectID": "posts/distribucion-normal.html#ejemplo.",
    "href": "posts/distribucion-normal.html#ejemplo.",
    "title": "La Distribución Normal",
    "section": "Ejemplo.",
    "text": "Ejemplo.\nUno de los ejemplos más clásicos del uso de la distribución normal es en el estudio de la estatura de las personas. Imaginemos un ejemplo en el cual de una ciudad de \\(2\\) millones de habitantes se quiere inferir información acerca de la altura de los hombres, para ello se extrajo una muestra de \\(2100\\) de personas (en otros artículos se hablara de como extraer y que tamaño deben tener las muestras), de dicha muestra se obtuvo su media \\(\\mu=1.8\\) y la varianza \\(\\sigma^2=0.16\\).\n\nEstablezca una distribución que se ajuste a los datos de altura y realice su gráfico de densidad:\n\nDado que la muestra de datos de altura son mayores a \\(30\\) se puede asumir por TCL que la distribución de muestreo de la media es una distribución normal.\n\n\nCode\n# Dado que no tenemos dicho conjunto de datos, vamos a generarlos aleatoriamente usando NumPy:\n\n# Para ello, primero establezcamos la media y la desviacion estandar:\nmedia = 1.8\nvarianza = 0.16\ndev_sta = np.sqrt(varianza)\n\n# Y ahora si podemos generar los datos:\nalturas_muestra = np.random.normal(media, dev_sta, 2100)\n\n# Con lo cual podremos realizar el gráfico de densidad:\nplt.figure(figsize=(10, 6))\nsns.histplot(alturas_muestra, bins=30, kde=True, color='blue')\n\nplt.title('Muestra de Alturas')\nplt.xlabel('Altura en Metros')\nplt.ylabel('No. de Personas con dicha altura')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPor criterio estadístico se considera que alguien tiene una estatura “normal” cuando esta a menos de \\(2\\) desviaciones estandar \\(2\\sigma\\) de distancia de la media, ¿cuál es la probabilidad de que alguien sufra de gigantismo o enanismo?.\n\nPara hallar dicho valor, deberemos establecer, según nuestra muestra, que alturas se consideran enanismo y gigantismo, para ello utilizaremos los z-valores dados como \\(z=\\frac{x\\pm\\mu}{\\sigma}\\) (en donde el eje ordenado sera \\(\\mu\\)) de donde se despeja \\(x\\) resultando:\n\\[x=|2\\sigma^2\\pm\\mu|\\]\ncalculandolo usando python obtendremos:\n\n\nCode\n# llamaremos a los límites que definen el gigantismo y enanismo x_2 y x_1 respectivamente:\nx_1 = abs((2*(dev_sta**2)-media).round(4))\nx_2 = abs((2*(dev_sta**2)+media).round(4))\n\nprint(f'Una altura por debajo de {x_1} se considera enanismo, una altura por encima de {x_2} se considera gigantismo.')\n\n\nUna altura por debajo de 1.48 se considera enanismo, una altura por encima de 2.12 se considera gigantismo.\n\n\nLo siguiente es usar la función CDF \\(F\\) para calcular la probabilidad de cada una. - En el caso del enanismo es un uso directo del CDF:\n\\[\n\\begin{align}\n\\text{Probabilidad de ser enano}&=P(X&lt;x_1)\\\\\n                                &=F(x_1).\n\\end{align}\n\\]\n\nPara el caso del gigantismo podremos calcular su probabilidad de 2 formas distintas, la primera es restandole al área total (que como vimos en las distribuciones de probabilidad siempre es \\(1\\)) el área por el lado izquierdo de x_2 y la segunda implica utiliza el hecho de que curva de la distribución normal es simetrica alrededor de la media, por lo cual, al estar x_1 y x_2 a la misma distancia de esta tendriamos que:\n\n\\[\n\\begin{align}\n\\text{Probabilidad de ser gigante}&=P(x_2&lt;X) \\\\\n                                  &=1-F(x_2) \\\\\n                                  &=F(x_1).\n\\end{align}\n\\]\nPara entender esto mejor vease la siguiente gráfica:\n\n\nCode\n# Crear la gráfica:\nplt.figure(figsize=(10, 6))\nsns.kdeplot(alturas_muestra, color='blue')\n\n# Área bajo la curva entre 0 y x_1:\nx_fill_1 = np.linspace(0.5, x_1, 2100)\ny_fill_1 = norm.pdf(x_fill_1, media, dev_sta)\nplt.fill_between(x_fill_1, y_fill_1, color='cyan', alpha=0.5, label=r'$F(x_1)$')\n\n# Recta de x_1:\nplt.axvline(x_1, color='black', linestyle='--', ymax=0.7, label='límite inferior de altura')\nplt.text(x_1, -0.02, '$x_1$', fontsize=12, color='black', ha='left')\n\n# Área bajo la curva entre x_2 y 3.5:\nx_fill_2 = np.linspace(x_2, 3.5, 2100)\ny_fill_2 = norm.pdf(x_fill_2, media, dev_sta)\nplt.fill_between(x_fill_2, y_fill_2, color='cyan', alpha=0.5, label=r'$1-F(x_2)$')\n\n# Recta de x_2:\nplt.axvline(x_2, color='black', linestyle='--', ymax=0.7, label='límite superior de altura')\nplt.text(x_2, -0.02, '$x_2$', fontsize=12, color='black', ha='left')\n\n# Recta de la media:\nplt.axvline(media, color='red', linestyle='-', ymax=0.95, label=r'eje de la media $(\\mu)$')\nplt.text(media, -0.02, '$\\mu$', fontsize=12, color='red', ha='left')\n\n# Añadir las etiquetas y la leyenda\nplt.legend()\n\n# Insertando los correspondientes títulos:\nplt.title('Muestra de Alturas')\nplt.xlabel('Altura en Metros')\nplt.ylabel('No. de Personas con dicha altura')\n\n# Mostrar la gráfica:\nplt.show()\n\n\n\n\n\n\n\n\n\nYa sabiendo esto, basta con aplicar dicha función CDF, antiguamente, se utilizaba la función error de Gauss \\(\\erf()\\) y su expansión en series de Taylor para crear tablas con valores aproximados del CDF que se necesitara (tanto asi, que podran encontrar estas tablas en los anexos de los libros que dejo de referencia). Hoy día se puede utilizar lenguajes de programación como Python o R para hacer esto, en el caso específico de Python existen varias formas de hacer esto, una es crear la función desde \\(0\\) y guardarla como una clase o modulo y otra es usar un módulo predefinido ya existente. En nuestro caso haremos esto último, los \\(2\\) más comunmente usados son SciPy y StatsModels, ambas funcionan de manera muy similar y para lo que las usaremos nos vale cualquiera de ellas.\n\n\nCode\n# Ya que previamente importamos la funcionalidad de SciPy, utilicemos esta:\n## Para ello debemos suministrarle a la función la media y desviación estándar\nprobabilidad_enanismo=probabilidad_gigantismo=(norm.cdf(x_1, loc=media, scale=dev_sta)).round(4)\n\nprint(f'tanto la probabilidad de tener gigantismo o enanismo son iguales y equivalen aproximadamente a {probabilidad_enanismo*100}%.')\n\n\ntanto la probabilidad de tener gigantismo o enanismo son iguales y equivalen aproximadamente a 21.19%.\n\n\nAl tener un desarrollo más largo y un poco más técnico, las demostraciones de la varianza y la media con \\(\\sigma^2\\) y \\(\\mu\\) respectivamente, la prueba del teorema central del límite y posiblemente tambien el como aproximar la PDF normal mediante métodos puramente numéricos agregando una nueva sección para quienes esten interesados. Tratare de manera más precisa el como se trabaja con las áreas de probabilidad y la función PPF \\((F^{-1})\\) en mi artículo sobre las Pruebas de Hipótesis por medio del Valor Critico.\nGracias por leer."
  },
  {
    "objectID": "posts/distribucion-normal.html#referencias.",
    "href": "posts/distribucion-normal.html#referencias.",
    "title": "La Distribución Normal",
    "section": "Referencias.",
    "text": "Referencias.\n\nDevore, Jay L. Probabilidad y estadística para ingeniería y ciencias. Novena edición, Pearson, 2011.\nJ.Kazmier, Leonard.(1998).Estadística Aplicada a la Administración y a la Economía.McGRAW-HILL.\nWalpole, R. E., & Myers, R. H. (1992). Probabilidad y Estadística (4ª ed.). McGraw-Hill.\nNield, Thomas.(2020).Essential Math for Data Science. O’Reilly Media."
  },
  {
    "objectID": "posts/introduccion-matematicas-financieras.html",
    "href": "posts/introduccion-matematicas-financieras.html",
    "title": "Introducción a las Matemáticas Financieras",
    "section": "",
    "text": "imagen"
  },
  {
    "objectID": "posts/introduccion-matematicas-financieras.html#el-valor-del-dinero-en-el-tiempo",
    "href": "posts/introduccion-matematicas-financieras.html#el-valor-del-dinero-en-el-tiempo",
    "title": "Introducción a las Matemáticas Financieras",
    "section": "El Valor del Dinero en el Tiempo",
    "text": "El Valor del Dinero en el Tiempo\nLlamamos préstamo al acto en el que un ente natural o jurídico, al que llamaremos acreedor (o inversionista), presta o cede un bien de capital a otro ente, al que llamaremos deudor (o emisor). En la gran mayoría de los casos, esto se hace a cambio de que el deudor, además del valor del bien de capital que fue prestado en un principio (cuya devolución dependerá de las condiciones en las que se haya establecido el préstamo), entregue un beneficio extra (normalmente en porcentaje) al del valor del capital inicial y que además de depender de este dependerá también del tiempo. Dicho beneficio se conoce como interés.\nUna pregunta que todos en algún momento de nuestras vidas nos hemos hecho es ¿por qué existen los intereses? La respuesta a esta pregunta tiene que ver con el tema de partida en matemática financiera, que es el valor del dinero en el tiempo. En las finanzas modernas se considera que dentro de una economía de mercado (es decir, en un mercado donde los precios se establecen por la acción libre de la oferta y demanda de bienes y servicios), además de a los bienes o factores de capital, como la base de la riqueza, el tiempo también es crucial. Esto se debe a una serie de razones, de las cuales he decidido revisar las tres más importantes:\n\nLa inflación o pérdida de valor de l dinero en el tiempo: Su efecto más inmediato es la subida generalizada de los precios o la pérdida de poder adquisitivo del dinero.\nEl costo de oportunidad: El capital invertido en préstamos siempre presenta un costo en el sentido de que dicha riqueza podría haber sido invertida en otros activos materiales, humanos o proyectos que no solo podrían proteger de la inflación sino además ser potencialmente más útiles o rentables. Esto último está vinculado al riesgo inherente que corre el prestamista o inversor.\nLa preferencia temporal: Se refiere al aspecto humano que nos hace preferir las utilidades inmediatas a las futuras, querer comprar lo que se puede hoy a lo que se podría conseguir mañana. Para invertir es necesario ahorrar, lo cual siempre implica abstenerse del consumo presente o de las ya mencionadas utilidades inmediatas, siempre buscando aumentar la riqueza a futuro.\n\nEstos tres factores son igualmente importantes. Incluso si la inflación no existiera, el valor del dinero en el futuro es menor al del presente debido al costo de oportunidad vinculado al riesgo y a la preferencia temporal. Hablando de la inflación, cuando se dice que esta disminuye el “valor del dinero”, nos referimos al poder adquisitivo, es decir, a cuántos productos y servicios puedo comprar con una cantidad determinada de dinero. Por ello, no se pierde nada en estudiar cuáles pueden llegar a ser las tres causas lógicas de la inflación. Aquí un pequeño resumen de cada una:\n\nLa inflación por demanda: Ocurre cuando la capacidad monetaria de una población aumenta demasiado y resulta ser excesiva e imposible de cubrir por la oferta de bienes y servicios. El resultado de esto es que los precios suben y se ajustan al aumento de capacidad monetaria en la economía. Una de las muchas causas posibles de este tipo de inflación es el aumento del crédito por parte de los bancos comerciales o la disminución de los tipos de interés por parte del banco central. Ejemplos históricos de esto son los Estados Unidos de la postguerra y China en la última década.\nLa inflación de costos: Sucede cuando la producción vinculada a la oferta se encarece o disminuye. Lo más común es que se produzca cuando se reduce o elimina el acceso a una materia prima o cuando se reduce la mano de obra vinculada a un sector productivo. Un ejemplo anecdótico de lo primero es la crisis del petróleo en Estados Unidos de \\(1973\\) a \\(1974\\) debido al embargo de esta materia prima por parte de la OPEP.\nExpansión monetaria: Esta es la más común y sucede cuando un país expande la emisión de dinero, normalmente como medida para intentar contrarrestar el déficit o pérdida fiscal (el déficit fiscal es la diferencia entre los altos gastos y bajos ingresos del estado). El caso más reconocible para los lectores es la hiperinflación sucedida en Venezuela en el periodo de 2016 a 2022, aunque este periodo sigue en discusión, que se debió a la expansión monetaria del banco central de Venezuela, el BCV.\n\nEstimar la inflación es un proceso que requiere de modelos de análisis de series de tiempo y mediciones poblacionales macroeconómicas, que tal vez toque a futuro. Por ahora, nos basta con saber qué es la inflación y cuáles son sus tipos.\nVolviendo al interés, este se puede entender entonces simplemente como la medida del valor del dinero en el tiempo \\(I\\) resultado de restar el valor del capital inicial (o presente) \\(\\text{P}\\) del capital resultante (o futuro) \\(\\text{F}\\):\n\\[I=\\text{F}-\\text{P}\\]\n\nTasa de Interés\nNormalmente, en vez de tratar con el interés en montos de ganancia fijos para cada caso, lo que se suele hacer es expresarlo en términos de un porcentaje con respecto al valor de la inversión. Dicho porcentaje se conoce como tasa de interés y normalmente se denota con \\(i\\) y es igual a la razón entre la ganancia de los intereses \\(I\\) y la cantidad original invertida \\(\\text{P}\\), es decir:\n\\[i=\\frac{\\text{F}-\\text{P}}{\\text{P}}=\\frac{I}{P}\\]\nNótese que la estructura \\(\\frac{x_1-x_0}{x_0}\\) corresponde a la medición de un cambio relativo, que es sumamente importante en todas las disciplinas prácticas que utilizan modelos matemáticos, por su relación con las derivadas y la medición de los cambios. Probablemente hable de ello en un futuro anexo. De dicha ecuación se puede deducir la fórmula para determinar la renta del interés por medio de su tasa:\n\\[I=\\text{P}\\times i\\]\nTanto el interés como su tasa se estudian con respecto a un periodo de tiempo determinado, lo cual implica que podremos trabajar con el interés por medio de funciones con respecto al tiempo (función de acumulación) como veremos más adelante. Una conclusión importante del estudio del interés es que dos valores diferentes en distintas fechas pueden llegar a considerarse equivalentes. Un valor presente \\(P_0\\) es equivalente a un valor futuro \\(\\text{F}_0\\) si el valor futuro cubre el valor presente más los intereses a la tasa exigida por el inversionista.\nClaro que aún nos queda hablar prácticamente de toda la teoría del interés. En el siguiente artículo en el que trate dicho tema, hablaré de la función de acumulación, el interés simple, compuesto y el valor presente. Con esto abrimos una nueva rama de las aplicaciones de la matemática al mundo real, además de la estadística, la matemática financiera (a pesar de que estas dos estén íntimamente relacionadas). Espero que esto ayude a refrescar no solo a los lectores, sino también a mí. También debo aclarar que elegí este tema (y no métodos de aproximación y medidas para experimentos) por la importancia que tiene en la vida de las personas comunes. Alguien puede vivir de forma relativamente normal sin saber cómo se determina si una medida atípica en un acelerador de partículas prueba un modelo cuántico o es solo una casualidad. Pero no va a ser responsable en sus finanzas y en su vida si no sabe cómo funcionan los intereses o el cómo y por qué de la inflación. Estamos viviendo un fenómeno en el cual la educación matemática y financiera está siendo cada vez menor entre la población, como muestran las pruebas PISA, y si puedo aportar mi granito de arena para prevenir esa situación, lo haré con gusto."
  },
  {
    "objectID": "posts/introduccion-matematicas-financieras.html#referencias",
    "href": "posts/introduccion-matematicas-financieras.html#referencias",
    "title": "Introducción a las Matemáticas Financieras",
    "section": "Referencias",
    "text": "Referencias\n\nKozikowski Zarska, Z. (2007). Matemáticas financieras: El valor del dinero en el tiempo. McGraw-Hill.\nMeza Orozco, Juan de Jesús (2011). Matemáticas financieras aplicadas: Uso de las calculadoras financieras y Excel (4ta Ed.). ECOE Ediciones."
  },
  {
    "objectID": "posts/tablas-de-pago.html",
    "href": "posts/tablas-de-pago.html",
    "title": "Análisis de Decisión I (Tablas de Pago y Árboles de Decisión)",
    "section": "",
    "text": "imagen"
  },
  {
    "objectID": "posts/tablas-de-pago.html#tablas-de-pagos",
    "href": "posts/tablas-de-pago.html#tablas-de-pagos",
    "title": "Análisis de Decisión I (Tablas de Pago y Árboles de Decisión)",
    "section": "Tablas de Pagos",
    "text": "Tablas de Pagos\nA partir de los objetos que acabamos de definir, podremos construir una estructura que sintetice toda la información, conocida como tabla de pagos, con lo cual la tabla identifica la “ganancia” o “pérdida” (de forma más general, resultados favorables o desfavorables) condicional vinculada con cada posible combinación de acciones y eventos de decisión y su probabilidad de ocurrencia (esto asumiendo que dichos eventos no están correlacionados). Sin embargo, es muy importante aclarar que los valores \\(X_{11},...,X_{mn}\\) dependen del criterio de toma de decisión que adoptemos. Esto se debe a que esos criterios son los que determinan la forma de calcularlos. Así es como se debería ver una tabla de pagos en general:\n\\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\textbf{Eventos} & \\textbf{Probabilidad} & {\\textbf{Acciones} \\ (A)} \\\\\n\\hline\n&  & \\textbf{A1} & \\textbf{A2} & \\textbf{A3} & \\textbf{...} & \\textbf{An} \\\\\n\\hline\nE1 & P1 & X_{11} & X_{12} & X_{13} & ... & X_{1n} \\\\\n\\hline\nE2 & P2 & X_{21} & X_{22} & X_{23} & ... & X_{2n} \\\\\n\\hline\nE3 & P3 & X_{31} & X_{32} & X_{33} & ... & X_{3n} \\\\\n\\hline\n... & ... & ... & ... & ... & ... & ... \\\\\n\\hline\nEm & Pm & X_{m1} & X_{m2} & X_{m3} & ... & X_{mn} \\\\\n\\hline\n\\end{array}\n\\]\n\nEjemplo (Formulación de Tablas de Pago):\nUna tienda estima que los valores de probabilidad asociados con la venta de \\(2, 3, 4\\) o \\(5\\) televisores a crédito durante los \\(3\\) primeros meses son de \\(0.3, 0.4, 0.2, 0.1\\) respectivamente. Por otro lado, el margen de ganancia de cada televisor vendido es \\(200\\). Si en el curso de los tres meses no se vendieran algunos televisores, la pérdida total por aparato sería de \\(300\\). Con esta información, podremos construir nuestra tabla de pago y este sería el resultado:\n\\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\textbf{Demanda del Mercado} & \\textbf{Probabilidad} & {\\textbf{Cantidad del Pedido} \\ (A)} \\\\\n\\hline\n&  & \\textbf{A1:2} & \\textbf{A2:3} & \\textbf{A3:4} & \\textbf{A4:5} \\\\\n\\hline\nE1:2 & 0.3 & 400 & 100 & -200 & -500 \\\\\n\\hline\nE2:3 & 0.4 & 400 & 600 & 300 & 0 \\\\\n\\hline\nE3:4 & 0.2 & 400 & 600 & 800 & 500 \\\\\n\\hline\nE4:5 & 0.1 & 400 & 600 & 800 & 1000 \\\\\n\\hline\n\\end{array}\n\\]\nNótese que, como mencionamos antes, el criterio de decisión con el que calculamos los valores \\(X\\) es el criterio basado en el pago esperado.\n\n\nToma de Decisiones Basada Únicamente en Probabilidades\nDescribir este proceso es bastante simple: se toma la decisión que tiene la mayor probabilidad de ocurrir. En el caso del ejemplo anterior, decidiríamos comprar \\(2\\) televisores, es decir la acción \\(A3\\), ya que la probabilidad de vender exactamente dos televisores es del \\(40%\\), que es mayor que la probabilidad de cualquier otro posible evento.\n\n\nToma de Decisiones con Base en las Consecuencias Económicas\nEste tipo de criterios de decisión es igual de simple al anterior, solo que en este caso se tienen en cuenta los datos de pérdida y ganancia asociados. Existen tres criterios que dependen de si queremos maximizar las ganancias o minimizar las pérdidas. Estos son:\n\nEl criterio maximín es aquel en el cual tomamos la decisión que se relaciona con el mayor mínimo valor posible. Dicho de otra manera, tomamos la decisión que nos aporta la mayor ganancia dando por hecho el “peor escenario posible” como verdadero. En el caso de la tabla anterior, la decisión que elegiríamos al asumir el criterio maximín sería \\(A2\\), ya que en el peor escenario posible solo nos comprarían \\(2\\) televisores, por lo que la mayor ganancia en ese caso sería tener solo \\(2\\) televisores, con una ganancia de \\(400\\).\nEl criterio maximax toma la decisión vinculada al mayor valor posible en general. Es decir, en este caso asumimos como verdadero el “mejor escenario posible” y tomamos la decisión que conlleva las mayores ganancias. En nuestro ejemplo, esto sería asumir que se van a vender todos los televisores que compremos, y en base a ello, lo más lógico es comprar la mayor cantidad (\\(5\\)), lo que conlleva a la mayor ganancia (\\(1000\\)). Esta es la acción \\(A4\\).\nEl criterio minimax es quizá el método más interesante de los que hemos mencionado hasta ahora, pues se basa en el concepto de coste de oportunidad, también llamado arrepentimiento, que se define como la diferencia entre el resultado de la acción \\(X\\) y el resultado de la mejor acción dando por hecho que haya ocurrido el evento dado. Después de calcular la pérdida de oportunidad de cada \\(X\\), debemos tomar el mayor de cada acción (columna) \\(A\\) y compararlo con el máximo de las demás acciones. Evidentemente, se elige la que tenga el menor (o mínima, de ahí minimax) coste de oportunidad.\n\nEn nuestro caso, por ejemplo, el coste de oportunidad de la acción del evento \\(A2\\), dado que suceda el evento \\(E3\\), sería la diferencia entre el mejor caso dado dicho evento, que sería \\(X_{33}\\), y el correspondiente al de \\(A2\\), dado \\(E3\\), el cual corresponde a \\(X_{32}\\), y estaría dada por el siguiente cálculo:\n\\[\\text{Perdida de Oportunidad}=X_{33}-X_{32}=800-600=200\\]\nEn general, podemos modificar la tabla de pagos anterior y terminar usando el criterio minimax como sigue:\n\\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\textbf{Demanda del Mercado} & {\\textbf{Cantidad del Pedido} \\ (A)} \\\\\n\\hline\n& \\textbf{A1:2} & \\textbf{A2:3} & \\textbf{A3:4} & \\textbf{A4:5} \\\\\n\\hline\nE1:2 & 0 & 300 & 600 & 900 \\\\\n\\hline\nE2:3 & 200 & 0 & 300 & 0 \\\\\n\\hline\nE3:4 & 400 & 200 & 0 & 300 \\\\\n\\hline\nE4:5 & 600 & 400 & 200 & 0 \\\\\n\\hline\n\\textbf{Arrepentimiento Maximo}: & 600 & 400 & 600 & 900 \\\\\n\\hline\n\\end{array}\n\\]\nDel arrepentimiento máximo de cada acción elegimos el menor, por lo que, bajo este criterio, deberíamos elegir la acción \\(A2\\). Es decir, comprar \\(3\\) televisores para vender.\n\n\nToma de Decisiones en Base a las Consecuencias y las Probabilidades\nUna forma de considerar tanto las posibles pérdidas y ganancias como las probabilidades de cada evento es multiplicando dichos valores por sus respectivas probabilidades. Al hacer esto, asignamos más o menos “peso” a los valores según si son más o menos probables, y para determinar el valor de cada columna, sumamos dichos pesos ya calculados. Este proceso da como resultado una fórmula que corresponde a la Esperanza Matemática \\(E(X)\\). Este criterio se conoce como Criterio del Pago Esperado (PE).\nPor ejemplo, si quisiéramos calcular el pago esperado resultante de comprar \\(4\\) televisores para vender, es decir, la acción \\(A3\\), se calcularía con la siguiente fórmula:\n\\[E(A3)=\\sum_{i=\\{1,2,3,4\\}}P_i\\cdot X_{i3}=(0.3)(200)+(0.4)(300)+(0.2)(400)+(0.1)(500)=300\\]\nComo es natural, después de determinar el pago esperado (PE) de cada acción, se elige la que sea mayor. En nuestro caso particular, el resultado será:\n\\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\textbf{Demanda del Mercado} & \\textbf{Probabilidad} & {\\textbf{Cantidad del Pedido} \\ (A)} \\\\\n\\hline\n&  & \\textbf{A1:2} & \\textbf{A2:3} & \\textbf{A3:4} & \\textbf{A4:5} \\\\\n\\hline\nE1:2 & 0.3 & 400 & 100 & -200 & -500 \\\\\n\\hline\nE2:3 & 0.4 & 400 & 600 & 300 & 0 \\\\\n\\hline\nE3:4 & 0.2 & 400 & 600 & 800 & 500 \\\\\n\\hline\nE4:5 & 0.1 & 400 & 600 & 800 & 1000 \\\\\n\\hline\n\\text{Pago Esperado (PE)}& & 400 & 450 & 300 & 50 \\\\\n\\hline\n\\end{array}\n\\]\nAl ser la acción con el mayor valor de pago esperado \\(A_2\\), según este criterio deberíamos elegir comprar \\(3\\) televisores para vender.\nOtra forma de utilizar tanto los valores como las probabilidades es mediante la Mínima Pérdida de Oportunidad Esperada (POE). Para determinar esto, se realiza el cálculo de la esperanza de cada acción, pero en lugar de usar los pagos originales, se utilizan los valores de arrepentimiento o coste de oportunidad obtenidos del criterio minimax. En el caso de nuestro ejemplo anterior, la solución sería la siguiente:\n\\[\n\\begin{array}{|c|c|c|c|c|c|c|}\n\\hline\n\\textbf{Demanda del Mercado} & \\textbf{Probabilidad} & {\\textbf{Cantidad del Pedido} \\ (A)} \\\\\n\\hline\n&  & \\textbf{A1:2} & \\textbf{A2:3} & \\textbf{A3:4} & \\textbf{A4:5} \\\\\n\\hline\nE1:2 & 0.3 & 0 & 300 & 600 & 900 \\\\\n\\hline\nE2:3 & 0.4 & 200 & 0 & 300 & 0 \\\\\n\\hline\nE3:4 & 0.2 & 400 & 200 & 0 & 300 \\\\\n\\hline\nE4:5 & 0.1 & 600 & 400 & 200 & 0 \\\\\n\\hline\n\\text{Perdida de Oportunidad Esperada (PE)}& & 220 & 170 & 320 & 330 \\\\\n\\hline\n\\end{array}\n\\]\nEvidentemente, elegimos la acción que nos suponga menor pérdida de oportunidad, es decir, \\(A2\\)."
  },
  {
    "objectID": "posts/tablas-de-pago.html#árboles-de-decisión",
    "href": "posts/tablas-de-pago.html#árboles-de-decisión",
    "title": "Análisis de Decisión I (Tablas de Pago y Árboles de Decisión)",
    "section": "Árboles de Decisión",
    "text": "Árboles de Decisión\nAunque todos los criterios que vimos anteriormente son útiles en la mayoría de los casos, cuando asumimos problemas de decisión no solo tratamos con una primera decisión, sino varias decisiones, unas dependiendo de las anteriores de forma secuencial. El análisis de los eventos y decisiones debe realizarse en un esquema como un todo. El Análisis de Árboles de Decisión es el método que podemos emplear para identificar tanto la mejor acción inicial como las mejores acciones subsecuentes. En este método, para calcular qué decisión debemos tomar en cada caso, usaremos el criterio del pago esperado (PE).\nEl algoritmo que utilizaremos para emplear este método es sumamente importante para más adelante, y por ello es relevante explicarlo paso a paso:\n\nEl árbol de decisión es una esquematización de una situación de decisiones secuenciales. Este se realiza de izquierda a derecha o de arriba a abajo. Primero, identificamos cuáles son las decisiones posibles y los eventos aleatorios para transformarlos en puntos de decisión, también conocidos como nodos. La decisión inicial será el primer punto de decisión y se conoce como raíz del árbol. Por último, los valores o pagos no tienen más nodos subsecuentes y se conocen como hojas.\nSe elaboran las ramas que conectan la raíz a los nodos y estos, ya sea a otros nodos o a una hoja, teniendo en cuenta las decisiones secuenciales. Muchos de los valores se encuentran a varios pasos de distancia de la raíz. Esto depende del enunciado y de cómo se estructure la toma de decisiones y sus consecuencias.\nDespués de elaborar el árbol de decisión, se agrega al diagrama las probabilidades asociadas con los eventos aleatorios y los valores correspondientes a sus respectivas ramas. Esto, por supuesto, se incluye si el problema contiene dichos valores; como veremos más adelante, este paso no se incluye en algunos otros usos de los árboles de decisión.\nPara determinar los pagos esperados (PE) de las acciones alternativas en el punto de decisión inicial, se calculan los pagos esperados de las decisiones de derecha a izquierda o de abajo hacia arriba en el árbol de decisión.\n\nEl resultado de aplicar este algoritmo es hallar el valor PE de una ruta de acciones en particular del árbol de decisión. Cabe mencionar que la Profundidad del árbol de decisión resultante es simplemente la ruta o trayectoria más larga entre la raíz y las hojas.\n\nEjemplo (Formulación de un Árbol de Decisión):\nUn inversionista quiere saber si debe invertir o no en una tienda con un depósito de \\(10,000\\). Sin embargo, estima que hay una probabilidad de 50-50 de que un competidor instale un local en la misma zona. Además, en cuanto al nivel de demanda, solo existen dos escenarios: el primero es que el mercado sea grande y el segundo que sea moderado, este último con una probabilidad del 60%. Si se realiza la inversión, el inversionista ya calculó el valor de sus ganancias netas en cada caso: si existe competencia y el mercado es grande, las ganancias son de \\(15,000\\); si, en cambio, es moderado, son de \\(-10,000\\). Por otro lado, si no hay competencia, con un mercado grande sus ganancias serán de \\(30,000\\), y si es moderado, serán de \\(10,000\\). ¿Es mejor realizar o no realizar la inversión? ¿Cuál es el valor esperado (PE) de dicha decisión?\nPara resolver esto, primero debemos establecer la secuencia de decisiones y sus respectivos valores (nótese que al ser ganancias netas, ya se restó la inversión inicial):\n\nNo realizar la inversión que no lleva a no ganar nada: \\(0\\)\nRealizar la inversión de donde se sigue un evento:\n\n\nHay competencia (probalilidad de un \\(50\\%\\)):\n\nHay un mercado grande (probabilidad del \\(100\\%-60\\%=40\\%\\)), por lo que las ganancias son de \\(15000\\)\nHay un mercado moderado (probabilidad del \\(60\\%\\)), por lo que las ganancias son de \\(-10000\\)\n\nNo hay competencia (probabilidad de un \\(50\\%\\)):\n\nHay un mercado grande (probabilidad del \\(40\\%\\)), por lo que las ganancias son de \\(30000\\)\nHay un mercado moderado (probabilidad del \\(60\\%\\)), por lo que las ganancias son de \\(10000\\)\n\n\nAl realizar el algoritmo mencionado antes, el árbol de decisión se verá de la siguiente manera:\n\n\nCode\n# Importamos los módulos\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pygraphviz as pgv\n\n# Crear el grafo dirigido\nG = nx.DiGraph()\n\n# Añadir nodos\nG.add_node(\"Inversión\", color='lightblue')\nG.add_node(\"No realizar la inversión: $0\", color='lightgreen')\nG.add_node(\"Se realiza la inversión\", color='lightblue')\nG.add_node(\"Hay competencia\", color='lightblue')\nG.add_node(\"No hay competencia\", color='lightblue')\nG.add_node(\"Mercado grande: $30000\", color='lightgreen')\nG.add_node(\"Mercado pequeño: $10000\", color='lightgreen')\nG.add_node(\"Mercado grande: $15000\", color='lightgreen')\nG.add_node(\"Mercado pequeño: -$10000\", color='lightgreen')\n\n# Añadir aristas con etiquetas\nG.add_edge(\"Inversión\", \"No realizar la inversión: $0\", label=\"No\")\nG.add_edge(\"Inversión\", \"Se realiza la inversión\", label=\"Sí\")\nG.add_edge(\"Se realiza la inversión\", \"Hay competencia\", label=\"0.5\")\nG.add_edge(\"Se realiza la inversión\", \"No hay competencia\", label=\"0.5\")\nG.add_edge(\"Hay competencia\", \"Mercado grande: $30000\", label=\"0.4\")\nG.add_edge(\"Hay competencia\", \"Mercado pequeño: $10000\", label=\"0.6\")\nG.add_edge(\"No hay competencia\", \"Mercado grande: $15000\", label=\"0.4\")\nG.add_edge(\"No hay competencia\", \"Mercado pequeño: -$10000\", label=\"0.6\")\n\n# Calcular posiciones con graphviz_layout\npos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n\n# Ajustar el tamaño de la figura para proporcionar más espacio horizontal\nplt.figure(figsize=(12, 8))\n\n# Dibujar nodos con colores personalizados\nnode_colors = [G.nodes[node]['color'] for node in G.nodes]\nnx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=3000, font_size=10, font_weight='bold', font_color='black', edge_color='grey', linewidths=2, arrows=True)\n\n# Dibujar etiquetas de las aristas\nedge_labels = nx.get_edge_attributes(G, 'label')\nnx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10, font_color='red')\n\n# Mostrar la gráfica\nplt.title(\"Árbol de Decisión de Inversión\")\nplt.show()\n\n\n\n\n\n\n\n\n\nLas únicas dos opciones posibles son que se realice o no la inversión; en el primer caso, si no se realiza la inversión, evidentemente el PE es \\(0\\). Ahora, para determinar el PE del caso en el que decidimos hacer la inversión, depende de los respectivos PE de las posibilidades del evento de “hay o no competencia” y estos. A su vez, dependerán de las posibilidades y los valores del evento “mercado grande o pequeño”. Por lo tanto, primero debemos relizar los siguientes cálculos:\\(\\DeclareMathOperator{\\PE}{PE}\\)\n\\[\\PE(\\text{Hay competencia})=(0.4)(15000)+(0.6)(-10000)=0 \\]\n\\[\\PE(\\text{No hay competencia})=(0.4)(30000)+(0.6)(10000)=18000 \\]\nCon esta información, podremos determinar el PE de la decisión “Invertir”:\n\\[\\PE(\\text{Invertir})=(0.5)(0)+(0.5)(18000)=9000\\]\nPor lo cual, el pago esperado de hacer la inversión es mayor al de no hacerla, y por tanto esa primera es la mejor decisión a tomar bajo este método.\nLos árboles de decisión, como se habrá notado, son en realidad funciones que asignan valores discretos o categorías a otros valores discretos o categorías. A pesar de ello, podemos trabajar con valores continuos reales si, en vez de tomar “decisiones”, elegimos “desigualdades”. De esta manera, nuestros conjuntos de entrada y de salida en los árboles serán segmentos o intervalos reales. Debido a lo anterior, todas nuestras decisiones serán de tipo binario (están dentro del intervalo o fuera de este). Podremos trabajar con conjuntos numéricos de datos utilizando árboles de decisión y, por lo tanto, realizar procesos de Clasificación y de Regresión utilizando árboles. Para ello, necesariamente tendremos que reformular el algoritmo mediante el cual construimos el modelo; este nuevo algoritmo se conoce como CART. Para utilizar los árboles de decisión como funciones reales, simplemente introducimos el valor de entrada y vamos revisando si se encuentra o no en los intervalos correspondientes a la decisión en secuencia, hasta llegar a un valor o hoja que corresponderá al resultado de la función o imagen de dicha entrada.\nOriginalmente, pensaba hacer un artículo aparte para los árboles de decisión, pero en medio de mi investigación encontré dos posts muy buenos que explican de manera sumamente detallada el uso de este modelo para clasificación y regresión. Así que realizaré un resumen del algoritmo CART para ambos casos y adjuntaré los enlaces a dicho blog."
  },
  {
    "objectID": "posts/tablas-de-pago.html#clasificación-con-árboles-de-decisión",
    "href": "posts/tablas-de-pago.html#clasificación-con-árboles-de-decisión",
    "title": "Análisis de Decisión I (Tablas de Pago y Árboles de Decisión)",
    "section": "Clasificación con Árboles de Decisión",
    "text": "Clasificación con Árboles de Decisión\nPara crear un árbol de decisión que clasifique los valores de un conjunto de datos, utilizamos la versión de clasificación del algoritmo CART. Para poder describirlo, utilizaremos un ejemplo abstracto. Lo primero es diferenciar al conjunto de características o variables independientes \\(\\overline{X}\\), en este ejemplo asumiremos que solo hay una única característica, es decir, \\(X\\) es de dimensión \\(1\\) (o sea, solo una columna), del conjunto objetivo o variable dependiente \\(Y\\). El método de clasificación mediante árbol aleatorio necesita que se establezcan las clases en las que vamos a “colocar” los puntos de datos; en nuestro caso, supondremos \\(K\\) clases, denotadas por \\(C^1, C^2, \\ldots, C^k\\).\nEl algoritmo se compone de los siguientes pasos:\n\nGeneración de umbrales candidatos \\((u)\\): Estos corresponden a los puntos medios entre los valores consecutivos de las características \\(X\\).\n\nEn nuestro ejemplo, si \\(x_{m}, x_{m+1} \\in X\\) son dos puntos consecutivos, entonces su umbral candidato es \\(u_m = \\frac{x_{m+1} - x_m}{2}\\). Si la cantidad de valores en \\(X\\) es \\(n\\) (es decir, \\(|X| = n\\)), entonces la cantidad de umbrales candidatos será \\(n - 1\\). El resultado de cada umbral \\((u)\\) será una partición binaria de \\(X\\). Para \\(u_m\\), llamaremos a la sección izquierda de la partición \\(u_{m1}\\) y a la derecha \\(u_{m2}\\).\n\nCálculo del Índice de Impureza de Gini \\((I)\\) de cada sección de la partición generada por el umbral candidato: Esto se hace para cada uno de los umbrales candidatos.\n\nEl índice de impureza de Gini se calcula mediante la siguiente fórmula:\n\\[I_{mj}=1-\\sum_{i=1}^{K}P_{C_i,U_{mj}}^{2}\\]\nDonde \\(P_{C_i, U_{mj}}\\) son las probabilidades de encontrar un punto de la clase \\(C^i\\) en la sección de la partición \\(u_{mj}\\). Esta probabilidad se calcula dividiendo el número de puntos de dicha clase en el segmento entre el total de puntos, es decir:\n\\[P_{C_i,u_{mj}}=\\frac{|C_{u_{mj}}^{i}|}{\\sum_{t=1}^{K}|C_{u_{mj}}^{t}|}.\\]\n\nDeterminación de la Función de Costo (CF) asociada a cada umbral candidato.\n\nPara los valores del ejemplo, esto se hace mediante la siguiente fórmula:\\(\\DeclareMathOperator{\\CF}{CF}\\)\n\\[\\CF(u_m)=I_{m1}\\frac{\\sum_{i=1}^{K}|C_{u_{m1}}^{i}|}{|X|}+I_{m2}\\frac{\\sum_{i=1}^{K}|C_{u_{m2}}^{i}|}{|X|}.\\]\n\nSelección del umbral candidato con menor función de costo: De entre todos los umbrales candidatos \\((u)\\), elegimos como “decisión” o nodo el que posea menor función de costo. Evidentemente, en el primer ciclo del algoritmo, el primer nodo que obtenemos es la raíz.\nIteración del proceso hasta cumplir una condición de parada: Este proceso se repite o itera, generando nodos (exceptuando las hojas) hasta cumplir con alguna condición de parada (esta puede ser un límite para la profundidad del árbol).\n\nDespués de construir el árbol de decisión mediante el algoritmo anterior, para utilizarlo al momento de clasificar, simplemente debemos introducir el valor de entrada, movernos por la ruta correspondiente a las desigualdades y, al llegar a la hoja de salida, ver qué clase posee más puntos en la región de datos vinculada a la hoja. Esa será la clase a la que pertenece el valor de entrada. Pueden ver un ejemplo más específico con dos categorías y algo más detallado en el siguiente post.\n\nRegresión con Árboles de Decisión\nAl igual que en el uso anterior, crearemos un árbol de decisión que funcione como función de regresión. Para ello, nuevamente utilizaremos el algoritmo CART, con algunas modificaciones. Las dos más importantes son: en este caso, los valores \\(x\\) que pertenecen a las variables independientes \\(X\\) no se someten a ninguna “categoría”, y de ahí se infiere que la métrica que usamos para determinar la función de costo no es el coeficiente de Gini sino un promedio. Usemos la misma notación de antes. Así, el algoritmo de regresión CART:\n\nGeneración de umbrales candidato \\((u)\\) como antes.\n\nEn este nuevo ejemplo, usaremos la misma notación de umbral candidato \\(u_m\\), de secciones izquierda y derecha \\(u_{m1}\\) y \\(u_{m2}\\), respectivamente. Además diremos que el número de puntos de una sección \\(u_{mj}\\) del umbral candidato \\(u_m\\) será \\(|u_{mj}|=S\\).\n\nCálculo del Error Cuadrático Medio (MSE) de cada sección del respectivo umbral candidato.\n\nPara calcular el MSE, primero debemos determinar el promedio \\(\\overline{x}{u{mj}}\\) de cada sección. Este promedio es la división entre la suma de todos los valores de los puntos dentro de la sección y el número total de valores de los datos también dentro de la sección \\(|u_{mj}|=S\\). Llamaremos al promedio de la sección \\(u_{mj}\\) con el símbolo \\(\\overline{x}{u{mj}}\\). Teniendo en cuenta lo anterior, el error cuadrático medio MSE se calcula con la siguiente fórmula:\\(\\DeclareMathOperator{\\MSE}{MSE}\\)\n\\[\\MSE(u_{mj})=\\frac{\\sum_{i=1}^{S}(x_{mj,i}-\\overline{x}_{u_{mj}})^2}{|u_{mj}|}=\\frac{\\sum_{i=1}^{S}(x_{mj,i}-\\overline{x}_{u_{mj}})^2}{S}\\]\n\nDeterminación de la función de costo de cada umbral candidato, que simplemente será la suma del MSE de cada una de sus secciones.\n\nLo anterior equivale a decir:\n\\[\\CF(u_{m})=\\MSE(u_{m1})+\\MSE(u_{m2})\\]\n\nSelección del umbral candidato con menor función de costo: Al igual que antes, elegimos como umbral el candidato con menor función de costo, y su criterio será un nodo de nuestro árbol de decisión (las hojas, por otro lado, corresponderán a los promedios que mencionamos antes).\nRepetición de los pasos anteriores hasta satisfacer alguna condición de parada.\n\nPara utilizar el árbol de decisión generado por ese algoritmo, a diferencia de la clasificación (y dado que aquí no tenemos categorías), al llegar a la hoja se tomará el promedio de su sección correspondiente \\(\\overline{x}_{u{mj}}\\), y este será el resultado para el valor de entrada. Realmente, podemos usar el coeficiente de impureza de Gini o el MSE tanto para regresión como para clasificación de manera indiferenciada. Este cambio es para que se entienda que la métrica que se utiliza para generar los nodos del árbol puede variar. Por ejemplo, en vez de las dos fórmulas antes mencionadas, podemos utilizar el criterio de ganancia de información. De nuevo, hay un ejemplo práctico algo más detallado en este post del otro blog."
  },
  {
    "objectID": "posts/tablas-de-pago.html#bosques-aleatorios",
    "href": "posts/tablas-de-pago.html#bosques-aleatorios",
    "title": "Análisis de Decisión I (Tablas de Pago y Árboles de Decisión)",
    "section": "Bosques Aleatorios",
    "text": "Bosques Aleatorios\nEl último tema que vamos a revisar son los bosques aleatorios. Este modelo es un conjunto de múltiples árboles de decisión, y por lo mismo, las aplicaciones de los bosques aleatorios también incluyen modelos de regresión y clasificación. Para entender cómo se construye un bosque aleatorio, primero debemos explicar en qué consiste el método de muestreo Bagging.\n\nRemuestreo Bagging\nLa agregación bootstrap, también conocida como bagging, es una técnica de remuestreo que consiste en los siguientes pasos:\n\nBootstrapping: De nuestro conjunto de datos, tomamos una cantidad preestablecida de subconjuntos cuyos elementos o puntos deben ser seleccionados al azar y con reemplazo. Esto significa que, si por ejemplo en el primer subconjunto elegimos aleatoriamente el punto \\(p\\), al escoger los puntos de los otros subconjuntos, podemos volver a elegir aleatoriamente ese mismo punto \\(p\\).\nEntrenamiento en paralelo: Se aplica a cada subconjunto un algoritmo o modelo que realice la tarea que queremos hacer. Por ejemplo, aquí podríamos aplicar el algoritmo CART, ya sea de clasificación o regresión por medio de árboles de decisión, a cada uno de los subconjuntos (esto se hace al mismo tiempo, de ahí la expresión “entrenamiento paralelo”).\nAgregación: Finalmente, se toman los resultados del modelo en cada subconjunto y se realiza una estimación.\n\nCon lo anterior en mente, basta decir que el bosque aleatorio es un modelo de aprendizaje automático que usa el bagging y, en cada subconjunto, aplica un algoritmo de árbol de decisión de clasificación o regresión, dependiendo de lo que queramos hacer. Algunas anotaciones: en el bosque aleatorio, el paso 3 del bagging es distinto en cada caso:\n\nSi estamos realizando una regresión con bosque aleatorio, el resultado final (paso 3) consiste en tomar el promedio entre los resultados de todos los árboles de decisión. Por ejemplo, si en nuestro bosque solo tenemos dos árboles y al aplicar un valor, el resultado del primero es \\(8\\) y del segundo es \\(2\\), el resultado del bosque será \\(\\frac{8+2}{2} = 5\\). Este criterio de estimación se conoce como votación suave.\nPor otro lado, si estamos haciendo una clasificación con bosque aleatorio, el criterio de estimación consiste en tomar la moda, es decir, la categoría que más se repite entre los árboles de decisión y escogerla como resultado del bosque. Por ejemplo, si tenemos tres árboles que clasifican nuestros valores en colores (azul o rojo), y al clasificar un valor, el primer árbol da como resultado “rojo” y los otros dos árboles “azul”, el resultado final del bosque aleatorio debe ser “azul” por votación. Este criterio se conoce como votación dura.\n\n\n\nCaracterísticas de los Métodos Anteriores\nPodemos pensar en varias ventajas y desventajas de los bosques aleatorios. Entre las ventajas están que el bagging soluciona el problema del sobreajuste que puede ocurrir al utilizar únicamente árboles de decisión. Podemos utilizar las métricas que descartamos en el CART para determinar la importancia de las características de los datos, una ventaja de los árboles de decisión en general. Además, al tomar intervalos o secciones, podemos trabajar con conjuntos de datos que presenten muchos valores faltantes sin tanto problema. Entre las desventajas, la más importante a mencionar es el costo computacional, lo que puede llevar a un aumento en el tiempo de entrenamiento y una mayor exigencia de memoria y procesamiento. Este problema es único del bosque aleatorio (al igual que el sobreajuste se presenta más en los árboles de decisión), por lo que deberemos elegir entre arriesgarnos a un sobreajuste o a una falta de recursos computacionales si estamos eligiendo entre árboles de decisión y bosque aleatorio.\nPara la clasificación, tanto en árboles de decisión como en bosque aleatorio, se necesita establecer de antemano las categorías (obviamente incluyendo su número), lo cual puede ser un gran problema en el caso de detección de clases, donde no sabemos cuáles son las características. Para el bosque aleatorio, además, debemos establecer cuántos árboles (y por tanto subconjuntos) queremos construir. En todos los casos, debemos establecer una condición de parada, como la profundidad máxima de los árboles e, incluso, de ser necesario, realizar una “poda”.\nEn matemáticas financieras, estos métodos se suelen utilizar para análisis de riesgo crediticio, detección de fraude y la estimación de precios para las opciones. Además, tienen usos en muchas otras disciplinas."
  },
  {
    "objectID": "posts/tablas-de-pago.html#referencias",
    "href": "posts/tablas-de-pago.html#referencias",
    "title": "Análisis de Decisión I (Tablas de Pago y Árboles de Decisión)",
    "section": "Referencias",
    "text": "Referencias\n\nJ.Kazmier, Leonard.(1998).Estadística Aplicada a la Administración y a la Economía.McGRAW-HILL.\nCodificando Bits. Clasificación con árboles de decisión: El algoritmo CART. Recuperado de https://www.codificandobits.com/blog/clasificacion-arboles-decision-algoritmo-cart/\nCodificando Bits. Regresión con árboles de decisión: El algoritmo CART. Recuperado de https://www.codificandobits.com/blog/regresion-arboles-decision-algoritmo-cart/\nIBM. Random forest. Recuperado de https://www.ibm.com/topics/random-forest\nIBM. Bagging. Recuperado de https://www.ibm.com/es-es/topics/bagging"
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "",
    "text": "imagen"
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html#cómo-establecer-las-hipótesis",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html#cómo-establecer-las-hipótesis",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "¿Cómo Establecer las Hipótesis?",
    "text": "¿Cómo Establecer las Hipótesis?\nPara realizar pruebas de hipótesis relacionadas con el valor de la media de la población, necesitaremos tres parámetros que debemos obtener de los datos o que deben estar proporcionados por el problema. Estos son: la media poblacional \\(\\mu\\) , la desviación estándar de la población \\(\\sigma\\), y la media de la muestra \\(\\overline{x}\\). La prueba de hipótesis se realizará para determinar cuál de las dos siguientes opciones es cierta:\n\nLa media muestral obtenida surgió de manera aleatoria de la muestra, sin ninguna causa externa.\nLa media muestral no surgió de manera aleatoria, indicando que hubo algún factor externo que influyó en ella.\n\nSi asumimos que el valor obtenido experimentalmente de la media poblacional es \\(\\mu_0\\). Lo anterior se traduce estadísticamente en la formulación de las hipótesis de la siguiente manera:\n\nLa hipótesis nula será que, dadas \\(\\overline{x}\\) y \\(\\sigma\\), la media poblacional es igual al valor obtenido experimentalmente. Es decir: \\[H_0:\\mu=\\mu_0\\]\nLa hipótesis alternativa será que, dadas \\(\\overline{x}\\) y \\(\\sigma\\), la media de la población no es igual al valor obtenido experimentalmente. Es decir: \\[H_1:\\mu\\neq\\mu_0\\]\n\n\nEjemplo (Fórmulación de las Hipótesis):\nEl departamento de marketing de una empresa ha lanzado una nueva campaña de publicidad y quiere saber si esta ha afectado de alguna forma las ventas. En el pasado, las ventas promediaron \\(\\$10345\\) al día con una desviación estándar de \\(\\$552\\). La nueva campaña de publicidad se ejecutó durante \\(45\\) días y el promedio de ventas fue de \\(\\$11641\\) por día.\n¿La campaña afectó las ventas?\nPara solucionar este problema deberemos realizar una prueba de hipótesis, lo primero a hacer es establecer las hipótesis, para ello nótese que el valor de la media poblacional es \\(\\mu_0=10345\\), donde los otros dos párametros son la desviación estándar poblacional \\(\\sigma=552\\) y la media muestral \\(\\overline{x}=11641\\). Así, nuestrashipótesis serán:\n\\[\n\\begin{align}\nH_0 &: \\mu=10345 \\text{ y}\\\\\nH_1 &: \\mu\\neq10345.\n\\end{align}\n\\]\nGuardemos nuestros parámetros en las líneas de código de python:\n\n\nCode\n# Establezcamos nuestros parámetros:\nmedia_poblacional = 10345\nmedia_muestral = 11641\ndesv_std = 552\n\n\nAntes de aplicar el siguiente paso, es importante notar que es imposible estar absolutamente seguros de que alguna de las dos opciones sea completamente verdadera. Sin embargo, lo que sí podemos hacer es, a partir de un nivel de probabilidad que consideremos aceptable, afirmar que una de las opciones es verdadera. Para ello, se establece un nivel de significancia estadística, que ya definimos en el artículo anterior sobre pruebas de hipótesis. Lo más común es establecer dicho porcentaje en un \\(5\\%\\). Cómo se utiliza este parámetro se entenderá mejor en la siguiente parte."
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html#tipos-de-prueba",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html#tipos-de-prueba",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "Tipos de Prueba",
    "text": "Tipos de Prueba\nLas pruebas de hipótesis pueden ser unilaterales, lo cual significa que solo se consideran las áreas y puntos críticos de una cola de la curva de la distribución (puede ser la izquierda o la derecha), o bilaterales, en las cuales, en lugar de considerar solo una cola, se divide la significancia estadística en dos y se estudian ambas colas, la derecha y la izquierda.\n\nEjemplo (Prueba Bilateral):\nPara nuestro problema, no se nos pide diferenciar si los posibles efectos de la campaña de marketing fueron positivos o negativos para las ventas, así que consideraremos ambos. Por lo tanto, la prueba será bilateral. En este caso, asumiendo la significancia estadística más común del \\(\\frac{0.05}{2}\\) y \\(1-\\frac{0.05}{2}\\), lo que equivale al rango de aceptación de la hipótesis nula en probabilidades, que será \\((0.025,0.975)\\).\n\n\nCode\n# Establezcamos la significancia y la probabilidad de las colas:\nSignificancia = 0.05\nCola_Izq_Prob = 0.025\nCola_Der_Prob = 0.975\n\n\nAhora bien, existen tres métodos para realizar la prueba referente al valor de la media de la población. Estos son los siguientes:"
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html#método-del-valor-crítico",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html#método-del-valor-crítico",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "Método del Valor Crítico",
    "text": "Método del Valor Crítico\nEn este método, según el rango de probabilidades establecido anteriormente con base en la significancia estadística, calcularemos los puntos críticos \\(x_1\\) y \\(x_2\\) utilizando la función punto de probabilidad (PPF) que, como mencionamos antes, se determinarán sobre la distribución normal con los parámetros de la población:\n\\[x_1=F^{-1}(\\text{Probabilidad Cola Izquierda };\\mu,\\sigma)\\] \\[x_2=F^{-1}(\\text{Probabilidad Cola Derecha };\\mu,\\sigma)\\]\nDespués de ello, debemos determinar si la media muestral \\(\\overline{x}\\) se encuentra dentro o no del rango de los puntos críticos \\((x_1,x_2)\\). Si ese es el caso y \\(\\overline{x}\\in{(x_1,x_2)}\\), entonces se acepta la hipótesis nula, de lo contrario se rechaza.\n\nEjemplo (Método del Valor Crítico):\nYa que antes establecimos los parámetros, podremos calcular la funcion de punto de probabilidad PPF por medio del módulo SciPy de Python:\n\n\nCode\n# Importamos el paquete necesario:\nfrom scipy.stats import norm\n\n# Calculamos los puntos críticos usando SciPy:\nx_1 = norm.ppf(Cola_Izq_Prob, media_poblacional, desv_std).round(4)\nx_2 = norm.ppf(Cola_Der_Prob, media_poblacional, desv_std).round(4)\nprint(f\"Los valores críticos son x_1={x_1} y x_2={x_2}\")\n\n# Veríficamos si la media muestral esta dentro o no del rango de los puntos críticos, para rechazar o aceptar H_0:\nif media_muestral &gt;= x_1 and media_muestral &lt;= x_2:\n  print('No puede rechazar la hipótesis nula H_0.')\nelse:\n  print('Se puede rechazar la hipótesis nula H_0.')\n\n\nLos valores críticos son x_1=9263.0999 y x_2=11426.9001\nSe puede rechazar la hipótesis nula H_0.\n\n\nDebido a la naturaleza de las pruebas estadísticas, podremos observar este resultado gráficamente:\n\n\nCode\n# Importemos los paquetes necesarios para realizar las gráficas:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Creemos la distribución de datos de la Población:\nx = np.linspace(media_poblacional - 3*desv_std, media_poblacional + 3*desv_std, 1000)\n# Utilizamos Scipy para generar la distribución de probabilidad:\nf_x = norm.pdf(x, media_poblacional, desv_std)\n\n# Crear la gráfica:\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=f_x, color='blue')\n\n# Área bajo la curva entre 8690 y x_1:\nx_fill_1 = np.linspace(8690, x_1, 2100)\ny_fill_1 = norm.pdf(x_fill_1, media_poblacional, desv_std)\nplt.fill_between(x_fill_1, y_fill_1, color='cyan', alpha=0.5, label='Región de rechazo')\n\n# Área bajo la curva entre x_2 y 12000:\nx_fill_2 = np.linspace(x_2, 12000, 2100)\ny_fill_2 = norm.pdf(x_fill_2, media_poblacional, desv_std)\nplt.fill_between(x_fill_2, y_fill_2, color='cyan', alpha=0.5, label='de la Hipótesis Nula')\n\n# Agregemos una línea que establezca donde se encuentra la media muestral:\nplt.axvline(media_muestral, color='red', linestyle='--', label=r'Media Muestral $\\overline{x}$')\n\n# Añadir las etiquetas y la leyenda\nplt.legend()\n\n# Insertando los correspondientes títulos:\nplt.title(r'Comparación den la Media Muestral $\\overline{x}$ con los Valores Críticos')\nplt.xlabel('Valor Total de las Ventas ($)')\nplt.ylabel('Valores de Probabilidad')\n\n# Mostrar la gráfica:\nplt.show()"
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html#método-del-p-valor",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html#método-del-p-valor",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "Método del \\(p\\)-Valor",
    "text": "Método del \\(p\\)-Valor\nA diferencia del método anterior, este busca calcular directamente la probabilidad de obtener la media muestral \\(\\overline{x}\\) dentro de la distribución de probabilidad de la media poblacional \\(\\mu\\) con desviación estándar \\(\\sigma\\). Esto dependerá del tipo de prueba en el que estemos. Si la prueba es unilateral, el \\(p\\)-valor corresponderá a su valor en la función acumulativa de probabilidad (CDF) con los parámetros poblacionales:\n\\[p=F(\\overline{x};\\mu,\\sigma).\\]\nEn el caso de las pruebas bilaterales, se determina el valor \\(p\\) seleccionando el menor valor de la función (CDF) entre las dos colas. Estos pueden ser: \\[\n\\begin{align}\np_1 &= F(\\overline{x};\\mu,\\sigma),\\\\\np_2 &= 1-F(\\overline{x};\\mu,\\sigma).\n\\end{align}\n\\]\nLuego, dado que dicha prueba debe tomar en cuenta la probabilidad dentro de un intervalo para representar las dos colas, se duplica dicho valor mínimo: \\[p=2\\min(p_1,p_2)\\]\nCon lo cual obtendremos el valor \\(p\\).\nDicho \\(p\\)-valor no es la probabilidad de que la hipótesis nula sea cierta bajo el resultado muestral, sino la probabilidad de obtener el resultado muestral dadon la hipótesis nula como cierta. Por lo cual, si dicho \\(p\\)-valor es mayor a la significancia estadística, se tendrá que aceptar; si en cambio, \\(p\\) es menor a la significancia, entonces la hipótesis nula se rechazará. Para terminar con este método, basta decir que hoy en día es el más utilizado debido a que es el más fácilmente aplicable a software de cómputo.\n\nEjemplo (Método del \\(p\\)-Valor):\nEn el caso que estamos siguiendo, al ser \\(\\overline{x}\\) mayor a \\(\\mu\\), por sentido común, el mínimo de los valores \\(p_1\\) y \\(p_2\\) debe ser \\(p_2\\). Igualmente, incluso sin esta suposición, podremos realizar todo el método por medio de Python:\n\n\nCode\n# Utilizamos la función CDF para determinar p_1:\np_1 = norm.cdf(media_muestral, media_poblacional, desv_std)\np_2 = 1-norm.cdf(media_muestral, media_poblacional, desv_std)\n# Calculamos p_2 y p por medio de la simetría:\np = 2*min(p_1,p_2) \nprint(f\"El p-valor es de {round(p,4)}\")\n\n# Veríficamos si el p-valor es superior o inferior a la significancia para aceptar o negar H_0:\nif p &lt; Significancia:\n print(\"La hipótesis nula H_0 se puede rechazar.\")\nelse:\n print(\"No se puede rechazar la hipótesis nula H_0.\")\n\n\nEl p-valor es de 0.0189\nLa hipótesis nula H_0 se puede rechazar."
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html#método-de-los-intervalos-de-confianza",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html#método-de-los-intervalos-de-confianza",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "Método de los Intervalos de Confianza",
    "text": "Método de los Intervalos de Confianza\nEste es un método similar al del valor crítico, solo que en este caso se creará un intervalo de confianza de \\(1-\\text{significancia estadística}\\) alrededor de la distribución normal estándar reescalada a la de la media muestral con la desviación estándar, y se determinará si la media poblacional de la hipótesis nula se encuentra dentro de dicho intervalo. Recordemos que esto se hace mediante la Unidad Tipificada o z-valores, por medio de la siguiente fórmula:\n\\[\\text{Límites del Intervalo de Confianza}=\\overline{x}\\pm z\\sigma\\]\nAl ser la versión estandarizada de la distribución normal de la que establecemos los valores críticos \\(z\\), estos siempre serán los mismos. Para evitar cálculos innecesarios, se muestran en la tabla de valores críticos de \\(z\\) en pruebas con intervalos de confianza:\n\\[\n\\begin{array}{|c|c|c|}\n\\hline\n\\text{Nivel Sig.} & \\text{Unilateral} & \\text{Bilateral} \\\\\n\\hline\n5\\% & +1.645 (\\text{o bien} -1.645) & \\pm1.96 \\\\\n\\hline\n1\\% & +2.33 (\\text{o bien} -2.33) & \\pm2.58 \\\\\n\\hline\n\\end{array}\n\\]\nNótese que este método es, en esencia, igual al del método de los valores críticos, solo que en vez de dejar igual la distribución de los datos y calcular los puntos críticos dada la significancia estadística, se reescala la distribución normal estándar a la media muestral junto con la desviación estándar, se toman los puntos críticos de la distribución normal estándar que ya conocemos de antemano para reajustarlos y, por último, se verifica si la media poblacional se encuentra dentro o fuera del intervalo de confianza. Dado el caso, aceptaremos o rechazaremos la hipótesis nula \\(H_0\\) respectivamente.\nTambién se podría realizar el proceso inverso, es decir, normalizar y comparar la media muestral con la media poblacional dentro de la propia distribución normal estándar. Esto se haría mediante la fórmula de los \\(z\\)-valores sin despejar que ya conocemos:\n\\[z_{\\overline{x}}=\\frac{\\overline{x}-\\mu}{\\sigma}.\\]\nCualquiera de los dos métodos es igualmente válido.\n\nEjemplo (Método del Intervalo de Confianza):\nEstablezcamos los límites del intervalo de confianza en python y veamos si la media poblacional se encuentra dentro o fuera de ella:\n\n\nCode\n# Veamos los límites del intervalo de confianza re-escalados:\nz_1 = media_muestral-(1.96)*desv_std \nz_2 = media_muestral+(1.96)*desv_std \nprint(f\"El intervalo de confianza del 95% es ({z_1},{z_2})\")\n\n# Verifiquemos si la media poblacional se encuentra dentro o fuera del intervalo:\nif media_poblacional &gt;= z_1 and media_poblacional &lt;= z_2:\n print(\"Se puede aceptar la hipótesis nula H_0.\")\nelse:\n print(\"Se puede rechazar la hipótesis nula H_0.\")\n\n\nEl intervalo de confianza del 95% es (10559.08,12722.92)\nSe puede rechazar la hipótesis nula H_0.\n\n\nEn la otra forma, al no necesitar redimensionar los puntos críticos, debemos comparar la media muestral \\(\\overline{x}\\) normalizandola:\n\n\nCode\n# Normalicemos la media muestral:\nz_x = (media_muestral-media_poblacional)/desv_std\nprint(f\"La media muestral normalizada es z_x = {round(z_x,4)}\")\n\n# Ahora verificamos si dicha media se encuentra dentro o fuera del rango estádar:\nif z_x&gt;=-1.96 and z_x&lt;=1.96:\n    print(\"Se puede aceptar la hipótesis nula.\")\nelse:\n    print(\"Se puede rechazar la hipótesis nula.\")\n\n\nLa media muestral normalizada es z_x = 2.3478\nSe puede rechazar la hipótesis nula.\n\n\nAl igual que en el método del valor crítico, se puede mostrar gráficamente el resultado anterior. Sin embargo, en este caso esto se hace sobre la distribución normal estándar, nótese que es una evidencia clara de la similitud entre ambas pruebas:\n\n\nCode\n# Gracias a la secuencialidad de comandos podremos utilizar las mismas variables re-valuandolas, para esta gráfica:\n\n# Creemos la distribución de datos de la Población:\nx = np.linspace(-3,3, 2000)\n# Utilizamos Scipy para generar la distribución de probabilidad:\nf_x = norm.pdf(x, 0, 1)\n\n# Crear la gráfica:\nplt.figure(figsize=(10, 6))\nsns.lineplot(x=x, y=f_x, color='blue')\n\n# Área bajo la curva entre -3 y x_1:\nx_fill_1 = np.linspace(-3, -1.96, 2000)\ny_fill_1 = norm.pdf(x_fill_1, 0, 1)\nplt.fill_between(x_fill_1, y_fill_1, color='cyan', alpha=0.5, label='Región de rechazo')\n\n# Área bajo la curva entre x_2 y 3:\nx_fill_2 = np.linspace(1.96, 3, 2000)\ny_fill_2 = norm.pdf(x_fill_2, 0, 1)\nplt.fill_between(x_fill_2, y_fill_2, color='cyan', alpha=0.5, label='de la Hipótesis Nula')\n\n# Agregemos una línea que establezca donde se encuentra la media muestral:\nplt.axvline(z_x, color='red', linestyle='--', label=r'Media Muestral Normalizada $z_{\\overline{x}}$')\n\n# Añadir las etiquetas y la leyenda\nplt.legend()\n\n# Insertando los correspondientes títulos:\nplt.title(r'Comparación den la Media Muestral Normalizada $z_{\\overline{x}}$ en la Distribución Normal Estándar')\nplt.xlabel(r'Distancia de los valores a la Media en ($\\sigma$)')\nplt.ylabel('Valores de Probabilidad')\n\n# Mostrar la gráfica:\nplt.show()"
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html#notas-finales",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html#notas-finales",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "Notas Finales",
    "text": "Notas Finales\nDespués de realizar estos métodos, dependiendo del resultado, deberemos tomar una decisión. Como ya se mencionó, no podemos asegurar por completo que alguna de las hipótesis es absolutamente cierta. Sin embargo, si nuestras pruebas están bien hechas, es decir, si tomamos una muestra con el tamaño adecuado, que no estaba sesgada y cuyos datos se comportaban según una distribución normal, es poco probable que estos caigan en algún tipo de error. De cómo saber cuál debe ser dicho tamaño y cómo calcular los valores \\(\\alpha\\) y \\(\\beta\\) de los errores tipo I y II respectivamente, hablaré en un futuro artículo.\n\nEjemplo (Toma de Decisión):\nDado que los tres métodos que aplicamos dieron como resultado que debemos rechazar la hipótesis nula, es razonable asumirlo. Así, podemos dar por verdadero que el resultado de la media muestral \\(\\overline{x}\\) no pudo surgir de forma puramente aleatoria de los datos poblacionales. Se llega a la conclusión de que la campaña de marketing tuvo, efectivamente, un efecto real en las ventas."
  },
  {
    "objectID": "posts/tipos-de-pruebas-de-hipotesis.html#referencias",
    "href": "posts/tipos-de-pruebas-de-hipotesis.html#referencias",
    "title": "Pruebas de Hipótesis Usando la Distribución Normal",
    "section": "Referencias",
    "text": "Referencias\n\nDevore, Jay L. Probabilidad y estadística para ingeniería y ciencias. Novena edición, Pearson, 2011.\nNield, Thomas.(2020).Essential Math for Data Science. O’Reilly Media."
  },
  {
    "objectID": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html",
    "href": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html",
    "title": "Predicción del Precio de los Vehículos por medio de Arboles de Desición.",
    "section": "",
    "text": "Nuestro objetivo en este caso es crear un modelo de pronóstico para los precios de venta de automóviles, para ello tenemos a nuestra disposición la base de datos de una concecionaria de autos de la India con varias sucursales (esto implica que los precios estan dados en rupias indias), antes que nada deberemos importar los datos y ver sus variables y características:\n\n\nCode\n# Importemos los paquetes necesarios para leer los datos:\nimport pandas as pd\n\n# Extraigamos la información local:\ndatos = pd.read_csv(\"DataSets/cardekho_data.csv\")\n# Vemeamos los datos con los que vamos a trabajar:\ndatos\n\n\n\n\n\n\n\n\n\n\nCar_Name\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nFuel_Type\nSeller_Type\nTransmission\nOwner\n\n\n\n\n0\nritz\n2014\n3.35\n5.59\n27000\nPetrol\nDealer\nManual\n0\n\n\n1\nsx4\n2013\n4.75\n9.54\n43000\nDiesel\nDealer\nManual\n0\n\n\n2\nciaz\n2017\n7.25\n9.85\n6900\nPetrol\nDealer\nManual\n0\n\n\n3\nwagon r\n2011\n2.85\n4.15\n5200\nPetrol\nDealer\nManual\n0\n\n\n4\nswift\n2014\n4.60\n6.87\n42450\nDiesel\nDealer\nManual\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n296\ncity\n2016\n9.50\n11.60\n33988\nDiesel\nDealer\nManual\n0\n\n\n297\nbrio\n2015\n4.00\n5.90\n60000\nPetrol\nDealer\nManual\n0\n\n\n298\ncity\n2009\n3.35\n11.00\n87934\nPetrol\nDealer\nManual\n0\n\n\n299\ncity\n2017\n11.50\n12.50\n9000\nDiesel\nDealer\nManual\n0\n\n\n300\nbrio\n2016\n5.30\n5.90\n5464\nPetrol\nDealer\nManual\n0\n\n\n\n\n301 rows × 9 columns\n\n\n\n\nVemos que los datos poseen \\(9\\) variables, a saber: nombre del auto, año, precio de venta, precio actual (los cuales intuimos estan en millones), Kilometraje, tipo de combustible tipo de vendedor, transmición y propietario. Estaría bien conocer cuales son las clases de nuestras variables categóricas (y al mismo tiempo la frecuencia de cada uno):\n\n\nCode\nfor i in [\"Car_Name\", \"Fuel_Type\", \"Seller_Type\", \"Transmission\", \"Year\"]:\n    print(datos[i].value_counts())\n\n\nCar_Name\ncity                        26\ncorolla altis               16\nverna                       14\nfortuner                    11\nbrio                        10\n                            ..\nHonda CB Trigger             1\nYamaha FZ S                  1\nBajaj Pulsar 135 LS          1\nActiva 4g                    1\nBajaj Avenger Street 220     1\nName: count, Length: 98, dtype: int64\nFuel_Type\nPetrol    239\nDiesel     60\nCNG         2\nName: count, dtype: int64\nSeller_Type\nDealer        195\nIndividual    106\nName: count, dtype: int64\nTransmission\nManual       261\nAutomatic     40\nName: count, dtype: int64\nYear\n2015    61\n2016    50\n2014    38\n2017    35\n2013    33\n2012    23\n2011    19\n2010    15\n2008     7\n2009     6\n2006     4\n2005     4\n2003     2\n2007     2\n2018     1\n2004     1\nName: count, dtype: int64\n\n\nDebido a la gran variedad de categorías presentes en Car_Name y a que no aportará al modelo información relevante, es mejor descartar dicha variable:\n\n\nCode\ndatos.drop(columns=\"Car_Name\", inplace=True)\n\n\nAhora deberiamos recavar información básica de nuestros datos para poder limpiarlos correctamente:\n\n\nCode\ndatos.describe()\n\n\n\n\n\n\n\n\n\n\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nOwner\n\n\n\n\ncount\n301.000000\n301.000000\n301.000000\n301.000000\n301.000000\n\n\nmean\n2013.627907\n4.661296\n7.628472\n36947.205980\n0.043189\n\n\nstd\n2.891554\n5.082812\n8.644115\n38886.883882\n0.247915\n\n\nmin\n2003.000000\n0.100000\n0.320000\n500.000000\n0.000000\n\n\n25%\n2012.000000\n0.900000\n1.200000\n15000.000000\n0.000000\n\n\n50%\n2014.000000\n3.600000\n6.400000\n32000.000000\n0.000000\n\n\n75%\n2016.000000\n6.000000\n9.900000\n48767.000000\n0.000000\n\n\nmax\n2018.000000\n35.000000\n92.600000\n500000.000000\n3.000000\n\n\n\n\n\n\n\n\nComo se puede ver, los valores de Selling_Price, Present_Price y Kms_Driven presentan una desviación estándar atípica, esto devido a valores atípicamente altos como se puede corroborar atravez de los cuartiles, sin embargo, no seria conveniente ajustar los valores de Selling_Price debido a que es la variable que queremos predecir, cosa que no pasa con las otras \\(2\\).\nAntes de hacer algun cambio, verifiquemos dichos valores atípicos de las otras \\(2\\) variables a traves de diagramas de caja:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfigura,ejes = plt.subplots(1, 2, figsize=(8,4))\n\nsns.boxplot(y=\"Present_Price\", data=datos, ax=ejes[0])\nejes[0].set_title('Gráfico de Caja del Precio Actual')\n\nsns.boxplot(y=\"Kms_Driven\", data=datos, ax=ejes[1])\nejes[1].set_title('Grafico de Caja del Kilometraje')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPor lo que queda bastante claro que existen valores atípicos que en este caso debemos de limpiar.\nEsto lo haremos calculando un rango que separe los datos comúnes de los atípicos, esto se hace mediante el Rango Intercuartilico (IQR) al cual debemos asignarle un coeficiente \\(1,5\\). El coeficiente \\(1,5\\) se elige empíricamente y es ampliamente aceptado en la práctica estadística.Representa una distancia suficiente para capturar valores que son inusualmente altos/bajos. Además, tendremos que utilizar algebra lineal por lo que importaremos NumPy tambien:\n\n\nCode\n# instalamos NumPy:\nimport numpy as np\n\n# Hacer esto sera más fácil mediante una función:\ndef filtrador(col):\n# Primero calculamos el primer(25%) y tercer(75%) cuartil:\n Q1 = datos[col].quantile(0.25)\n Q3 = datos[col].quantile(0.75)\n# Calculamos el Rango Intercuartilico:\n IQR = Q3-Q1   \n# Determinamos los límites para clasificar outliers: \n limite_superior = Q3+1.5*IQR\n limite_inferior = Q1-1.5*IQR\n# Ahora filtramos los datos que esten fuera de dichos límites usando NumPy: \n datos[col] = np.where(datos[col] &gt; limite_superior, limite_superior, \n                                   np.where(datos[col] &lt; limite_inferior, limite_inferior, datos[col]))\n    \n# Claro que esta función es optimizable, sin embargo la prefiero asi por ser más ilustrativa.\n    \n# Ahora usemos esta función para filtrar nuestras variables:\nfiltrador(\"Present_Price\")\nfiltrador(\"Kms_Driven\")\n\n\nPodemos utilizar nuevamente los diagrámas de caja para verificar que los Outsiders han sido correctamente filtrados:\n\n\nCode\nfigura,ejes = plt.subplots(1, 2, figsize=(8,4))\n\nsns.boxplot(y=\"Present_Price\", data=datos, ax=ejes[0])\nejes[0].set_title('Gráfico de Caja del Precio Actual')\n\nsns.boxplot(y=\"Kms_Driven\", data=datos, ax=ejes[1])\nejes[1].set_title('Grafico de Caja del Kilometraje')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPara términar con el procesamiento de los datos debemos notar que existen al menos \\(3\\) variables categóricas que pueden transformarse en variables numéricas discretas estas son Fuel_Type, Seller_Type y Transmission. Esto se puede hacer por dos medios distintos:\n\nEl Label Encoding que es simplemente asignar un número a cada categoría, por ejemplo asignar \\(1\\) al diesel, \\(2\\) a la gasolina y \\(3\\) al CNG.\nEl One Hot Encoding se trata de sustituir cada categoría por una matriz de entradas binarias. Para este proyecto utilizaremos este último método debido a que el label encoding puede hacer que nuestro modelo de regresión otorge un “mayor valor” que no se corresponde a la realidad a las categorias con un número mayor o menor asignado.\n\nLo anterior lo podemos realizar mediante el método del mismo nombre de Sklearn.\n\n\nCode\n# Importemos el respectivo método\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Definamos los parámetros de nuestra función de codificación \ncodificador = OneHotEncoder(sparse_output=False, drop='first')\n# Aplicamos la función, lo que nos dara las columnas de sus matrices binarias\ncodificador_cols = codificador.fit_transform(datos[['Fuel_Type', 'Seller_Type', 'Transmission']])\n\n# Debemos extraer las categorías de la codificación\ncategorias = codificador.categories_\n\n# Establecemos los nombres em base a las categorías anteriores (a exepción de la primera)\nnombres_categorias = []\nfor i, category_list in enumerate(categorias):\n    nombres_categorias.extend([f'{datos.columns[i + 2]}_{category}' for category in category_list[1:]])\n\n# Establecemos el DataFrame codificado\ncodificacion_datos =  pd.DataFrame(codificador_cols, columns=nombres_categorias)\n\n# Concatenamos los datos con los datos codificados\ndatos_codificados = pd.concat([datos.drop(['Fuel_Type', 'Seller_Type', 'Transmission'], axis=1), codificacion_datos], axis=1)\n\n# Finalmente observemos el DataFrame codificado\ndatos_codificados\n\n\n\n\n\n\n\n\n\n\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nOwner\nPresent_Price_Diesel\nPresent_Price_Petrol\nKms_Driven_Individual\nFuel_Type_Manual\n\n\n\n\n0\n2014\n3.35\n5.59\n27000.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n2013\n4.75\n9.54\n43000.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n2\n2017\n7.25\n9.85\n6900.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n2011\n2.85\n4.15\n5200.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n4\n2014\n4.60\n6.87\n42450.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n296\n2016\n9.50\n11.60\n33988.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n297\n2015\n4.00\n5.90\n60000.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n298\n2009\n3.35\n11.00\n87934.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n299\n2017\n11.50\n12.50\n9000.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n300\n2016\n5.30\n5.90\n5464.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n301 rows × 9 columns"
  },
  {
    "objectID": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html#pre-procesamiento-y-limpieza-de-datos",
    "href": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html#pre-procesamiento-y-limpieza-de-datos",
    "title": "Predicción del Precio de los Vehículos por medio de Arboles de Desición.",
    "section": "",
    "text": "Nuestro objetivo en este caso es crear un modelo de pronóstico para los precios de venta de automóviles, para ello tenemos a nuestra disposición la base de datos de una concecionaria de autos de la India con varias sucursales (esto implica que los precios estan dados en rupias indias), antes que nada deberemos importar los datos y ver sus variables y características:\n\n\nCode\n# Importemos los paquetes necesarios para leer los datos:\nimport pandas as pd\n\n# Extraigamos la información local:\ndatos = pd.read_csv(\"DataSets/cardekho_data.csv\")\n# Vemeamos los datos con los que vamos a trabajar:\ndatos\n\n\n\n\n\n\n\n\n\n\nCar_Name\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nFuel_Type\nSeller_Type\nTransmission\nOwner\n\n\n\n\n0\nritz\n2014\n3.35\n5.59\n27000\nPetrol\nDealer\nManual\n0\n\n\n1\nsx4\n2013\n4.75\n9.54\n43000\nDiesel\nDealer\nManual\n0\n\n\n2\nciaz\n2017\n7.25\n9.85\n6900\nPetrol\nDealer\nManual\n0\n\n\n3\nwagon r\n2011\n2.85\n4.15\n5200\nPetrol\nDealer\nManual\n0\n\n\n4\nswift\n2014\n4.60\n6.87\n42450\nDiesel\nDealer\nManual\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n296\ncity\n2016\n9.50\n11.60\n33988\nDiesel\nDealer\nManual\n0\n\n\n297\nbrio\n2015\n4.00\n5.90\n60000\nPetrol\nDealer\nManual\n0\n\n\n298\ncity\n2009\n3.35\n11.00\n87934\nPetrol\nDealer\nManual\n0\n\n\n299\ncity\n2017\n11.50\n12.50\n9000\nDiesel\nDealer\nManual\n0\n\n\n300\nbrio\n2016\n5.30\n5.90\n5464\nPetrol\nDealer\nManual\n0\n\n\n\n\n301 rows × 9 columns\n\n\n\n\nVemos que los datos poseen \\(9\\) variables, a saber: nombre del auto, año, precio de venta, precio actual (los cuales intuimos estan en millones), Kilometraje, tipo de combustible tipo de vendedor, transmición y propietario. Estaría bien conocer cuales son las clases de nuestras variables categóricas (y al mismo tiempo la frecuencia de cada uno):\n\n\nCode\nfor i in [\"Car_Name\", \"Fuel_Type\", \"Seller_Type\", \"Transmission\", \"Year\"]:\n    print(datos[i].value_counts())\n\n\nCar_Name\ncity                        26\ncorolla altis               16\nverna                       14\nfortuner                    11\nbrio                        10\n                            ..\nHonda CB Trigger             1\nYamaha FZ S                  1\nBajaj Pulsar 135 LS          1\nActiva 4g                    1\nBajaj Avenger Street 220     1\nName: count, Length: 98, dtype: int64\nFuel_Type\nPetrol    239\nDiesel     60\nCNG         2\nName: count, dtype: int64\nSeller_Type\nDealer        195\nIndividual    106\nName: count, dtype: int64\nTransmission\nManual       261\nAutomatic     40\nName: count, dtype: int64\nYear\n2015    61\n2016    50\n2014    38\n2017    35\n2013    33\n2012    23\n2011    19\n2010    15\n2008     7\n2009     6\n2006     4\n2005     4\n2003     2\n2007     2\n2018     1\n2004     1\nName: count, dtype: int64\n\n\nDebido a la gran variedad de categorías presentes en Car_Name y a que no aportará al modelo información relevante, es mejor descartar dicha variable:\n\n\nCode\ndatos.drop(columns=\"Car_Name\", inplace=True)\n\n\nAhora deberiamos recavar información básica de nuestros datos para poder limpiarlos correctamente:\n\n\nCode\ndatos.describe()\n\n\n\n\n\n\n\n\n\n\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nOwner\n\n\n\n\ncount\n301.000000\n301.000000\n301.000000\n301.000000\n301.000000\n\n\nmean\n2013.627907\n4.661296\n7.628472\n36947.205980\n0.043189\n\n\nstd\n2.891554\n5.082812\n8.644115\n38886.883882\n0.247915\n\n\nmin\n2003.000000\n0.100000\n0.320000\n500.000000\n0.000000\n\n\n25%\n2012.000000\n0.900000\n1.200000\n15000.000000\n0.000000\n\n\n50%\n2014.000000\n3.600000\n6.400000\n32000.000000\n0.000000\n\n\n75%\n2016.000000\n6.000000\n9.900000\n48767.000000\n0.000000\n\n\nmax\n2018.000000\n35.000000\n92.600000\n500000.000000\n3.000000\n\n\n\n\n\n\n\n\nComo se puede ver, los valores de Selling_Price, Present_Price y Kms_Driven presentan una desviación estándar atípica, esto devido a valores atípicamente altos como se puede corroborar atravez de los cuartiles, sin embargo, no seria conveniente ajustar los valores de Selling_Price debido a que es la variable que queremos predecir, cosa que no pasa con las otras \\(2\\).\nAntes de hacer algun cambio, verifiquemos dichos valores atípicos de las otras \\(2\\) variables a traves de diagramas de caja:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfigura,ejes = plt.subplots(1, 2, figsize=(8,4))\n\nsns.boxplot(y=\"Present_Price\", data=datos, ax=ejes[0])\nejes[0].set_title('Gráfico de Caja del Precio Actual')\n\nsns.boxplot(y=\"Kms_Driven\", data=datos, ax=ejes[1])\nejes[1].set_title('Grafico de Caja del Kilometraje')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPor lo que queda bastante claro que existen valores atípicos que en este caso debemos de limpiar.\nEsto lo haremos calculando un rango que separe los datos comúnes de los atípicos, esto se hace mediante el Rango Intercuartilico (IQR) al cual debemos asignarle un coeficiente \\(1,5\\). El coeficiente \\(1,5\\) se elige empíricamente y es ampliamente aceptado en la práctica estadística.Representa una distancia suficiente para capturar valores que son inusualmente altos/bajos. Además, tendremos que utilizar algebra lineal por lo que importaremos NumPy tambien:\n\n\nCode\n# instalamos NumPy:\nimport numpy as np\n\n# Hacer esto sera más fácil mediante una función:\ndef filtrador(col):\n# Primero calculamos el primer(25%) y tercer(75%) cuartil:\n Q1 = datos[col].quantile(0.25)\n Q3 = datos[col].quantile(0.75)\n# Calculamos el Rango Intercuartilico:\n IQR = Q3-Q1   \n# Determinamos los límites para clasificar outliers: \n limite_superior = Q3+1.5*IQR\n limite_inferior = Q1-1.5*IQR\n# Ahora filtramos los datos que esten fuera de dichos límites usando NumPy: \n datos[col] = np.where(datos[col] &gt; limite_superior, limite_superior, \n                                   np.where(datos[col] &lt; limite_inferior, limite_inferior, datos[col]))\n    \n# Claro que esta función es optimizable, sin embargo la prefiero asi por ser más ilustrativa.\n    \n# Ahora usemos esta función para filtrar nuestras variables:\nfiltrador(\"Present_Price\")\nfiltrador(\"Kms_Driven\")\n\n\nPodemos utilizar nuevamente los diagrámas de caja para verificar que los Outsiders han sido correctamente filtrados:\n\n\nCode\nfigura,ejes = plt.subplots(1, 2, figsize=(8,4))\n\nsns.boxplot(y=\"Present_Price\", data=datos, ax=ejes[0])\nejes[0].set_title('Gráfico de Caja del Precio Actual')\n\nsns.boxplot(y=\"Kms_Driven\", data=datos, ax=ejes[1])\nejes[1].set_title('Grafico de Caja del Kilometraje')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPara términar con el procesamiento de los datos debemos notar que existen al menos \\(3\\) variables categóricas que pueden transformarse en variables numéricas discretas estas son Fuel_Type, Seller_Type y Transmission. Esto se puede hacer por dos medios distintos:\n\nEl Label Encoding que es simplemente asignar un número a cada categoría, por ejemplo asignar \\(1\\) al diesel, \\(2\\) a la gasolina y \\(3\\) al CNG.\nEl One Hot Encoding se trata de sustituir cada categoría por una matriz de entradas binarias. Para este proyecto utilizaremos este último método debido a que el label encoding puede hacer que nuestro modelo de regresión otorge un “mayor valor” que no se corresponde a la realidad a las categorias con un número mayor o menor asignado.\n\nLo anterior lo podemos realizar mediante el método del mismo nombre de Sklearn.\n\n\nCode\n# Importemos el respectivo método\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Definamos los parámetros de nuestra función de codificación \ncodificador = OneHotEncoder(sparse_output=False, drop='first')\n# Aplicamos la función, lo que nos dara las columnas de sus matrices binarias\ncodificador_cols = codificador.fit_transform(datos[['Fuel_Type', 'Seller_Type', 'Transmission']])\n\n# Debemos extraer las categorías de la codificación\ncategorias = codificador.categories_\n\n# Establecemos los nombres em base a las categorías anteriores (a exepción de la primera)\nnombres_categorias = []\nfor i, category_list in enumerate(categorias):\n    nombres_categorias.extend([f'{datos.columns[i + 2]}_{category}' for category in category_list[1:]])\n\n# Establecemos el DataFrame codificado\ncodificacion_datos =  pd.DataFrame(codificador_cols, columns=nombres_categorias)\n\n# Concatenamos los datos con los datos codificados\ndatos_codificados = pd.concat([datos.drop(['Fuel_Type', 'Seller_Type', 'Transmission'], axis=1), codificacion_datos], axis=1)\n\n# Finalmente observemos el DataFrame codificado\ndatos_codificados\n\n\n\n\n\n\n\n\n\n\nYear\nSelling_Price\nPresent_Price\nKms_Driven\nOwner\nPresent_Price_Diesel\nPresent_Price_Petrol\nKms_Driven_Individual\nFuel_Type_Manual\n\n\n\n\n0\n2014\n3.35\n5.59\n27000.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n1\n2013\n4.75\n9.54\n43000.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n2\n2017\n7.25\n9.85\n6900.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n3\n2011\n2.85\n4.15\n5200.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n4\n2014\n4.60\n6.87\n42450.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n296\n2016\n9.50\n11.60\n33988.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n297\n2015\n4.00\n5.90\n60000.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n298\n2009\n3.35\n11.00\n87934.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n299\n2017\n11.50\n12.50\n9000.0\n0\n1.0\n0.0\n0.0\n1.0\n\n\n300\n2016\n5.30\n5.90\n5464.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n\n\n301 rows × 9 columns"
  },
  {
    "objectID": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html#regresión-por-medio-de-los-bosques-aleatorios",
    "href": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html#regresión-por-medio-de-los-bosques-aleatorios",
    "title": "Predicción del Precio de los Vehículos por medio de Arboles de Desición.",
    "section": "2. Regresión por medio de los Bosques Aleatorios",
    "text": "2. Regresión por medio de los Bosques Aleatorios\nRealizaré una breve explicación de la regresión por medio de bosque aleatorio para entender su aplicación como método de Sklearn, que está automatizado, esto conforme avanzamos en nuestro análisis. Lo primero que debemos hacer es separar la variable dependiente \\(y\\), la cual queremos predecir. Esta corresponde al precio de venta (es decir, la variable Selling_Price) de las demás variables independientes \\(X\\) (en mayúscula pues corresponde a una matriz de datos) que utilizaremos para entrenar el modelo y como nuestros valores de entrada para obtener nuestra predicción de precios. Es por ello que codificamos las variables categóricas con el método one-hot encoding. Después de ello, separaremos nuestros datos entre datos de entrenamiento y de prueba para el modelo en una proporción de \\(80\\%\\) y \\(20\\%\\) respectivamente.\n\n\nCode\n# Establecemos las variables independientes y dependientes\nX = datos_codificados.drop(columns=['Selling_Price'])\ny = datos_codificados['Selling_Price']\n\n# Separamos los datos de entrenamiento y prueba\nfrom sklearn.model_selection import train_test_split\nX_ent, X_pru, y_ent, y_pru =  train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Observemos como son los datos de las categorias de entrenamiento\nX_ent.head()\n\n\n\n\n\n\n\n\n\n\nYear\nPresent_Price\nKms_Driven\nOwner\nPresent_Price_Diesel\nPresent_Price_Petrol\nKms_Driven_Individual\nFuel_Type_Manual\n\n\n\n\n184\n2008\n0.750\n26000.0\n1\n0.0\n1.0\n1.0\n1.0\n\n\n132\n2017\n0.950\n3500.0\n0\n0.0\n1.0\n1.0\n1.0\n\n\n194\n2008\n0.787\n50000.0\n0\n0.0\n1.0\n1.0\n1.0\n\n\n75\n2015\n6.800\n36000.0\n0\n0.0\n1.0\n0.0\n1.0\n\n\n111\n2016\n1.500\n8700.0\n0\n0.0\n1.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n2.1. Bagging y CART\nEl bosque aleatorio, como método de aprendizaje automático, es un algoritmo que combina el resultado de varios árboles de decisión. Para ello, utiliza el método de remuestreo de Bagging, que genera subconjuntos de características tomados de forma aleatoria y con reemplazo. Luego, se utiliza el método de regresión con árboles de decisión en cada subconjunto.\nEl algoritmo que genera una regresión con un árbol de decisión se denomina CART y consiste en generar particiones con valores (en el caso de tener solo una variable independiente o característica) o vectores (en el caso de múltiples características). Dichas particiones se realizan utilizando el MSE y la función de costo, generando una parte de un árbol de decisión. Este proceso se repite hasta cumplir con una condición de parada preestablecida, y da como resultado un árbol de decisión que podemos usar para evaluar otros valores o vectores. Si quieren indagar en el algoritmo, pueden verlo en el siguiente enlace. En nuestro caso, nuestra condición de parada será el tamaño máximo que pueden tener los árboles, que será de \\(11\\). Veamos cómo se crea y entrena dicho modelo de regresión usando Sklearn.\n\n\nCode\n# Importemos el respectivo método\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Definimos la funcion del modelo y lo entrenamos con los datos\nmodelo = RandomForestRegressor(random_state=42, max_depth=11)\nmodelo.fit(X_ent, y_ent)\n\n\nRandomForestRegressor(max_depth=11, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=11, random_state=42)\n\n\n\n\n2.2. Predicciones y Medidas de Error\nEl resultado del CART es un árbol de decisión en el cual, para realizar pronósticos, solo tendremos que introducir un valor \\(\\overline{x}\\in{X}\\) y obtener la hoja resultante. En nuestro caso, los datos que vamos a introducir son los datos de categoría de prueba y validación X_pru. Claro que, al tener el bosque aleatorio varios árboles de decisión resultantes, la imagen resultante no será la hoja de un árbol particular, sino más bien un promedio entre dichas hojas. Para medir qué tan acertados fueron dichos pronósticos, disponemos de varias métricas, entre ellas el coeficiente de correlación R cuadrado, el error cuadrático medio MSE y el error absoluto medio MAE. En nuestro caso, utilizaremos las tres y compararemos nuestros resultados con los valores reales de prueba y_pru.\n\n\nCode\n# Realicemos el pronóstico del modelo\ny_pred = modelo.predict(X_pru)\n\n# Importemos las métricas de error\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n\n# Determinamos el error según cada métrica (función práctica):\ndef calculadora_error(a, b):\n    mse = mean_squared_error(a, b)\n    mae = mean_absolute_error(a, b)\n    r2 = r2_score(a, b)\n    print(f'Error cuadrático medio (MSE): {round(mse, 5)}')\n    print(f'Error absoluto medio (MAE): {round(mae, 5)}')\n    print(f'Coeficiente de determinación $r^2$: {round(r2, 5) * 100}%')\n    \n# Usemos la función que creamos antes para determinar el error\ncalculadora_error(y_pru, y_pred)\n\n\nError cuadrático medio (MSE): 1.54604\nError absoluto medio (MAE): 0.7049\nCoeficiente de determinación $r^2$: 93.28800000000001%\n\n\n\n\n2.3. Optimización del Modelo por Hiperparámetros\nViendo el resultado de nuestras medidas de error anteriores, podemos preguntarnos ¿cómo podemos mejorar nuestro modelo? Una de las posibles formas de hacer esto es mediante la búsqueda de los hiperparámetros más óptimos. Con hiperparámetros nos referimos a los parámetros que utiliza el método, ya sea algorítmico o computacional. Sklearn posee una función automatizada de búsqueda aleatoria para dichos parámetros; solo debemos establecer una variable aleatoria discreta uniforme en un rango de búsqueda de los hiperparámetros, esto se hace con Scipy.\n\n\nCode\n# Importamos las funciones de busqueda y de variable aleatoria\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Definimos la distribución de los parámetros para la busqueda aleatoria\ndistribucion_parametros = {\n    'max_depth': randint(1, 50),  \n    'n_estimators': randint(10, 100),  \n    'min_samples_split': randint(2, 10),  \n    'min_samples_leaf': randint(1, 10),  \n}\n\n# Definimos el modelo que vamos a ajustar\nmodelo = RandomForestRegressor()  # Aquí puedes usar el modelo que prefieras\n\n# Establecemos las metricas necesarias para tener un criterio en la busqueda aleatoria\nmetricas = {'mse': 'neg_mean_squared_error', 'r2': 'r2'}\n\n# Ahora definimos el modelo y realizamos el entrenamiento\nbusqueda_aleatoria = RandomizedSearchCV(estimator=modelo, param_distributions=distribucion_parametros, n_iter=100, cv=5, random_state=42, scoring=metricas, refit='mse')\nbusqueda_aleatoria.fit(X_ent, y_ent)\n\n# Obtendremos los hiperparámetros como una propiedad de la función anterior, así también podremos saber cuáles son\nparametros = busqueda_aleatoria.best_params_\nprint(f'Hiperparámetros obtenidos de la busqueda aleatoria: {parametros}')\n\n\nHiperparámetros obtenidos de la busqueda aleatoria: {'max_depth': 27, 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 26}\n\n\nPor último, podremos volver a entrenar nuestro modelo, solo que esta vez con los hiperparámetros que encontramos, para realizar el mejor modelo de regresión posible por este método. Para verificar que esta versión es mejor, usaremos otra vez las métricas de antes.\n\n\nCode\n# De nuevo obtenemos el modelo con los hiperparametros ya integrados como propiedad de la busqueda aleatoria\nmodelo_mejorado = busqueda_aleatoria.best_estimator_\n\n# Al igual que antes realizamos el pronóstico, esta vez con el modelo mejorado\ny_pred = modelo_mejorado.predict(X_pru)\n\n# Veamos si las métricas de error presentan una mejora con respecto al anterior\ncalculadora_error(y_pru, y_pred)\n\n\nError cuadrático medio (MSE): 0.97004\nError absoluto medio (MAE): 0.61936\nCoeficiente de determinación $r^2$: 95.789%"
  },
  {
    "objectID": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html#comparación-gráfica",
    "href": "proyects/Prediccion_de_Precios_con_Bosque_Aleatorio.html#comparación-gráfica",
    "title": "Predicción del Precio de los Vehículos por medio de Arboles de Desición.",
    "section": "2.4. Comparación Gráfica",
    "text": "2.4. Comparación Gráfica\nAunque las métricas de error ya nos proporcionaron información acerca de qué tan acertados son los pronósticos de nuestro modelo, tal vez esto no sea ideal al momento de explicar o ilustrar la efectividad o los fallos del mismo para un grupo de trabajo u otras personas en general. Por ello, de manera complementaria, podemos realizar una gráfica que compare directamente los valores de prueba con los predichos por la regresión.\nAntes de eso debemos organizar y transformar nuestros datos para poderlos gráficar:\n\n\nCode\n# Importemos Pandas para transformar los datos del prónostico y poderlos graficar\nimport pandas as pd\n\n# Las predicciones del modelo estan en formato 'array' de pandas asi que debemos transformarlo en una serie de\n# Pandas\ny_pred = pd.Series(y_pred)\n# Como mencionamos antes, los valores índice de la serie de validación estan mal ordenados asi que debemos resetearlo\ny_pru = y_pru.reset_index(drop=True)\n\n\nUna vez hecho lo anterior, podemos realizar la gráfica comparativa usand MatPlotLib.\n\n\nCode\n# Importemos el paquete necesario para crear las gráficas\nimport matplotlib.pyplot as plt\n\n# Realicemos el gráfico\nplt.figure(figsize=(16,6))\n\nplt.plot(y_pru.index, y_pru, label='Valores reales')\nplt.plot(y_pred.index, y_pred, label='Valores pronosticados por la regresión')\n\nplt.xlabel('Indice')\nplt.ylabel('Precio de Venta de los Autos')\n\nplt.title('Gráfico Comparativo del Modelo')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "proyects/Series_de_Tiempo_con_XGBoost.html#extracción-y-pre-procesamisento-de-los-datos",
    "href": "proyects/Series_de_Tiempo_con_XGBoost.html#extracción-y-pre-procesamisento-de-los-datos",
    "title": "Modelo de Pronóstico para el S&P \\(500\\).",
    "section": "1.Extracción y Pre-procesamisento de los Datos:",
    "text": "1.Extracción y Pre-procesamisento de los Datos:\nPrimero debemos extraer los datos de Apertura, Cierre, Mayor/Menor valor y el Volumen de Mercado, tambien conocidos como OHLCV del índice S&P \\(500\\) en la bolsa de valores. Para ello utilizaremos la API de uso parcialmente gratuito alphavantage en formato JSON y procesarlo de tal manera que podamos trabajar con ellos.\nPara esto debemos verificar que nuestra petición será respondida:\n\n\nCode\n# Realicemos un solicitud de Datos de la API:\napi_url = 'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=SPY&outputsize=full&apikey=EMLUJFB0MBBTTW4O'\nrespuesta  = requests.get(api_url)\n\n# Verifiquemos que obtuvimos los datos de la API y veamos cuales son sus \"Llaves\":\nif respuesta.status_code == 200:\n archivo = respuesta.json()\n print(f'Llaves del archivo JSON: {archivo.keys()}')\nelse: print('Error al obtener los datos de la API')\n\n# Transformemos nuestros datos en un DataFrame de Pandas:\nsp = pd.DataFrame.from_dict(archivo['Time Series (Daily)'], orient='index')\n\n# Realicemos el procesamiento de los datos:\nsp = sp.rename(columns={'1. open':'apertura', '2. high':'mayor', '3. low':'menor', '4. close':'cierre', '5. volume':'volumen'})\nsp = sp[::-1]\nsp = sp.applymap(lambda x: pd.to_numeric(x, errors='coerce') if sp.columns.name != 'index' else x)\nsp.index = pd.to_datetime(sp.index, errors='coerce', infer_datetime_format=True)\n\nprint('DataFrame de nuestra Serie de Tiempo:')\nprint(sp)\n\n\nLlaves del archivo JSON: dict_keys(['Meta Data', 'Time Series (Daily)'])\nDataFrame de nuestra Serie de Tiempo:\n            apertura     mayor     menor    cierre   volumen\n1999-11-01  136.5000  137.0000  135.5625  135.5625   4006500\n1999-11-02  135.9687  137.2500  134.5937  134.5937   6516900\n1999-11-03  136.0000  136.3750  135.1250  135.5000   7222300\n1999-11-04  136.7500  137.3593  135.7656  136.5312   7907500\n1999-11-05  138.6250  139.1093  136.7812  137.8750   7431500\n...              ...       ...       ...       ...       ...\n2023-11-21  453.1842  454.1310  451.9599  453.2700  49244639\n2023-11-22  454.9800  456.3800  453.8895  455.0200  59446573\n2023-11-24  455.0700  455.5000  454.7300  455.3000  29737375\n2023-11-27  454.6500  455.4901  454.0799  454.4800  50505985\n2023-11-28  454.0800  456.2700  453.5000  454.9300  61136740\n\n[6058 rows x 5 columns]\n\n\nSobre los datos se puede decir que estos son los datos historicos del OHLCV de más de \\(20\\) años con una frecuencia Diaria, más especificamente, datos desde el año \\(1999\\) hasta la el día de la última petición hecha.\nPara verificar que el tratamiento de nuestros datos fue el correcto, veamos una gráfica de los mismos:\n\n\nCode\nplt.figure(figsize=(14,4))\n\nplt.plot(sp.index, sp['cierre'], label='Valor de las Acciones durante el Cierre')\n\nplt.xlabel('Indice de Orden Temporal')\nplt.ylabel('Precio (US)')\n\nplt.title('Gráfico de las Series de Tiempo')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n### 1.1. Arreglando nuestros datos: Para poder crear un modelo satisfactorio para pronosticar los valores futuros de nuestros datos tendremos que comparar las predicciones de dicho modelo con algunos datos. Para ello separaremos nuestros datos entre los de entrenamiento y validación, estos últimos corresponderan a aproximadamente \\(3\\) meses o lo que es igual estos seran casi \\(93\\) valores.\nPara ello primero debemos transformar dichos datos en un objeto “Serie de Tiempo” en Darts, despues rellenar los posibles valores faltantes y escalar dichos valores dentro de un rango \\((0,1)\\). Para ello crearemos un Pipline simple.\n\n\nCode\n# Instanciemos nuestros datos como Series de Tiempo en Darts:\nTS_SP = TimeSeries.from_dataframe(sp, freq='D')\n\n# Construyamos una serie de instrucciones pipline, para rellenar y escalar las Series:\npipeline = Pipeline([MissingValuesFiller(), Scaler()])\n\n# Procesemos las series de tiempo:\nPTS_SP = pipeline.fit_transform(TS_SP)\n\n# Por último separemos los datos de Entrenamiento y validación:\nSP_ENT, SP_VAL = PTS_SP.split_before(pd.Timestamp('20230901'))\n\n\n\n\nCode\n# Veamos como se comporta dichas transformaciones y la separación que hicimos gráficamente:\nSP_ENT_GRA = SP_ENT.pd_dataframe()\nSP_VAL_GRA = SP_VAL.pd_dataframe()\n\nplt.figure(figsize=(14,4))\n\nplt.plot(SP_ENT_GRA.index, SP_ENT_GRA['cierre'], label='Valores de Entrenamiento')\nplt.plot(SP_VAL_GRA.index, SP_VAL_GRA['cierre'], label='Valores de Prueba')\n\nplt.xlabel('Indice de Orden Temporal')\nplt.ylabel('Valor de Cierre de la Accion (US)')\n\nplt.title('Gráfico de Cierre Procesado')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n1.1.1. Serie de Tiempo Diferenciada.\nLos valores de esta serie de tiempo no parecen poseer una tendencia o una estacionalidad clara, lo cual podría complicarnos la construcción de un modelo de pronóstico clásico. Por lo tanto, podemos utilizar la “Diferenciación” para transformar los datos de tal manera que podamos extraer información útil de ellos. Además de esto, eliminaremos los datos atípicos y realizaremos su preprocesamiento.\n\n\nCode\n# Llamaremos a la serie de tiempo diferenciada como sigue:\nD_TS_SP = PTS_SP.diff(n=1)\n\n# Eliminemos los Outsiders:\nD_TS_SP = D_TS_SP.pd_dataframe()\n\nvariables_a_filtrar = ['apertura', 'mayor', 'menor', 'cierre']\n\nfor variable in variables_a_filtrar:\n    D_TS_SP[variable] = D_TS_SP[variable].apply(lambda x: x if (x &gt;= -1.5 and x &lt;= 1.5) else None)\n\nD_TS_SP = TimeSeries.from_dataframe(D_TS_SP)\n\n# Normalicemos y rellenemos los datos:\nD_PTS_SP = pipeline.fit_transform(D_TS_SP)\n\n\n\n\nCode\n# Separemos además esta serie diferenciada entre entrenamiento y validación:\nD_SP_ENT, D_SP_VAL = D_PTS_SP.split_before(pd.Timestamp('20230901'))\n\n\n\n\nCode\n# Podremos tambien graficar dichas series diferenciadas:\nD_PTS_SP_GRA = D_PTS_SP.pd_dataframe()\nD_SP_ENT_GRA = D_SP_ENT.pd_dataframe()\nD_SP_VAL_GRA = D_SP_VAL.pd_dataframe()\n\nplt.figure(figsize=(14,4))\n\nplt.plot(D_SP_ENT_GRA.index, D_SP_ENT_GRA['cierre'], label='Valores de entrenamiento diferenciados')\nplt.plot(D_SP_VAL_GRA.index, D_SP_VAL_GRA['cierre'], label='Valores de prueba diferenciados')\n\nplt.xlabel('Indice de Orden Temporal')\n\nplt.title('Gráfico de Cierre Procesado')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nEvidentemente, la Serie de Tiempo diferenciada no posee tendencia; aun asi podria resultar útil para hallar un modelo de pronóstico."
  },
  {
    "objectID": "proyects/Series_de_Tiempo_con_XGBoost.html#estudio-preliminar.",
    "href": "proyects/Series_de_Tiempo_con_XGBoost.html#estudio-preliminar.",
    "title": "Modelo de Pronóstico para el S&P \\(500\\).",
    "section": "2. Estudio Preliminar.",
    "text": "2. Estudio Preliminar.\nPara poder hallar un modelo que pueda realizar un pronóstico satisfactorio de los datos debemos extraer información de las mismas; además de las propiedades estadísticas básicas que obtendremos mediante la propiedad describe(), para las series de tiempo tendremos que buscar otro tipo de propiedades.\n\n\nCode\nSP_ENT_GRA.describe()\n\n\n\n\n  \n    \n\n\n\n\n\ncomponent\napertura\nmayor\nmenor\ncierre\nvolumen\n\n\n\n\ncount\n8705.000000\n8705.000000\n8705.000000\n8705.000000\n8705.000000\n\n\nmean\n0.305063\n0.303830\n0.305777\n0.305945\n0.121702\n\n\nstd\n0.247891\n0.249959\n0.247902\n0.249006\n0.103856\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.124638\n0.121803\n0.125538\n0.124829\n0.057117\n\n\n50%\n0.186891\n0.184716\n0.187414\n0.186890\n0.092089\n\n\n75%\n0.436392\n0.433850\n0.438136\n0.437378\n0.156330\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nDos propiedades fundamentales a estudiar en las series de tiempo son la tendencia y estacionalidad de los datos. No es la excepción para nuestro caso, por suerte Darts nos ofrece métodos para poder determinarlos.\nEn este caso utilizaremos además de buscar la estacionalidad realizaremos la prueba Dickey-Fuller y las gráficas de la Función de Auto Correlación (ACF), esto normalmente se realiza en un rango de \\(25\\) observaciones anteriores al momento “presente” (LAGS). Lastimosamente, este tipo de pruebas son de tipo Univariado.\n\n\nCode\n# Utilicemos un ciclo 'for' para construir una función que busque la estacionalidad:\ndef buscador_de_estacionalidad(x, estacionalidad_encontrada=False):\n # Itera sobre el rango de valores de 'm'\n for m in range(2, 25):\n    try:\n        # Intenta buscar la estacionalidad para el valor de 'm'\n        is_seasonal, period = check_seasonality(x, m=m)\n        if is_seasonal :\n            estacionalidad_encontrada=True\n            print(f\"Estacionalidad encontrada para m={m}\")\n    except Exception as e:\n        # Captura excepciones (pueden ocurrir si 'm' no es adecuado para la serie de tiempo)\n        pass\n\n # Verifica si se encontró al menos una estacionalidad\n if not estacionalidad_encontrada:\n    print(\"No se encontró ninguna estacionalidad dentro del rango.\")\n\n\n\n\nCode\n# Apliquemos dicha función a nuestra serie:\nbuscador_de_estacionalidad(SP_ENT['cierre'])\n\n\nNo se encontró ninguna estacionalidad dentro del rango.\n\n\nPara verificar la estacionalidad de las serie usaremos la Prueba Dickey-Fuller, esta es prueba estadística que generara un \\(p\\)-valor que nos permitira aceptar o negar la hipótesis nula. Más específicamente, si rechazamos la hipótesis nula nos inclinaremos a aceptar la estacionalidad, si no tendremos el caso opuesto de tal manera que creeremos muy improbable que exista la estacionalidad.\n\n\nCode\n#Tomemos la variable sobre la que realizaremos la prueba:\nPrueba_1 = SP_ENT['cierre'].pd_dataframe()\n#Utilicemos el método de Darts para realizar la prueba:\nadft_TS = adfuller(Prueba_1, autolag='AIC')\n\n#Procesemos los resultados de dicha prueba:\nprueba_TS = pd.DataFrame(\n    {\"Valores\":[adft_TS[0], adft_TS[1], adft_TS[2], adft_TS[3], adft_TS[4]['1%'], adft_TS[4]['5%'], adft_TS[4]['10%']],\n     \"Métrica\":['Prueba Estadística', 'p-Valor', 'No. de Lags usados', 'No. de Observaciones usadas', 'Valor Critico(1%)', 'Valor Critico(5%)', 'Valor Critico(10%)']}\n    )\n\n# Veamos los resultados de la pruba:\nprueba_TS\n\n\n\n\n  \n    \n\n\n\n\n\n\nValores\nMétrica\n\n\n\n\n0\n1.285865\nPrueba Estadística\n\n\n1\n0.996529\np-Valor\n\n\n2\n37.000000\nNo. de Lags usados\n\n\n3\n8667.000000\nNo. de Observaciones usadas\n\n\n4\n-3.431105\nValor Critico(1%)\n\n\n5\n-2.861874\nValor Critico(5%)\n\n\n6\n-2.566948\nValor Critico(10%)\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nAl tener un \\(p\\)-Valor tan alto es muy probable que, en efecto, esta serie no posea ninguna estacionalidad. Apesar de ello si parece poseer una tendencia de crecimiento positiva (al menos una lineal).\nTal vez, no podamos hallar una estacionalidad clara, sin embargo, podemos intentar obtener información apartir de una gráfica de Autocorrelación, aunque no tengamos una estacionalidad definida:\n\n\nCode\nplot_acf(SP_ENT['cierre'])\n\n\n\n\n\n\n\n\n\nDebido a que en la gráfica anterior no muestra que la correlación decaiga hacia \\(0\\) no parece tener sentido aplicar ni un modelo AR ni uno MA (consecuentemente tampoco un ARIMA).\nApesar de ello, podemos intentar obtener más información apartir de la gráfica de la función de Auto Correlación Parcial PACF.\n\n\nCode\nplot_pacf(SP_ENT['cierre'])\n\n\n\n\n\n\n\n\n\nEn dicha gráfica se muestra una correlación significativa en el primer desfase, seguidas de correlaciones que no son significativas. Esto nos sugiere que, en el caso de utilizar el modelo ARIMA, deberíamos ajustar el parámetro autoregresivo \\(p\\) a \\(2\\). Podremos utilizar el hecho de que los datos diferenciados poseen una correlación significativa de \\(1\\) LAG para ajustar más adelante los parámetros del modelo."
  },
  {
    "objectID": "proyects/Series_de_Tiempo_con_XGBoost.html#modelo-de-pronóstico.",
    "href": "proyects/Series_de_Tiempo_con_XGBoost.html#modelo-de-pronóstico.",
    "title": "Modelo de Pronóstico para el S&P \\(500\\).",
    "section": "3 Modelo de Pronóstico.",
    "text": "3 Modelo de Pronóstico.\nDebido al análisis realizado en la sección anterior, se probaron distintos modelos de pronóstico pertenecientes a Darts. Entre ellos, se encuentran los siguientes:\n\nModelos que podríamos llamar “Clásicos”, tales como el Modelo Auto-Regresivo de Media Móvil (ARIMA), el Modelo de Suavizado Exponencial, el Modelo Theta (\\(\\theta\\)), y el Modelo TBATS (Trigonometría Estacional, Transformación Box-Cox, errores ARMA y componentes Estacionales y de Tendencia).\nModelos de Redes Neuronales, como RNN (Modelo de Redes Neuronales Recurrentes), el Modelo NBEATS (Análisis de expansión de base neuronal para el Pronóstico de series temporales), y NHITS.\n\nTodos estos modelos ajustaron sus parámetros y se aplicaron sobre nuestra serie de tiempo. Sin embargo, aquel que ha demostrado la mejor aproximación a los valores de prueba, con parámetros ajustados, fue el modelo de regresión basado en XGBoost (Refuerzo de Gradientes Extremo).\nUna posible excepción es el modelo RNN a Bloques, que obtuvo un MAPE (Error Porcentual Absoluto Medio) de \\(6.83\\). Sin embargo, esto se debe más a que sigue una tendencia de crecimiento lineal que no se corresponde con las fluctuaciones propias de nuestra serie. Por lo tanto, aplicamos el modelo de regresión XGBoost, que es mucho más eficiente en términos de tiempo y capacidad computacional.\n\n\nCode\n#Empecemos por utilizar la información de LAGS de la gráfica ACF para los parámetros del modelo:\nmodelo_xgb = XGBModel(\n    lags=1,\n    output_chunk_length=80,\n    random_state=42,\n    multi_models=True,\n    use_static_covariates=False\n)\n\n\n\n\nCode\n#Ajustemos el modelo a nuestros datos:\nmodelo_xgb.fit(SP_ENT)\n\n\nXGBModel(lags=1, lags_past_covariates=None, lags_future_covariates=None, output_chunk_length=80, add_encoders=None, likelihood=None, quantiles=None, random_state=42, multi_models=True, use_static_covariates=False)\n\n\nPara determinar el error en todas las pruebas de modelos que se realizaron utlilizamos el Error de Porcentaje Medio Absoluto (MAPE), entre los pronosticos y los datos de validación como sigue:\n\n\nCode\n#Realicemos el pronóstico de los casi 3 meses:\nprediccion_xgb = modelo_xgb.predict(len(SP_VAL), verbose=True, show_warnings=True)\n#Determinemos su MAPE:\nprint(\"El MAPE del modelo de Regresión XGBoost es: {:.2f}\".format(mape(SP_VAL, prediccion_xgb)))\n\n\nEl MAPE del modelo de Regresión XGBoost es: 8.06\n\n\n\n\nCode\n#Comparemos gráficamente los prónosticos de la Regresión con los valores de Prueba:\nXGB_GRA = prediccion_xgb.pd_dataframe()\n\nplt.figure(figsize=(14,4))\n\nplt.plot(XGB_GRA.index, XGB_GRA['cierre'], label='Pronóstico del Modelo XGBoost')\nplt.plot(SP_VAL_GRA.index, SP_VAL_GRA['cierre'], label='Valores de Comprobación')\n\nplt.xlabel('Indice de Orden Temporal')\nplt.ylabel('Valor de Cierre de la Accion (US)')\n\nplt.title('Gráfico de los Precios de Cierre')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nLa regresión XGBoost no acepta ningún tipo de Covariables ni estacionarias, pasadas o futuras, lo cual es una clara desvenja con respecto a otros métodos tales como el RNN o N-BEATS. Existen varios datos que podriamos haber utilizado como Covariables pasadas, especialmente el dato de la liquidez de la Reserva Federal Estadounidence (FED) que esta estrechamente relacionado con las bolsas de las que se obtiene este ínice. Sin embargo, nosotros estamos límitados y no tenemos acceso a ese tipo de datos.\n\n3.2 Backtesting\nEl backtesting simula predicciones que históricamente se habrían obtenido con un modelo dado. Dichos pronósticos simulados siempre se definen con respecto a un horizonte de pronóstico, que es el número de pasos de tiempo que separan el tiempo de predicción del tiempo de pronóstico.\nEn este caso se fueron realizando pronósticos en horizontes/periodos de \\(3\\) días con una versión optimizada del Backtesting, esta requiere menos tiempo y coste computacional a cambio de no reentrenar los datos (retrain=0).\n\n\nCode\n#Utilicemos el método para en Backtesting:\npronosticos_historicos = modelo_xgb.historical_forecasts(SP_ENT, start=pd.Timestamp('20220101'), retrain=0, forecast_horizon=3, verbose=True, show_warnings=True, enable_optimization=True)\n\n\n\n\nCode\n#Veamos que tanto error posee:\nprint(\"El MAPE entre los datos de entrenamiento y el pronóstico historico es: {:.2f}\".format(mape(SP_ENT, pronosticos_historicos)))\n\n\nEl MAPE entre los datos de entrenamiento y el pronóstico historico es: 3.98\n\n\n\n\nCode\n#Comparemoslo graficamente con los valores de la serie:\nHIS_GRA = pronosticos_historicos.pd_dataframe()\n\nplt.figure(figsize=(14,4))\n\nplt.plot(HIS_GRA.index, HIS_GRA['cierre'], label='Pronóstico del Modelo XGBoost')\nplt.plot(SP_ENT_GRA.index, SP_ENT_GRA['cierre'], label='Valores de Comprobación')\n\nplt.xlabel('Indice de Orden Temporal')\nplt.ylabel('Valor de Cierre de la Accion (US)')\n\nplt.title('Gráfico de los Precios de Cierre')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nComo se puede apreciar el valor de los pronósticos concuerda, en un margen aceptable con los datos historicos del S&P \\(500\\).\n\n\n3.3 Pruebas de Residuos.\nLas pruebas de residuos se utilizan para evaluar la calidad de un modelo de pronóstico para una serie de tiempo. Los residuos son las diferencias entre los valores observados y los valores pronosticados por el modelo.\nDentro del modulo Darts para hallar los residuos de una serie con respecto a nuestra Serie de Tiempo deberemos utilizar el modelo de pronóstico que deseemos probar y entrenarlo con datos univariados.\n\n\nCode\n#Obtengamos los residuos de este modelo aplicado a la serie de tiempo:\nmodelo_xgb_res = XGBModel(lags=1,output_chunk_length=80,random_state=42,multi_models=True,use_static_covariates=False)\nmodelo_xgb_res.fit(PTS_SP['cierre'])\n\nresiduos = modelo_xgb_res.residuals(PTS_SP['cierre'], forecast_horizon=3, retrain=False)\n\n\n\n\nCode\n# Realicemos las gráficas que resuman la información que podemos obtener de dichos residuos:\nplot_residuals_analysis(residuos)\n\n\n\n\n\n\n\n\n\nComo se puede observar nuestra distribución esta centrada en \\(0\\) con lo cual nuestro modelo XGBoost no esta sesgado. Apesar de ello, la gráfica ACF muestra un desface significativo en los primeros LAGS, por lo que dicho modelo esta perdiendo información principalmente de los datos anteriores al año \\(2004\\). Sin embargo, dichos datos no creemos sean tan significativos al momento de influir en los pronósticos de nuestro modelo.\nNos interesa saber si la regresión basada en el XGBoost es capaz de capturar las Tendencias de la serie de tiempo, para ello realizaremos una Prueba de Raíz Unitaria, esto se hace mediante la prueba de Dickey-Fuller Aumentada como sigue:\n\n\nCode\n# Utilicemos Darts para realizar la prueba:\nstat_res = stationarity_test_adf(PTS_SP['cierre'], maxlag=24)\n\n# Procesemos los datos de la prueba (para poder leerlos bien):\nprueba_raiz =  pd.DataFrame(\n    {\"Valores\":[stat_res[0],stat_res[1],stat_res[2],stat_res[3],stat_res[4]['1%'],stat_res[4]['5%'],stat_res[4]['10%'],stat_res[5]],\n     \"Métrica\":['ADF', 'p-valor', 'LAGS usados', 'No. Observaciones usadas', 'Valor Critico(1%)', 'Valor Critico(5%)','Valor Critico(10%)','Auto-LAG no nulo']})\n\n# Veamos los resultados de la prueba:\nprueba_raiz\n\n\n\n\n  \n    \n\n\n\n\n\n\nValores\nMétrica\n\n\n\n\n0\n0.929806\nADF\n\n\n1\n0.993467\np-valor\n\n\n2\n21.000000\nLAGS usados\n\n\n3\n8772.000000\nNo. Observaciones usadas\n\n\n4\n-3.431096\nValor Critico(1%)\n\n\n5\n-2.861870\nValor Critico(5%)\n\n\n6\n-2.566945\nValor Critico(10%)\n\n\n7\n-69265.770480\nAuto-LAG no nulo\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\nEl P-Valor de esta prueba indica que tan probable es que los residuos de nuestra serie de tiempo tengan una raíz unitaria (Esta es nuestra Hipótesis Nula H\\(0\\)), entre menor sea P más probable sera que la serie de tiempo tenga una raiz unitaria y viceberza.\nNuestro P-Valor es extremadamente alto, con lo cual es muy probable este modelo de pronóstico SI capture la tendencia de nuestra serie de tiempo."
  },
  {
    "objectID": "proyects/Series_de_Tiempo_con_XGBoost.html#concluciones.",
    "href": "proyects/Series_de_Tiempo_con_XGBoost.html#concluciones.",
    "title": "Modelo de Pronóstico para el S&P \\(500\\).",
    "section": "Concluciones.",
    "text": "Concluciones.\n\nDebido a la naturaleza del mercado de acciones, es extremadamente difícil encontrar un modelo de pronóstico para las series temporales de los OHLCV (Open, High, Low, Close, Volume) de acciones de empresas.\nDe acuerdo con lo anterior, debido a la falta de una estacionalidad o tendencia clara en este tipo de series temporales, los modelos clásicos como ARIMA y SARIMA pueden no resultar satisfactorios en sus pronósticos.\nNuevos tipos de modelos basados en Redes Neuronales e Inteligencia Artificial pueden resultar más efectivos para este tipo de datos al realizar pronósticos. De los que hemos probado, como los modelos de Redes Neuronales Recurrentes (RNN), el Análisis de expansión de la base neuronal (N-BEATS), etc., el que se ajustó mejor a nuestros datos de validación es el modelo de regresión basado en XGBoost.\nA pesar de que la regresión XGBoost posee un Error de Porcentaje Medio Absoluto (MAPE) menor al 10%, debido a la naturaleza de los datos, este modelo resulta ineficaz para predecir cambios de valor en las acciones del S&P 500 en plazos relativamente largos de tiempo, notese la divergencia entre los pronósticos del modelo y los datos de validación a partir del \\(15\\) de Septiembre de \\(2023\\).\nEl modelo de pronóstico que utilizamos para estos datos, en cambio, resulta útil para prever en un grado aceptable las tendencias de dichas series temporales, como lo evidencian el Backtesting y las Pruebas de residuos. Esto es válido siempre y cuando se consideren datos exógenos importantes, como la liquidez de la FED, eventos geopolíticos o desastres naturales."
  }
]